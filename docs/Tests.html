<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Statistical tests | Econometrics and Statistics</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="We run a statistical test when we want to know whether some hypothesis about a vector of parameters \(\theta\) —that is imperfectly observed— is consistent with data that are seen as random, and...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 3 Statistical tests | Econometrics and Statistics">
<meta property="og:type" content="book">
<meta property="og:description" content="We run a statistical test when we want to know whether some hypothesis about a vector of parameters \(\theta\) —that is imperfectly observed— is consistent with data that are seen as random, and...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Statistical tests | Econometrics and Statistics">
<meta name="twitter:description" content="We run a statistical test when we want to know whether some hypothesis about a vector of parameters \(\theta\) —that is imperfectly observed— is consistent with data that are seen as random, and...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Econometrics and Statistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Econometrics and statistics</a></li>
<li><a class="" href="Basics.html"><span class="header-section-number">1</span> Basic statistical results</a></li>
<li><a class="" href="TCL.html"><span class="header-section-number">2</span> Central Limit Theorem</a></li>
<li><a class="active" href="Tests.html"><span class="header-section-number">3</span> Statistical tests</a></li>
<li><a class="" href="ChapterLS.html"><span class="header-section-number">4</span> Linear Regressions</a></li>
<li><a class="" href="Panel.html"><span class="header-section-number">5</span> Panel regressions</a></li>
<li><a class="" href="estimation-methods.html"><span class="header-section-number">6</span> Estimation Methods</a></li>
<li><a class="" href="binary-choice-models.html"><span class="header-section-number">7</span> Binary-choice models</a></li>
<li><a class="" href="TS.html"><span class="header-section-number">8</span> Time Series</a></li>
<li><a class="" href="append.html"><span class="header-section-number">9</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="Tests" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Statistical tests<a class="anchor" aria-label="anchor" href="#Tests"><i class="fas fa-link"></i></a>
</h1>
<p>We run a statistical test when we want to know whether some hypothesis about a vector of parameters <span class="math inline">\(\theta\)</span> —that is imperfectly observed— is consistent with data that are seen as random, and whose randomness depend on <span class="math inline">\(\theta\)</span>.</p>
<p>Typically, assume you observe a sample <span class="math inline">\(\mathbf{x}=\{x_1,\dots,x_n\}\)</span> where the <span class="math inline">\(x_i\)</span>’s are i.i.d., of mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (these two parameters being unobseved). One may want to know whether <span class="math inline">\(\mu=0\)</span> or, maybe, whether <span class="math inline">\(\sigma = 1\)</span>.</p>
<p>The hypothesis the researcher wants to test is called the <em>null hypothesis</em>. It is often denoted by <span class="math inline">\(H_0\)</span>. It is a conjecture about a given property of a population. Without loss of generality, it can be stated as:
<span class="math display">\[
H_0:\;\{\theta \in \Theta\}.
\]</span>
It can also be defined through a function <span class="math inline">\(h\)</span> (say):<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We can relate the previous and the next equation through &lt;span class="math inline"&gt;\(\Theta = \{\theta, \,s.t.\,h(\theta)=0\}\)&lt;/span&gt;.&lt;/p&gt;'><sup>3</sup></a>
<span class="math display">\[
H_0:\;h(\theta)=0.
\]</span></p>
<p>The <em>alternative hypothesis</em>, often denoted <span class="math inline">\(H_1\)</span> is then defined by <span class="math inline">\(H_1:\;\{\theta \in \Theta^c\}\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Note that researchers may want to find support for the null hypothesis &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; as well as for the alternative hypothesis &lt;span class="math inline"&gt;\(H_1\)&lt;/span&gt;.&lt;/p&gt;'><sup>4</sup></a></p>
<p>The ingredients of a statistical test are:</p>
<ul>
<li>a vector of (unknown) parameters (<span class="math inline">\(\theta\)</span>),</li>
<li>a <strong>test statistic</strong>, that is a function of the sample components (<span class="math inline">\(S(\mathbf{x})\)</span>, say), and</li>
<li>a <strong>critical region</strong> (<span class="math inline">\(\Omega\)</span>, say), defined as a set of implausible values of <span class="math inline">\(S\)</span> under <span class="math inline">\(H_0\)</span>.</li>
</ul>
<p>To implement a test statistic, we need to know the distribution of <span class="math inline">\(S\)</span> under the null hypothesis <span class="math inline">\(H_0\)</span>. Equipped, with such a distribution, we compute <span class="math inline">\(S(\mathbf{x})\)</span> and we look at its location; more specifically, we look whether it lies within the critical region <span class="math inline">\(\Omega\)</span>. The latter corresponds to “implausible” regions of this distribution (typically the tails). If <span class="math inline">\(S \in \Omega\)</span>, then we reject the null hypothesis. Looselly speaking; it amounts to saying: <em>if <span class="math inline">\(H_0\)</span> were true, it would have been unlikely to get such a large (or small) draw for <span class="math inline">\(S\)</span></em>.</p>
<p>Hence, there are two possible outcomes for a statisitcal test:</p>
<ul>
<li>
<span class="math inline">\(H_0\)</span> is rejected if <span class="math inline">\(S \in \Omega\)</span>;</li>
<li>
<span class="math inline">\(H_0\)</span> is not rejected if <span class="math inline">\(S \not\in \Omega\)</span>.</li>
</ul>
<p>Except in extreme cases, there is always a non-zero probability to reject <span class="math inline">\(H_0\)</span> while it is true (<strong>Type I error</strong>, or <strong>false positive</strong>), or to fail to reject it while it is false (<strong>Type II error</strong>, or <strong>false negative</strong>).</p>
<p>This vocabulary is widely used. For instance, the notions of false positive and false negative are used in the context of Early Warning Signals (see Example <a href="Tests.html#exm:FPFN">3.1</a>).</p>
<div class="example">
<p><span id="exm:FPFN" class="example"><strong>Example 3.1  (Early Warning Signals) </strong></span>These are approaches that aim to detect the occurrence of crises in advance. See, e.g., <a href="http://www.ecb.europa.eu/events/pdf/conferences/140623/Vasicek-et-al_Comparing-Different-Early-Warning-Systems.pdf?F96bbb525a26071ecf97f9154fb3cc73">ECB (2014)</a>, for applications to financial crises.</p>
<p>To implement such approaches, researchers look for signals/indices forecasting crises. Suppose one index (<span class="math inline">\(W\)</span>, say) tends to be large before financial crises; one may define an EWS by predicting a crisis when <span class="math inline">\(W&gt;a\)</span>, where <span class="math inline">\(a\)</span> is a given threshold. It is easliy seen that the lower (respectively higeher) <span class="math inline">\(a\)</span>, the larger the fraction of FP (resp. of FN).</p>
</div>
<div id="size-and-power-of-a-test" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Size and power of a test<a class="anchor" aria-label="anchor" href="#size-and-power-of-a-test"><i class="fas fa-link"></i></a>
</h2>
<div class="definition">
<p><span id="def:sizepower" class="definition"><strong>Definition 3.1  (Size and Power of a test) </strong></span>For a given test,</p>
<ul>
<li>the probability of type-I errors, denoted by <span class="math inline">\(\alpha\)</span>, is called the <strong>size</strong>, or <strong>significance level</strong>, of the test,</li>
<li>the <strong>power</strong> of a test is equal to <span class="math inline">\(1 - \beta\)</span>, where <span class="math inline">\(\beta\)</span> is the probability of type-II errors.</li>
</ul>
</div>
<p>Formally, the previous definitions can be written as follows:
<span class="math display">\[\begin{eqnarray}
\alpha &amp;=&amp; \mathbb{P}(S \in \Omega|H_0) \quad \mbox{(Proba. of a false positive)}\\
\beta &amp;=&amp; \mathbb{P}(S \not\in \Omega|H_1) \quad \mbox{(Proba. of a false negative)}.
\end{eqnarray}\]</span></p>
<p>The power is the probability that the test will lead to the rejection of the null hypothesis if the latter is false. Therefore, for a given size, we prefer tests with high power.</p>
<p>In most cases, there is a trade-off between size and power, whch is easily understood in the EWS context (Example <a href="Tests.html#exm:FPFN">3.1</a>): increasing the threshold <span class="math inline">\(a\)</span> reduces the fraction of FP (thereby reducing the size of the test), but it increases the fraction of FN (thereby reducing the power of the test).</p>
</div>
<div id="the-different-types-of-statistical-tests" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> The different types of statistical tests<a class="anchor" aria-label="anchor" href="#the-different-types-of-statistical-tests"><i class="fas fa-link"></i></a>
</h2>
<p>How to determine the critical region? Loosely speaking, we want the critical region to be a set of “implausible” values of the test statistic <span class="math inline">\(S\)</span> under the null hypothesis <span class="math inline">\(H_0\)</span>. The lower the size of the test (<span class="math inline">\(\alpha\)</span>), the more implausible these values. Recall that, by definition of the size of the test, <span class="math inline">\(\alpha = \mathbb{P}(S \in \Omega|H_0)\)</span>. That is, if <span class="math inline">\(\alpha\)</span> is small, there is only a small probability that <span class="math inline">\(S\)</span> lies in <span class="math inline">\(\Omega\)</span> under <span class="math inline">\(H_0\)</span>.</p>
<p>Consider the case where, under <span class="math inline">\(H_0\)</span>, the distribution of the test statistic is symmetrical (e.g., normal distribution or Student-t distribution). In this case, the critical region is usually defined by the union of the two tails of the distribution. The test is then said to be a <strong>two-tailed test</strong> or a <strong>two-sided test</strong>. This situation is illustrated by Figures <a href="Tests.html#fig:Illusttest1">3.1</a> and <a href="Tests.html#fig:Illusttest2">3.2</a>. (Use this <a href="https://jrenne.shinyapps.io/tests/">web interface</a> to explore alternative situations.)</p>
<p>Figure <a href="Tests.html#fig:Illusttest2">3.2</a> also illustrates the notion of <em>p-value</em> (in the case of a two-sided test). The p-value can be defined as the value of the size of the test <span class="math inline">\(\alpha\)</span> that is such that the computed test statistic, <span class="math inline">\(S\)</span>, is at the “frontier” of the critical region. Given this definition, if the p-value is smaller (respectively larger) than the size of the test, we reject (resp. cannot rejet) the null hypothesis at the <span class="math inline">\(\alpha\)</span> significance level.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:Illusttest1"></span>
<img src="EcoStat_files/figure-html/Illusttest1-1.png" alt="Two-sided test. Under $H_0$, $S \sim t(5)$. $\alpha$ is the size of the test." width="672"><p class="caption">
Figure 3.1: Two-sided test. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S \sim t(5)\)</span>. <span class="math inline">\(\alpha\)</span> is the size of the test.
</p>
</div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:Illusttest2"></span>
<img src="EcoStat_files/figure-html/Illusttest2-1.png" alt="Two-sided test. Under $H_0$, $S \sim t(5)$. $\alpha$ is the size of the test." width="672"><p class="caption">
Figure 3.2: Two-sided test. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S \sim t(5)\)</span>. <span class="math inline">\(\alpha\)</span> is the size of the test.
</p>
</div>
<p>Figures <a href="Tests.html#fig:IllusTestOneSided1">3.3</a> and <a href="Tests.html#fig:IllusTestOneSided2">3.4</a> illustrate the <strong>one-tailed</strong>, or <strong>one-sided</strong> situation. These tests are typically employed when the distribution of the test statistic under the null hypothesis has a support on <span class="math inline">\(\mathbb{R}^+\)</span> (e.g., the chi-square distribution <span class="math inline">\(\chi^2\)</span>, see Def. <a href="append.html#def:chi2">9.11</a>). Figure <a href="Tests.html#fig:IllusTestOneSided2">3.4</a> also illustrates the notion of p-value associated with a one-sided statistical test.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:IllusTestOneSided1"></span>
<img src="EcoStat_files/figure-html/IllusTestOneSided1-1.png" alt="One-sided test. Under $H_0$, $S \sim \chi^2(5)$. $\alpha$ is the size of the test." width="672"><p class="caption">
Figure 3.3: One-sided test. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S \sim \chi^2(5)\)</span>. <span class="math inline">\(\alpha\)</span> is the size of the test.
</p>
</div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:IllusTestOneSided2"></span>
<img src="EcoStat_files/figure-html/IllusTestOneSided2-1.png" alt="One-sided test. Under $H_0$, $S \sim \chi^2(5)$. $\alpha$ is the size of the test." width="672"><p class="caption">
Figure 3.4: One-sided test. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S \sim \chi^2(5)\)</span>. <span class="math inline">\(\alpha\)</span> is the size of the test.
</p>
</div>
<div class="example">
<p><span id="exm:Factory" class="example"><strong>Example 3.2  (A practical illustration of size and power) </strong></span>Consider a factory that produces metal cylinders whose diameter has to be equal to 1,cm. The tolerance is <span class="math inline">\(a=0.01\)</span> cm. More precisely, more than 90% of the pieces have to satisfy the tolerance for the whole production (say 1.000.000 pieces) to be bought by the client.</p>
<p>The production technology is such that a proportion <span class="math inline">\(\theta\)</span> (imperfectly known) of the pieces does not satisfy the tolerance. The (population) parameter <span class="math inline">\(\theta\)</span> could be computed by measuring all the pieces but this would be costly. Instead, it is decided that <span class="math inline">\(n \ll 1.000.000\)</span> pieces will be measured. In this context, the null hypothesis <span class="math inline">\(H_0\)</span> is <span class="math inline">\(\theta &lt; 10\%\)</span>. The producing firm would like <span class="math inline">\(H_0\)</span> to be true.</p>
<p>Let us denote by <span class="math inline">\(d_i\)</span> the binary indicator defined as:
<span class="math display">\[
d_i = \left\{
\begin{array}{cll}
0 &amp; \mbox{if the size of the $i^{th}$ cylinder is in $[1-a,1+a]$;}\\
1 &amp; \mbox{otherwise.}
\end{array}
\right.
\]</span></p>
<p>We set <span class="math inline">\(x_n=\sum_{i=1}^n d_i\)</span>. That is, <span class="math inline">\(x_n\)</span> is the number of measured pieces that do not satisfy the tolerance (out of <span class="math inline">\(n\)</span>).</p>
<p>The decision rule is: accept <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\dfrac{x_n}{n} \le b\)</span>, reject otherwise. A natural choice for <span class="math inline">\(b\)</span> is <span class="math inline">\(b=0.1\)</span>. However, this would not be a conservative choice, since it remains likely that <span class="math inline">\(x_n&lt;0.1\)</span> even if <span class="math inline">\(\mathbb{E}(x_n)=\theta&gt;0.1\)</span> (especially if <span class="math inline">\(n\)</span> is small). Hence, if one chooses <span class="math inline">\(b=0.1\)</span>, the probability of false negative may be high.</p>
<p>In this simple example, the size and the power of the test can be computed analytically. The test statistic is <span class="math inline">\(S_n=\frac{x_n}{n}\)</span> and the critical region is <span class="math inline">\(\Omega = [b,1]\)</span>. The probability to reject <span class="math inline">\(H_0\)</span> is:
<span class="math display">\[\begin{eqnarray*}
\mathbb{P}_\theta(S_n \in \Omega) = \sum_{i=b \times n+1}^{n}C_{n}^i\theta^i(1-\theta)^{n-i}.
\end{eqnarray*}\]</span></p>
<p>When <span class="math inline">\(\theta&lt;0.1\)</span>, the previous expression gives the size of the test, while it gives the power of the test when <span class="math inline">\(\theta&gt;0.1\)</span>. Figure <a href="Tests.html#fig:FactoryR">3.5</a> shows how this probability depends on <span class="math inline">\(\theta\)</span> for two sample sizes: <span class="math inline">\(n=100\)</span> (upper plot) and <span class="math inline">\(n=500\)</span> (lower plot).</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:FactoryR"></span>
<img src="EcoStat_files/figure-html/FactoryR-1.png" alt="Factory example." width="672"><p class="caption">
Figure 3.5: Factory example.
</p>
</div>
<p>(Alternative situations can be explored by using this <a href="https://jrenne.shinyapps.io/Factory/">web interface</a>.)</p>
</div>
</div>
<div id="asymptotic-properties-of-statistical-tests" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Asymptotic properties of statistical tests<a class="anchor" aria-label="anchor" href="#asymptotic-properties-of-statistical-tests"><i class="fas fa-link"></i></a>
</h2>
<p>In some cases, the distribution of the test statistic is known only asymptotically. For instance, it may be the case that the distribution of a given test statistic becomes normal only in large samples (e.g., related to the CLT, see Section <a href="TCL.html#TCL">2</a>). The level of the test is then not know for small sample sizes, but only asymptotically, i.e., when <span class="math inline">\(n\)</span> becomes infinitely large. That defines the asymptotic level of the test:</p>
<div class="definition">
<p><span id="def:asmyptlevel" class="definition"><strong>Definition 3.2  (Asymptotic level) </strong></span>An asymptotic test with critical region <span class="math inline">\(\Omega_n\)</span> has an asymptotic level equal to <span class="math inline">\(\alpha\)</span> if:
<span class="math display">\[
\underset{\theta \in \Theta}{\mbox{sup}} \quad \underset{n \rightarrow \infty}{\mbox{lim}} \mathbb{P}_\theta (S_n \in \Omega_n) = \alpha.
\]</span></p>
</div>
<div class="example">
<p><span id="exm:FactAsymptlevel" class="example"><strong>Example 3.3  (The factory example) </strong></span>Let us come back to the factory example (Example <a href="Tests.html#exm:Factory">3.2</a>). Because <span class="math inline">\(S_n =\bar{d}_n\)</span>, and since <span class="math inline">\(\mathbb{E}(d_i)=\theta\)</span> and <span class="math inline">\(\mathbb{V}ar(d_i)=\theta(1-\theta)\)</span>, the CLT (Theorem <a href="TCL.html#thm:LindbergLevyCLT">2.2</a>) leads to:
<span class="math display">\[
S_n \sim \mathcal{N}\left(\theta,\frac{1}{n}\theta(1-\theta)\right) \quad or \quad \frac{\sqrt{n}(S_n-\theta)}{\sqrt{\theta(1-\theta)}} \sim \mathcal{N}(0,1).
\]</span></p>
<p>Hence, <span class="math inline">\(\mathbb{P}_\theta (S_n \in \Omega_n)=\mathbb{P}_\theta (S_n &gt; b) \approx 1-\Phi\left(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}}\right)\)</span>. Since function <span class="math inline">\(\theta \rightarrow 1-\Phi\left(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}}\right)\)</span> increases w.r.t. <span class="math inline">\(\theta\)</span>, we have:
<span class="math display">\[
\underset{\theta \in \Theta=[0,0.1]}{\mbox{sup}} \quad \mathbb{P}_\theta (S_n &gt; b_n) = \mathbb{P}_{\theta=0.1} (S_n \in \Omega_n)\approx\]</span>
<span class="math display">\[
1-\Phi\left(\frac{\sqrt{n}(b_n-0.1)}{0.3}\right).
\]</span>
Hence, if we set <span class="math inline">\(b_n = 0.1 + 0.3\Phi^{-1}(1-\alpha)/\sqrt{n}\)</span>, we have <span class="math inline">\({\mbox{sup}}_{\theta \in \Theta=[0,0.1]} \quad \mathbb{P}_\theta (S_n &gt; b_n) \approx \alpha\)</span> for large values of <span class="math inline">\(n\)</span>.</p>
</div>
<p>Let us now turn to the power of the test. Although it is often difficult to compute the power of the test, it is sometimes feasible to demonstrate that the power of the test converges to one when <span class="math inline">\(n\)</span> goes to <span class="math inline">\(+\infty\)</span>. In that case, if <span class="math inline">\(H_0\)</span> is false, the probability to reject it tends to be close to one in large samples.</p>
<div class="definition">
<p><span id="def:asmyptconsisttest" class="definition"><strong>Definition 3.3  (Asymptotically consistent test) </strong></span>An asymptotic test with critical region <span class="math inline">\(\Omega_n\)</span> is consistent if:
<span class="math display">\[
\forall \theta \in \Theta^c, \quad \mathbb{P}_\theta (S_n \in \Omega_n) \rightarrow 1.
\]</span></p>
</div>
<div class="example">
<p><span id="exm:FactAsymptConsist" class="example"><strong>Example 3.4  (The factory example) </strong></span>Let us come back to the factory example (Example <a href="Tests.html#exm:Factory">3.2</a>). We proceed under the assumption that <span class="math inline">\(\theta&gt;0.1\)</span> and we consider <span class="math inline">\(b_n = b = 0.1\)</span>. We still have:
<span class="math display">\[
\mathbb{P}_\theta (S_n \in \Omega_n)=\mathbb{P}_\theta (S_n &gt; b) \approx 1-\Phi\left(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}}\right).
\]</span></p>
<p>Because <span class="math inline">\(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}} \underset{n \rightarrow \infty}{\rightarrow} -\infty\)</span>, we have
<span class="math display">\[
\mathbb{P}_\theta (S_n &gt; b) \approx 1- \underbrace{\Phi\left(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}}\right)}_{\underset{n \rightarrow \infty}{\rightarrow} 0} \underset{n \rightarrow \infty}{\rightarrow} 1.
\]</span>
Therefore, with <span class="math inline">\(b_n=b=0.1\)</span>, the test is consistent.</p>
</div>
</div>
<div id="example-normality-tests" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Example: Normality tests<a class="anchor" aria-label="anchor" href="#example-normality-tests"><i class="fas fa-link"></i></a>
</h2>
<p>Because many statistical results are valid only if the underlying data is normally distributed, researchers often have tu conduct normality tests. Under the null hypothesis, the data at hand (<span class="math inline">\(\mathbf{y}=\{y_1,\dots,y_n\}\)</span>, say) are drawn from a Gaussian distribution. A popular normality test is the Jarque-Bera test. It consists in verifying that the sample skewness and kurtosis of the <span class="math inline">\(y_i\)</span>’s are consistent with those of the normal distribution. Let us first define the skewness and kurtosis of a random variable.</p>
<p>Let <span class="math inline">\(f\)</span> be the p.d.f. of <span class="math inline">\(Y\)</span>. The <span class="math inline">\(k^{th}\)</span> <strong>standardized moment</strong> of <span class="math inline">\(Y\)</span> is defined as:
<span class="math display">\[
\psi_k = \frac{\mu_k}{\left(\sqrt{\mathbb{V}ar(Y)}\right)^k},
\]</span>
where <span class="math inline">\(\mathbb{E}(Y)=\mu\)</span> and
<span class="math display">\[
\mu_k = \mathbb{E}[(Y-\mu)^k]= \int_{-\infty}^{\infty} (y-\mu)^k f(y) dy
\]</span>
is the <span class="math inline">\(k^{th}\)</span> <strong>central moment</strong> of <span class="math inline">\(Y\)</span>. In particular, <span class="math inline">\(\mu_2 = \mathbb{V}ar(Y)\)</span>. Therefore:
<span class="math display">\[
\psi_k = \frac{\mu_k}{\left(\mu_2^{1/2}\right)^k},
\]</span></p>
<p>The <strong>skewness</strong> of <span class="math inline">\(Y\)</span> corresponds to <span class="math inline">\(\psi_3\)</span> and the <strong>kurtosis</strong> to <span class="math inline">\(\psi_4\)</span> (Def. <a href="append.html#def:skewnesskurtosis">9.6</a>).</p>
<div class="proposition">
<p><span id="prp:normSkewKurt" class="proposition"><strong>Proposition 3.1  (Skewness and kurtosis of the normal distribution) </strong></span>For a Gaussian var., the skewness (<span class="math inline">\(\psi_3\)</span>) is 0 and the kurtosis (<span class="math inline">\(\psi_4\)</span>) is 3.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>For a centered Gaussian distribution, <span class="math inline">\((-y)^3f(-y)=-y^3f(y)\)</span>. This implies that
<span class="math display">\[\begin{eqnarray*}
\int_{-\infty}^{\infty}y^3f(y)dy&amp;=&amp;\int_{-\infty}^{0}y^3f(y)dy+\int_{0}^{\infty}y^3f(y)dy\\
&amp;=&amp;-\int_{0}^{\infty}y^3f(y)dy+\int_{0}^{\infty}y^3f(y)dy=0,
\end{eqnarray*}\]</span>
which leads to the skewness result.</p>
<p>Moreover, for a Gaussian distribution, <span class="math inline">\(df(y)/dy=-yf(y)\)</span> and therefore <span class="math inline">\(\frac{d}{dy}(y^3f(y))=3y^2f(y)-y^4f(y)\)</span>. Partial integration leads to the kurtosis result.</p>
</div>
<p>Let us now introduce the sample analog of standardized moments. The <span class="math inline">\(k^{th}\)</span> <strong>central sample moment</strong> of <span class="math inline">\(Y\)</span> is given by:
<span class="math display">\[
m_k = \frac{1}{n}\sum_{i=1}^n(y_i - \bar{y})^k,
\]</span>
and the <span class="math inline">\(k^{th}\)</span> <strong>standardized sample moment</strong> of <span class="math inline">\(Y\)</span> is given by:
<span class="math display">\[
g_k = \frac{m_k}{m_2^{k/2}}.
\]</span></p>
<div class="proposition">
<p><span id="prp:conssitCentralMoments" class="proposition"><strong>Proposition 3.2  (Consistency of central sample moments) </strong></span>If the <span class="math inline">\(k^{th}\)</span> central moment of <span class="math inline">\(Y\)</span>, exists and if the <span class="math inline">\(y_i\)</span>’s are i.i.d., then the sample central moment <span class="math inline">\(m_k\)</span> is a consistent estimate of the central moment <span class="math inline">\(\mu_k\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:Asymptg3Normal" class="proposition"><strong>Proposition 3.3  (Asymptotic distribution of 3rd-order sample central moment of a normal distribution) </strong></span>If <span class="math inline">\(y_i\sim\,i.i.d.\,\mathcal{N}(\mu,\sigma^2)\)</span>, then <span class="math inline">\(\sqrt{n}g_3 \overset{d}{\rightarrow} \mathcal{N}(0,6)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>See, e.g. <a href="http://www.springer.com/la/book/9780387985954">Lehmann (1999)</a>.</p>
</div>
<div class="proposition">
<p><span id="prp:Asymptg4Normal" class="proposition"><strong>Proposition 3.4  (Asymptotic distribution of 4th-order sample central moment of a normal distribution) </strong></span>If <span class="math inline">\(y_i\sim\,i.i.d.\,\mathcal{N}(\mu,\sigma^2)\)</span>, then <span class="math inline">\(\sqrt{n}(g_4-3) \overset{d}{\rightarrow} \mathcal{N}(0,24)\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:Asymptg3g4Normal" class="proposition"><strong>Proposition 3.5  (Joint asymptotic distribution of 3rd and 4th-order sample central moments of a normal distribution) </strong></span>If <span class="math inline">\(y_i\sim\,i.i.d.\,\mathcal{N}(\mu,\sigma^2)\)</span>, then the vector <span class="math inline">\((\sqrt{n}g_3,\sqrt{n}(g_4-3))\)</span> is asymptotically bivariate Gaussian. Further its elements are uncorrelated and therefore independent.</p>
</div>
<p>The Jarque-Bera statistic is defined by:
<span class="math display">\[
JB = n \left( \frac{g_3^2}{6}+\frac{(g_4-3)^2}{24} \right) = \frac{n}{6}\left(g_3^2 + \frac{(g_4-3)^2}{4}\right).
\]</span></p>
<div class="proposition">
<p><span id="prp:JB" class="proposition"><strong>Proposition 3.6  (Jarque-Bera asympt. distri.) </strong></span>If <span class="math inline">\(y_i\sim\,i.i.d.\,\mathcal{N}(\mu,\sigma^2)\)</span>, <span class="math inline">\(JB \overset{d}{\rightarrow} \chi^2(2)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>This directly derives from Proposition <a href="Tests.html#prp:Asymptg3g4Normal">3.5</a>.</p>
</div>
<div class="example">
<p><span id="exm:JB" class="example"><strong>Example 3.5  (Consistency of the Jarque-Bera normality test) </strong></span>This example illustrates the consistency of the JB test (see Def. <a href="Tests.html#def:asmyptconsisttest">3.3</a>).</p>
<p>For each row of matrix <code>x</code>, the function <code>JB</code> (defined below) computes the Jarque-Bera test statistic. (That is, each row is considered as a given sample.)</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">JB</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="co"># number of samples</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="co"># sample size</span></span>
<span>  <span class="va">x.bar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x</span>,<span class="fl">1</span>,<span class="va">mean</span><span class="op">)</span></span>
<span>  <span class="va">x.x.bar</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">x.bar</span>,<span class="va">N</span>,<span class="va">n</span><span class="op">)</span></span>
<span>  <span class="va">m.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x.x.bar</span>,<span class="fl">1</span>,<span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span>
<span>  <span class="va">m.3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x.x.bar</span>,<span class="fl">1</span>,<span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">3</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span>
<span>  <span class="va">m.4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x.x.bar</span>,<span class="fl">1</span>,<span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">4</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span>
<span>  <span class="va">g.3</span> <span class="op">&lt;-</span> <span class="va">m.3</span><span class="op">/</span><span class="va">m.2</span><span class="op">^</span><span class="op">(</span><span class="fl">3</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">g.4</span> <span class="op">&lt;-</span> <span class="va">m.4</span><span class="op">/</span><span class="va">m.2</span><span class="op">^</span><span class="op">(</span><span class="fl">4</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">n</span><span class="op">*</span><span class="op">(</span><span class="va">g.3</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="fl">6</span> <span class="op">+</span> <span class="op">(</span><span class="va">g.4</span><span class="op">-</span><span class="fl">3</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="fl">24</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Let us first consider the case where <span class="math inline">\(H_0\)</span> is satisfied. Figure <a href="Tests.html#fig:JBTest2">3.6</a> displays, for different sample sizes <span class="math inline">\(n\)</span>, the distribution of the JB statistics when the <span class="math inline">\(y_i\)</span>’s are normal, consistently with <span class="math inline">\(H_0\)</span>. It appears that when <span class="math inline">\(n\)</span> grows, the distribution indeed converges to the <span class="math inline">\(\chi^2(2)\)</span> distribution (as stated by Proposition <a href="Tests.html#prp:JB">3.6</a>).</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">all.n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>,<span class="fl">10</span>,<span class="fl">20</span>,<span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">nb.sim</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">all.n</span><span class="op">)</span><span class="op">)</span>,<span class="va">nb.sim</span>,<span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">all.n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">all.n</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="va">all.n</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="fu">JB</span><span class="op">(</span><span class="va">y</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">]</span><span class="op">)</span>,nclass <span class="op">=</span> <span class="fl">200</span>,freq <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>       main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"n = "</span>,<span class="fu"><a href="https://rdrr.io/r/base/toString.html">toString</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span>,xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">xx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">10</span>,by<span class="op">=</span><span class="fl">.01</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">xx</span>,<span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">dchisq</a></span><span class="op">(</span><span class="va">xx</span>,df <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:JBTest2"></span>
<img src="EcoStat_files/figure-html/JBTest2-1.png" alt="Distribution of the JB test statistic under $H_0$ (normality)." width="672"><p class="caption">
Figure 3.6: Distribution of the JB test statistic under <span class="math inline">\(H_0\)</span> (normality).
</p>
</div>
<p>Now, replace <code>rnorm</code> with <code>runif</code>. The <span class="math inline">\(y_i\)</span>’s are then drawn from a uniform distribution. <span class="math inline">\(H_0\)</span> is not satisfied. Figure <a href="Tests.html#fig:JBTest3">3.7</a> then shows that, when <span class="math inline">\(n\)</span> grows, the distributions of the JB statistic shift to the right. This results in the consistency of the JB test (see Def. <a href="Tests.html#def:asmyptconsisttest">3.3</a>).</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:JBTest3"></span>
<img src="EcoStat_files/figure-html/JBTest3-1.png" alt="Distribution of the JB test statistic when the $y_i$'s are drawn from a uniform distribution (hence $H_0$ is not satisfied)." width="672"><p class="caption">
Figure 3.7: Distribution of the JB test statistic when the <span class="math inline">\(y_i\)</span>’s are drawn from a uniform distribution (hence <span class="math inline">\(H_0\)</span> is not satisfied).
</p>
</div>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="TCL.html"><span class="header-section-number">2</span> Central Limit Theorem</a></div>
<div class="next"><a href="ChapterLS.html"><span class="header-section-number">4</span> Linear Regressions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#Tests"><span class="header-section-number">3</span> Statistical tests</a></li>
<li><a class="nav-link" href="#size-and-power-of-a-test"><span class="header-section-number">3.1</span> Size and power of a test</a></li>
<li><a class="nav-link" href="#the-different-types-of-statistical-tests"><span class="header-section-number">3.2</span> The different types of statistical tests</a></li>
<li><a class="nav-link" href="#asymptotic-properties-of-statistical-tests"><span class="header-section-number">3.3</span> Asymptotic properties of statistical tests</a></li>
<li><a class="nav-link" href="#example-normality-tests"><span class="header-section-number">3.4</span> Example: Normality tests</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Econometrics and Statistics</strong>" was written by Jean-Paul Renne. It was last built on 2023-03-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
