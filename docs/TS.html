<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Time Series | Econometrics and Statistics</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="8.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="Chapter 8 Time Series | Econometrics and Statistics">
<meta property="og:type" content="book">
<meta property="og:description" content="8.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Time Series | Econometrics and Statistics">
<meta name="twitter:description" content="8.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.0/transition.js"></script><script src="libs/bs3compat-0.5.0/tabs.js"></script><script src="libs/bs3compat-0.5.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Econometrics and Statistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Econometrics and statistics</a></li>
<li><a class="" href="Basics.html"><span class="header-section-number">1</span> Basic statistical results</a></li>
<li><a class="" href="TCL.html"><span class="header-section-number">2</span> Central Limit Theorem</a></li>
<li><a class="" href="Tests.html"><span class="header-section-number">3</span> Statistical tests</a></li>
<li><a class="" href="ChapterLS.html"><span class="header-section-number">4</span> Linear Regressions</a></li>
<li><a class="" href="Panel.html"><span class="header-section-number">5</span> Panel regressions</a></li>
<li><a class="" href="estimation-methods.html"><span class="header-section-number">6</span> Estimation Methods</a></li>
<li><a class="" href="binary-choice-models.html"><span class="header-section-number">7</span> Binary-choice models</a></li>
<li><a class="active" href="TS.html"><span class="header-section-number">8</span> Time Series</a></li>
<li><a class="" href="append.html"><span class="header-section-number">9</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="TS" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Time Series<a class="anchor" aria-label="anchor" href="#TS"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-to-time-series" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Introduction to time series<a class="anchor" aria-label="anchor" href="#introduction-to-time-series"><i class="fas fa-link"></i></a>
</h2>
<p>A time series is an infinite sequence of random variables indexed by time: <span class="math inline">\(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\)</span>, <span class="math inline">\(y_i \in \mathbb{R}^k\)</span>. In practice, we only observe samples, typically: <span class="math inline">\(\{y_{1},\dots,y_T\}\)</span>.</p>
<p>Standard time series models are built using <strong>shocks</strong> that we will often denote by <span class="math inline">\(\varepsilon_t\)</span>. Typically, <span class="math inline">\(\mathbb{E}(\varepsilon_t)=0\)</span>. In many models, the shocks are supposed to be i.i.d., but there exist other (less restrictive) notions of shocks. In particular, the definition of many processes is based on white noises:</p>
<div class="definition">
<p><span id="def:whitenoise" class="definition"><strong>Definition 8.1  (White noise) </strong></span>The process <span class="math inline">\(\{\varepsilon_t\}_{t \in] -\infty,+\infty[}\)</span> is a white noise if, for all <span class="math inline">\(t\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li>
<span class="math inline">\(\mathbb{E}(\varepsilon_t)=0\)</span>,</li>
<li>
<span class="math inline">\(\mathbb{E}(\varepsilon_t^2)=\sigma^2&lt;\infty\)</span> and</li>
<li>for all <span class="math inline">\(s\ne t\)</span>, <span class="math inline">\(\mathbb{E}(\varepsilon_t \varepsilon_s)=0\)</span>.</li>
</ol>
</div>
<p>Another type of shocks that are commonly used are Martingale Difference Sequences:</p>
<div class="definition">
<p><span id="def:MDS" class="definition"><strong>Definition 8.2  (Martingale Difference Sequence) </strong></span>The process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a martingale difference sequence (MDS) if <span class="math inline">\(\mathbb{E}(|\varepsilon_{t}|)&lt;\infty\)</span> and if, for all <span class="math inline">\(t\)</span>,
<span class="math display">\[
\underbrace{\mathbb{E}_{t-1}(\varepsilon_{t})}_{\mbox{Expectation conditional on the past}}=0.
\]</span></p>
</div>
<p>By definition, if <span class="math inline">\(y_t\)</span> is a martingale, then <span class="math inline">\(y_{t}-y_{t-1}\)</span> is a MDS.</p>
<div class="example">
<p><span id="exm:ARCH" class="example"><strong>Example 8.1  (ARCH process) </strong></span>The Autoregressive conditional heteroskedasticity (ARCH) process is an example of shock that satisfies the MDS definition but that is not i.i.d.:
<span class="math display">\[
\varepsilon_{t} = \sigma_t \times z_{t},
\]</span>
where <span class="math inline">\(z_t \sim i.i.d.\,\mathcal{N}(0,1)\)</span> and <span class="math inline">\(\sigma_t^2 = w + \alpha \varepsilon_{t-1}^2\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:whiteNotMDS" class="example"><strong>Example 8.2  </strong></span>A (weak) white noise process is not necessarily a MDS. This is for instance the following process:
<span class="math display">\[
\varepsilon_{t} = z_t + z_{t-1}z_{t-2},
\]</span>
where <span class="math inline">\(z_t \sim i.i.d.\mathcal{N}(0,1)\)</span>.</p>
</div>
<p>In the following, to simplify the exposition, we will essentially consider <strong>strong white noises</strong>. A strong white noise is a particular case of white noise where the <span class="math inline">\(\varepsilon_{t}\)</span>’s are serially independent. Using the law of iterated expectation, it can be shown that a strong white noise is a martingale difference sequence. Indeed, when the <span class="math inline">\(\varepsilon_{t}\)</span>’s are serially independent, we have that <span class="math inline">\(\mathbb{E}(\varepsilon_{t}|\varepsilon_{t-1},\varepsilon_{t-2},\dots)=\mathbb{E}(\varepsilon_{t})=0\)</span>.</p>
<p>Let us now introduce the lag operator. The lag operator, denoted by <span class="math inline">\(L\)</span>, is defined on the time series space and is defined by:
<span class="math display" id="eq:lagOp">\[\begin{equation}
L: \{y_t\}_{t=-\infty}^{+\infty} \rightarrow \{w_t\}_{t=-\infty}^{+\infty} \quad \mbox{with} \quad w_t = y_{t-1}.\tag{8.1}
\end{equation}\]</span></p>
<p>We have: <span class="math inline">\(L^2 y_t = y_{t-2}\)</span> and, more generally, <span class="math inline">\(L^k y_t = y_{t-k}\)</span>.</p>
<p>Consider a time series <span class="math inline">\(y_t\)</span> defined by <span class="math inline">\(y_t = \mu + \phi y_{t-1} + \varepsilon_t\)</span>, where the <span class="math inline">\(\varepsilon_t\)</span>’s are i.i.d. <span class="math inline">\(\mathcal{N}(0,\sigma^2)\)</span>. Using the lag operator, the dynamics of <span class="math inline">\(y_t\)</span> can be expressed as follows:
<span class="math display">\[
(1-\phi L) y_t = \mu + \varepsilon_t.
\]</span></p>
<p>It is easily checked that we have <span class="math inline">\(L^2 y_t = y_{t-2}\)</span> and, generally, <span class="math inline">\(L^k y_t = y_{t-k}\)</span>.</p>
<p>If it exists, the <strong>unconditional (or marginal) mean</strong> of the random variable <span class="math inline">\(y_t\)</span> is given by:
<span class="math display">\[
\mu_t := \mathbb{E}(y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) dy_t,
\]</span>
where <span class="math inline">\(f_{Y_t}\)</span> is the unconditional (or marginal) density of <span class="math inline">\(y_t\)</span>. Similarly, if it exists, the <strong>unconditional (or marginal) variance</strong> of the random variable <span class="math inline">\(y_t\)</span> is:
<span class="math display">\[
\mathbb{V}ar(y_t) = \int_{-\infty}^{\infty} (y_t - \mathbb{E}(y_t))^2 f_{Y_t}(y_t) dy_t.
\]</span></p>
<div class="definition">
<p><span id="def:autocov" class="definition"><strong>Definition 8.3  (Autocovariance) </strong></span>The <span class="math inline">\(j^{th}\)</span> autocovariance of <span class="math inline">\(y_t\)</span> is given by:
<span class="math display">\[\begin{eqnarray*}
\gamma_{j,t} &amp;:=&amp; \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty} [y_t - \mathbb{E}(y_t)][y_{t-j} - \mathbb{E}(y_{t-j})] \times\\
&amp;&amp; f_{Y_t,Y_{t-1},\dots,Y_{t-j}}(y_t,y_{t-1},\dots,y_{t-j}) dy_t dy_{t-1} \dots dy_{t-j} \\
&amp;=&amp; \mathbb{E}([y_t - \mathbb{E}(y_t)][y_{t-j} - \mathbb{E}(y_{t-j})]),
\end{eqnarray*}\]</span>
where <span class="math inline">\(f_{Y_t,Y_{t-1},\dots,Y_{t-j}}(y_t,y_{t-1},\dots,y_{t-j})\)</span> is the joint distribution of <span class="math inline">\(y_t,y_{t-1},\dots,y_{t-j}\)</span>.</p>
</div>
<p>In particular, <span class="math inline">\(\gamma_{0,t} = \mathbb{V}ar(y_t)\)</span>.</p>
<div class="definition">
<p><span id="def:covstat" class="definition"><strong>Definition 8.4  (Covariance stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is covariance stationary —or weakly stationary— if, for all <span class="math inline">\(t\)</span> and <span class="math inline">\(j\)</span>,
<span class="math display">\[
\mathbb{E}(y_t) = \mu \quad \mbox{and} \quad \mathbb{E}\{(y_t - \mu)(y_{t-j} - \mu)\} = \gamma_j.
\]</span></p>
</div>
<p>Figure <a href="TS.html#fig:nonstat1">8.1</a> displays the simulation of a process that is not covariance stationary. This process follows <span class="math inline">\(y_t = 0.1t + \varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim\,i.i.d.\,\mathcal{N}(0,1)\)</span>. Indeed, for such a process, we have: <span class="math inline">\(\mathbb{E}(y_t)=0.1t\)</span>, which depends on <span class="math inline">\(t\)</span>.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:nonstat1"></span>
<img src="EcoStat_files/figure-html/nonstat1-1.png" alt="Example of a process that is not covariance stationary ($y_t = 0.1t + \varepsilon_t$, where $\varepsilon_t \sim \mathcal{N}(0,1)$)." width="95%"><p class="caption">
Figure 8.1: Example of a process that is not covariance stationary (<span class="math inline">\(y_t = 0.1t + \varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,1)\)</span>).
</p>
</div>
<div class="definition">
<p><span id="def:strictstat" class="definition"><strong>Definition 8.5  (Strict stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is strictly stationary if, for all <span class="math inline">\(t\)</span> and all sets of integers <span class="math inline">\(J=\{j_1,\dots,j_n\}\)</span>, the distribution of <span class="math inline">\((y_{t},y_{t+j_1},\dots,y_{t+j_n})\)</span> depends on <span class="math inline">\(J\)</span> but not on <span class="math inline">\(t\)</span>.</p>
</div>
<p>The following process is covariance stationary but not strictly stationary:
<span class="math display">\[
y_t = \mathbb{I}_{\{t&lt;1000\}}\varepsilon_{1,t}+\mathbb{I}_{\{t\ge1000\}}\varepsilon_{2,t},
\]</span>
where <span class="math inline">\(\varepsilon_{1,t} \sim \mathcal{N}(0,1)\)</span> and <span class="math inline">\(\varepsilon_{2,t} \sim \sqrt{\frac{\nu - 2}{\nu}} t(\nu)\)</span> and <span class="math inline">\(\nu = 4\)</span>.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:nonstat2"></span>
<img src="EcoStat_files/figure-html/nonstat2-1.png" alt="Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99\% confidence interval of the standard normal distribution ($\pm 2.58$)." width="95%"><p class="caption">
Figure 8.2: Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99% confidence interval of the standard normal distribution (<span class="math inline">\(\pm 2.58\)</span>).
</p>
</div>
<div class="proposition">
<p><span id="prp:gammaMinus" class="proposition"><strong>Proposition 8.1  </strong></span>If <span class="math inline">\(y_t\)</span> is covariance stationary, then <span class="math inline">\(\gamma_j = \gamma_{-j}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-33" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(y_t\)</span> is covariance stationary, the covariance between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-j}\)</span> (i.e <span class="math inline">\(\gamma_j\)</span>) is the same as that between <span class="math inline">\(y_{t+j}\)</span> and <span class="math inline">\(y_{t+j-j}\)</span> (i.e. <span class="math inline">\(\gamma_{-j}\)</span>).</p>
</div>
<div class="definition">
<p><span id="def:autocor" class="definition"><strong>Definition 8.6  (Auto-correlation) </strong></span>The <span class="math inline">\(j^{th}\)</span> auto-correlation of a covariance-stationary process is:
<span class="math display">\[
\rho_j = \frac{\gamma_j}{\gamma_0}.
\]</span></p>
</div>
<p>Consider a long historical time series of the Swiss GDP growth, taken from the <span class="citation">Jordà, Schularick, and Taylor (<a href="references.html#ref-JST_2017">2017</a>)</span> dataset.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Version 6 of the dataset, available on &lt;a href="https://www.macrohistory.net"&gt;this website&lt;/a&gt;.&lt;/p&gt;'><sup>18</sup></a></p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:autocov"></span>
<img src="EcoStat_files/figure-html/autocov-1.png" alt="Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database." width="95%"><p class="caption">
Figure 8.3: Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database.
</p>
</div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:autocov2"></span>
<img src="EcoStat_files/figure-html/autocov2-1.png" alt="For order $j$, the slope of the blue line is, approximately, $\hat{\gamma}_j/\widehat{\mathbb{V}ar}(y_t)$, where hats indicate sample moments." width="95%"><p class="caption">
Figure 8.4: For order <span class="math inline">\(j\)</span>, the slope of the blue line is, approximately, <span class="math inline">\(\hat{\gamma}_j/\widehat{\mathbb{V}ar}(y_t)\)</span>, where hats indicate sample moments.
</p>
</div>
<div class="definition">
<p><span id="def:ergodicity" class="definition"><strong>Definition 8.7  (Mean ergodicity) </strong></span>The covariance-stationary process <span class="math inline">\(y_t\)</span> is ergodic for the mean if:
<span class="math display">\[
\mbox{plim}_{T \rightarrow +\infty} \frac{1}{T}\sum_{t=1}^T y_t = \mathbb{E}(y_t).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:ergod2nd" class="definition"><strong>Definition 8.8  (Second-moment ergodicity) </strong></span>The covariance-stationary process <span class="math inline">\(y_t\)</span> is ergodic for second moments if, for all <span class="math inline">\(j\)</span>:
<span class="math display">\[
\mbox{plim}_{T \rightarrow +\infty} \frac{1}{T}\sum_{t=1}^T (y_t-\mu) (y_{t-j}-\mu) = \gamma_j.
\]</span></p>
</div>
<p>It should be noted that ergodicity and stationarity are different properties. Typically if the process <span class="math inline">\(\{x_t\}\)</span> is such that, <span class="math inline">\(\forall t\)</span>, <span class="math inline">\(x_t \equiv y\)</span>, where <span class="math inline">\(y \sim\,\mathcal{N}(0,1)\)</span> (say), then <span class="math inline">\(\{x_t\}\)</span> is stationary but not ergodic.</p>
<div class="theorem">
<p><span id="thm:CLTcovstat" class="theorem"><strong>Theorem 8.1  (Central Limit Theorem for covariance-stationary processes) </strong></span>If process <span class="math inline">\(y_t\)</span> is covariance stationary and if the series of autocovariances is absolutely summable (<span class="math inline">\(\sum_{j=-\infty}^{+\infty} |\gamma_j| &lt;\infty\)</span>), then:
<span class="math display" id="eq:TCL4ts">\[\begin{eqnarray}
\bar{y}_T \overset{m.s.}{\rightarrow} \mu &amp;=&amp; \mathbb{E}(y_t) \tag{8.2}\\
\mbox{lim}_{T \rightarrow +\infty} T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] &amp;=&amp; \sum_{j=-\infty}^{+\infty} \gamma_j \tag{8.3}\\
\sqrt{T}(\bar{y}_T - \mu) &amp;\overset{d}{\rightarrow}&amp; \mathcal{N}\left(0,\sum_{j=-\infty}^{+\infty} \gamma_j \right) \tag{8.4}.
\end{eqnarray}\]</span></p>
<p>[Mean square (m.s.) and distribution (d.) convergences: see Definitions <a href="append.html#def:cvgceDistri">9.17</a> and <a href="append.html#def:convergenceLr">9.15</a>.]</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-34" class="proof"><em>Proof</em>. </span>By Proposition <a href="append.html#prp:absMs">9.8</a>, Eq. <a href="TS.html#eq:TCL2">(8.3)</a> implies Eq. <a href="TS.html#eq:TCL20">(8.2)</a>. For Eq. <a href="TS.html#eq:TCL2">(8.3)</a>, see Appendix <a href="append.html#AppendixProof">9.5</a>. For Eq. <a href="TS.html#eq:TCL4ts">(8.4)</a>, see <span class="citation">Anderson (<a href="references.html#ref-Anderson_1971">1971</a>)</span>, p. 429.</p>
</div>
<div class="definition">
<p><span id="def:LRV" class="definition"><strong>Definition 8.9  (Long-run variance) </strong></span>Under the assumptions of Theorem <a href="TS.html#thm:CLTcovstat">8.1</a>, the limit appearing in Eq. <a href="TS.html#eq:TCL2">(8.3)</a> exists and is called <strong>long-run variance</strong>. It is denoted by <span class="math inline">\(S\)</span>, i.e.:
<span class="math display">\[
S = \Sigma_{j=-\infty}^{+\infty} \gamma_j  = \mbox{lim}_{T \rightarrow +\infty} T \mathbb{E}[(\bar{y}_T - \mu)^2].
\]</span></p>
</div>
<p>If <span class="math inline">\(y_t\)</span> is ergodic for second moments (see Def. <a href="TS.html#def:ergod2nd">8.8</a>), a natural estimator of <span class="math inline">\(S\)</span> is:
<span class="math display" id="eq:covSmplMean">\[\begin{equation}
\hat\gamma_0 + 2 \sum_{\nu=1}^{q} \hat\gamma_\nu, \tag{8.5}
\end{equation}\]</span>
where <span class="math inline">\(\hat\gamma_\nu = \frac{1}{T}\sum_{\nu+1}^{T} (y_t - \bar{y})(y_{t-\nu} - \bar{y})\)</span>.</p>
<p>However, for small samples, Eq. <a href="TS.html#eq:covSmplMean">(8.5)</a> does not necessarily result in a positive definite matrix. <span class="citation">Newey and West (<a href="references.html#ref-Newey_West_1987">1987</a>)</span> have proposed an estimator that does not have this defect. Their estimator is given by:
<span class="math display" id="eq:NWest">\[\begin{equation}
S^{NW}=\hat\gamma_0 + 2 \sum_{\nu=1}^{q}\left(1-\frac{\nu}{q+1}\right) \hat\gamma_\nu.\tag{8.6}
\end{equation}\]</span></p>
<p>Loosely speaking, Theorem <a href="TS.html#thm:CLTcovstat">8.1</a> says that, for a given sample size, the higher the “persistency” of a process, the lower the accuracy of the sample mean as an estimate of the population mean. To illustrate, consider three processes that feature the same marginal variance (equal to one, say), but different autocorrelations: 0%, 70%, and 99.9%. Figure <a href="TS.html#fig:TVTCL">8.5</a> displays simulated paths of such three processes. It indeed appears that, the larger the autocorrelation of the process, the further the sample mean (dashed red line) from the population mean (red solid line).</p>
<p>The same type of simulations can be performed using <a href="https://jrenne.shinyapps.io/MacroEc/">this ShinyApp</a> (use panel “AR(1) Simulation”).</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:TVTCL"></span>
<img src="EcoStat_files/figure-html/TVTCL-1.png" alt="The three samples have been simulated using the following data generating process: $x_t = \mu + \rho (x_{t-1}-\mu) + \sqrt{1-\rho^2}\varepsilon_t$, where $\varepsilon_t \sim \mathcal{N}(0,1)$. Case A: $\rho = 0$;  Case B: $\rho = 0.7$;  Case C: $\rho = 0.999$. In the three cases, $\mathbb{E}(x_t)=\mu=2$ and $\mathbb{V}ar(x_t)=1$." width="100%"><p class="caption">
Figure 8.5: The three samples have been simulated using the following data generating process: <span class="math inline">\(x_t = \mu + \rho (x_{t-1}-\mu) + \sqrt{1-\rho^2}\varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,1)\)</span>. Case A: <span class="math inline">\(\rho = 0\)</span>; Case B: <span class="math inline">\(\rho = 0.7\)</span>; Case C: <span class="math inline">\(\rho = 0.999\)</span>. In the three cases, <span class="math inline">\(\mathbb{E}(x_t)=\mu=2\)</span> and <span class="math inline">\(\mathbb{V}ar(x_t)=1\)</span>.
</p>
</div>
</div>
<div id="univariate-processes" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Univariate processes<a class="anchor" aria-label="anchor" href="#univariate-processes"><i class="fas fa-link"></i></a>
</h2>
<div id="moving-average-ma-processes" class="section level3" number="8.2.1">
<h3>
<span class="header-section-number">8.2.1</span> Moving Average (MA) processes<a class="anchor" aria-label="anchor" href="#moving-average-ma-processes"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:unlabeled-div-35" class="definition"><strong>Definition 8.10  </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (Def. <a href="TS.html#def:whitenoise">8.1</a>). Then <span class="math inline">\(y_t\)</span> is a first-order moving average process if, for all <span class="math inline">\(t\)</span>:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}.
\]</span></p>
</div>
<p>If <span class="math inline">\(\mathbb{E}(\varepsilon_t^2)=\sigma^2\)</span>, it is easily obtained that the unconditional mean and variances of <span class="math inline">\(y_t\)</span> are:
<span class="math display">\[
\mathbb{E}(y_t) = \mu, \quad \mathbb{V}ar(y_t) = (1+\theta^2)\sigma^2.
\]</span></p>
<p>The first auto-covariance is:
<span class="math display">\[
\gamma_1=\mathbb{E}\{(y_t - \mu)(y_{t-1} - \mu)\} = \theta \sigma^2.
\]</span></p>
<p>Higher-order auto-covariances are zero (<span class="math inline">\(\gamma_j=0\)</span> for <span class="math inline">\(j&gt;1\)</span>). Therefore: An MA(1) process is covariance-stationary (Def. <a href="TS.html#def:covstat">8.4</a>).</p>
<p>For a MA(1) process, the autocorrelation of order <span class="math inline">\(j\)</span> (see Def. <a href="TS.html#def:autocor">8.6</a>) is given by:
<span class="math display">\[
\rho_j =
\left\{
\begin{array}{lll}
1 &amp;\mbox{ if }&amp; j=0,\\
\theta / (1 + \theta^2) &amp;\mbox{ if }&amp; j = 1\\
0 &amp;\mbox{ if }&amp; j&gt;1.
\end{array}
\right.
\]</span></p>
<p>Notice that process <span class="math inline">\(y_t\)</span> defined through:
<span class="math display">\[
y_t = \mu + \varepsilon_t +\theta \varepsilon_{t-1},
\]</span>
where <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=\sigma^2\)</span>, has the same mean and autocovariances as
<span class="math display">\[
y_t = \mu + \varepsilon^*_t +\frac{1}{\theta}\varepsilon^*_{t-1},
\]</span>
where <span class="math inline">\(\mathbb{V}ar(\varepsilon^*_t)=\theta^2\sigma^2\)</span>. That is, even if we perfectly know the mean and auto-covariances of this process, it is not possible to identify which specification is the one that has been used to generate the data. Only one of these two specifications is said to be <em>fundamental</em>, that is the one that satisfies <span class="math inline">\(|\theta_1|&lt;1\)</span>.</p>
<div class="definition">
<p><span id="def:MAq" class="definition"><strong>Definition 8.11  (MA(q) process) </strong></span>A <span class="math inline">\(q^{th}\)</span> order Moving Average process is defined through:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}.
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process (Def. <a href="TS.html#def:whitenoise">8.1</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:covMAq" class="proposition"><strong>Proposition 8.2  (Covariance-stationarity of an MA(q) process) </strong></span>Finite-order Moving Average processes are covariance-stationary.</p>
<p>Moreover, the autocovariances of an MA(q) process (as defined in Def. <a href="TS.html#def:MAq">8.11</a>) are given by:
<span class="math display" id="eq:autocovMA">\[\begin{equation}
\gamma_j = \left\{ \begin{array}{ll} \sigma^2(\theta_j\theta_0 + \theta_{j+1}\theta_{1} +  \dots + \theta_{q}\theta_{q-j}) &amp;\mbox{for} \quad j \in \{0,\dots,q\} \\ 0 &amp;\mbox{for} \quad j&gt;q, \end{array} \right.\tag{8.7}
\end{equation}\]</span>
where we use the notation <span class="math inline">\(\theta_0=1\)</span>, and <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=\sigma^2\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-36" class="proof"><em>Proof</em>. </span>The unconditional expectation of <span class="math inline">\(y_t\)</span> does not depend on time, since <span class="math inline">\(\mathbb{E}(y_t)=\mu\)</span>. Let’s turn to autocovariances. We can extend the series of the <span class="math inline">\(\theta_j\)</span>’s by setting <span class="math inline">\(\theta_j=0\)</span> for <span class="math inline">\(j&gt;q\)</span>. We then have:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}((y_t-\mu)(y_{t-j}-\mu)) &amp;=&amp; \mathbb{E}\left[(\theta_0 \varepsilon_t +\theta_1 \varepsilon_{t-1} + \dots +\theta_j \varepsilon_{t-j}+\theta_{j+1} \varepsilon_{t-j-1} + \dots) \right.\times \\
&amp;&amp;\left. (\theta_0 \varepsilon_{t-j} +\theta_1 \varepsilon_{t-j-1} + \dots)\right].
\end{eqnarray*}\]</span>
Then use the fact that <span class="math inline">\(\mathbb{E}(\varepsilon_t\varepsilon_s)=0\)</span> if <span class="math inline">\(t \ne s\)</span> (because <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process).</p>
</div>
<p>Figure <a href="TS.html#fig:simMA">8.6</a> displays simulated paths of two MA processes (an MA(1) and an MA(4)). Such simulations can be produced by using panel “ARMA(p,q)” of <a href="https://jrenne.shinyapps.io/MacroEc/">this web interface</a>.</p>
<div class="sourceCode" id="cb128"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">100</span>;<span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">y.0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">c</span> <span class="op">&lt;-</span> <span class="fl">1</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>;<span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span> <span class="co"># MA(1) specification</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,<span class="va">sigma</span>,<span class="cn">T</span>,<span class="va">y.0</span>,<span class="va">nb.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.2</span>,<span class="fl">.9</span>,<span class="fl">.2</span>,<span class="fl">.85</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>,<span class="st">"=1, "</span>,<span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,<span class="st">"=1"</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="va">c</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span> <span class="co"># MA(4) specification</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,<span class="va">sigma</span>,<span class="cn">T</span>,<span class="va">y.0</span>,<span class="va">nb.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>,<span class="st">"=...="</span>,<span class="va">theta</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span>,<span class="st">"=1"</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="va">c</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:simMA"></span>
<img src="EcoStat_files/figure-html/simMA-1.png" alt="Simulation of MA processes." width="95%"><p class="caption">
Figure 8.6: Simulation of MA processes.
</p>
</div>
<p>What if the order <span class="math inline">\(q\)</span> of an MA(q) process gets infinite? The notion of <strong>infinite-order Moving Average process</strong> exists and is important in time series analysis. The (infinite) sequence of <span class="math inline">\(\theta_j\)</span> has to satisfy some conditions for such a process to be well-defined (see Theorem <a href="TS.html#thm:infMA">8.2</a> below). These conditions relate to the “summability” of <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> (see Definition <a href="TS.html#def:summability">8.12</a>).</p>
<div class="definition">
<p><span id="def:summability" class="definition"><strong>Definition 8.12  (Absolute and square summability) </strong></span>The sequence <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is absolutely summable if <span class="math inline">\(\sum_{i=0}^{\infty}|\theta_i| &lt; + \infty\)</span>, and it is square summable if <span class="math inline">\(\sum_{i=0}^{\infty} \theta_i^2 &lt; + \infty\)</span>.</p>
</div>
<p>According to Prop. <a href="append.html#prp:absMs">9.8</a>, absolute summability implies square summability.</p>
<div class="theorem">
<p><span id="thm:infMA" class="theorem"><strong>Theorem 8.2  (Existence condition for an infinite MA process) </strong></span>If <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is square summable (see Def. <a href="TS.html#def:summability">8.12</a>) and if <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process (see Def. <a href="TS.html#def:whitenoise">8.1</a>), then
<span class="math display">\[
\mu + \sum_{i=0}^{+\infty} \theta_{i} \varepsilon_{t-i}
\]</span>
defines a well-behaved [covariance-stationary] process, called infinite-order MA process (MA(<span class="math inline">\(\infty\)</span>)).</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-37" class="proof"><em>Proof</em>. </span>See Appendix 3.A in Hamilton. “Well behaved” means that <span class="math inline">\(\Sigma_{i=0}^{T} \theta_{t-i} \varepsilon_{t-i}\)</span> converges in mean square (Def. <a href="append.html#def:convergenceLr">9.15</a>) to some random variable <span class="math inline">\(Z_t\)</span>. The proof makes use of the fact that:
<span class="math display">\[
\mathbb{E}\left[\left(\sum_{i=N}^{M}\theta_{i} \varepsilon_{t-i}\right)^2\right] = \sum_{i=N}^{M}|\theta_{i}|^2 \sigma^2,
\]</span>
and that, when <span class="math inline">\(\{\theta_{i}\}\)</span> is square summable, <span class="math inline">\(\forall \eta&gt;0\)</span>, <span class="math inline">\(\exists N\)</span> s.t. the right-hand-side term in the last equation is lower than <span class="math inline">\(\eta\)</span> for all <span class="math inline">\(M \ge N\)</span> (static Cauchy criterion, Theorem <a href="append.html#thm:cauchycritstatic">9.2</a>). This implies that <span class="math inline">\(\Sigma_{i=0}^{T} \theta_{i} \varepsilon_{t-i}\)</span> converges in mean square (stochastic Cauchy criterion, see Theorem <a href="append.html#thm:cauchycritstochastic">9.3</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:momentsMAinf" class="proposition"><strong>Proposition 8.3  (First two moments of an infinite MA process) </strong></span>If <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is absolutely summable, i.e. if <span class="math inline">\(\sum_{i=0}^{\infty}|\theta_i| &lt; + \infty\)</span>, then</p>
<ol style="list-style-type: lower-roman">
<li>
<span class="math inline">\(y_t = \mu + \sum_{i=0}^{+\infty} \theta_{i} \varepsilon_{t-i}\)</span> exists (Theorem <a href="TS.html#thm:infMA">8.2</a>) and is such that:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(y_t) &amp;=&amp; \mu\\
\gamma_0 = \mathbb{E}([y_t-\mu]^2) &amp;=&amp; \sigma^2(\theta_0^2 +\theta_1^2 + \dots)\\
\gamma_j = \mathbb{E}([y_t-\mu][y_{t-j}-\mu]) &amp;=&amp; \sigma^2(\theta_0\theta_j + \theta_{1}\theta_{j+1} + \dots).
\end{eqnarray*}\]</span>
</li>
<li>Process <span class="math inline">\(y_t\)</span> has absolutely summable auto-covariances, which implies that the results of Theorem <a href="TS.html#thm:CLTcovstat">8.1</a> (Central Limit) apply.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-38" class="proof"><em>Proof</em>. </span>The absolute summability of <span class="math inline">\(\{\theta_{i}\}\)</span> and the fact that <span class="math inline">\(\mathbb{E}(\varepsilon^2)&lt;\infty\)</span> imply that the order of integration and summation is interchangeable (see Hamilton, 1994, Footnote p. 52), which proves (i). For (ii), see end of Appendix 3.A in Hamilton (1994).</p>
</div>
</div>
<div id="ARsection" class="section level3" number="8.2.2">
<h3>
<span class="header-section-number">8.2.2</span> Auto-Regressive (AR) processes<a class="anchor" aria-label="anchor" href="#ARsection"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:AR1" class="definition"><strong>Definition 8.13  (First-order AR process (AR(1))) </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (see Def. <a href="TS.html#def:whitenoise">8.1</a>). Process <span class="math inline">\(y_t\)</span> is an AR(1) process if it is defined by the following difference equation:
<span class="math display">\[
y_t = c + \phi y_{t-1} + \varepsilon_t.
\]</span></p>
</div>
<p>That kind of process can be simulated by using panel “AR(1) Simulation” of <a href="https://jrenne.shinyapps.io/MacroEc/">this web interface</a>.</p>
<p>If <span class="math inline">\(|\phi|\ge1\)</span>, <span class="math inline">\(y_t\)</span> is not stationary. Indeed, we have:
<span class="math display">\[
y_{t+k} = c + \varepsilon_{t+k} + \phi  ( c + \varepsilon_{t+k-1})+ \phi^2  ( c + \varepsilon_{t+k-2})+ \dots + \phi^{k-1}  ( c + \varepsilon_{t+1}) + \phi^k y_t.
\]</span>
Therefore, the conditional variance
<span class="math display">\[
\mathbb{V}ar_t(y_{t+k}) = \sigma^2(1 + \phi^2 + \phi^4 + \dots + \phi^{2(k-1)})
\]</span>
does not converge for large <span class="math inline">\(k\)</span>’s. This implies that <span class="math inline">\(\mathbb{V}ar(y_{t})\)</span> does not exist.</p>
<p>By contrast, if <span class="math inline">\(|\phi| &lt; 1\)</span>, one can see that:
<span class="math display">\[
y_t = c + \varepsilon_t + \phi  ( c + \varepsilon_{t-1})+ \phi^2  ( c + \varepsilon_{t-2})+ \dots + \phi^k  ( c + \varepsilon_{t-k}) + \dots
\]</span>
Hence, if <span class="math inline">\(|\phi| &lt; 1\)</span>, the unconditional mean and variance of <span class="math inline">\(y_t\)</span> are:
<span class="math display">\[
\mathbb{E}(y_t) = \frac{c}{1-\phi} =: \mu \quad \mbox{and} \quad \mathbb{V}ar(y_t) = \frac{\sigma^2}{1-\phi^2}.
\]</span></p>
<p>Let us compute the <span class="math inline">\(j^{th}\)</span> autocovariance of the AR(1) process:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}([y_{t} - \mu][y_{t-j} - \mu]) &amp;=&amp; \mathbb{E}([\varepsilon_t + \phi  \varepsilon_{t-1}+ \phi^2 \varepsilon_{t-2} + \dots + \color{red}{\phi^j \varepsilon_{t-j}} + \color{blue}{\phi^{j+1} \varepsilon_{t-j-1}} \dots]\times \\
&amp;&amp;[\color{red}{\varepsilon_{t-j}} + \color{blue}{\phi \varepsilon_{t-j-1}} + \phi^2 \varepsilon_{t-j-2} + \dots + \phi^k \varepsilon_{t-j-k} + \dots])\\
&amp;=&amp; \mathbb{E}(\color{red}{\phi^j \varepsilon_{t-j}^2}+\color{blue}{\phi^{j+2} \varepsilon_{t-j-1}^2}+\phi^{j+4} \varepsilon_{t-j-2}^2+\dots)\\
&amp;=&amp; \frac{\phi^j \sigma^2}{1 - \phi^2}.
\end{eqnarray*}\]</span></p>
<p>Therefore <span class="math inline">\(\rho_j = \phi^j\)</span>.</p>
<p>By what precedes, we have:</p>
<div class="proposition">
<p><span id="prp:statioAR1" class="proposition"><strong>Proposition 8.4  (Covariance-stationarity of an AR(1) process) </strong></span>The AR(1) process, as defined in Def. <a href="TS.html#def:AR1">8.13</a>, is covariance-stationary iff <span class="math inline">\(|\phi|&lt;1\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:ARp" class="definition"><strong>Definition 8.14  (AR(p) process) </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (see Def. <a href="TS.html#def:whitenoise">8.1</a>). Process <span class="math inline">\(y_t\)</span> is a <span class="math inline">\(p^{th}\)</span>-order autoregressive process (AR(p)) if its dynamics is defined by the following difference equation (with <span class="math inline">\(\phi_p \ne 0\)</span>):
<span class="math display" id="eq:AR">\[\begin{equation}
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t.\tag{8.8}
\end{equation}\]</span></p>
</div>
<p>As we will see, the covariance-stationarity of process <span class="math inline">\(y_t\)</span> hinges on matrix <span class="math inline">\(F\)</span> defined as:
<span class="math display" id="eq:F">\[\begin{equation}
F = \left[
\begin{array}{ccccc}
\phi_1 &amp; \phi_2 &amp; \dots&amp; &amp; \phi_p \\
1 &amp; 0 &amp;\dots &amp;&amp; 0 \\
0 &amp; 1 &amp;\dots &amp;&amp; 0 \\
\vdots &amp;  &amp; \ddots &amp;&amp; \vdots \\
0 &amp; 0 &amp;\dots &amp;1&amp; 0 \\
\end{array}
\right].\tag{8.9}
\end{equation}\]</span></p>
<p>Note that this matrix <span class="math inline">\(F\)</span> is such that if <span class="math inline">\(y_t\)</span> follows Eq. <a href="TS.html#eq:AR">(8.8)</a>, then process <span class="math inline">\(\mathbf{y}_t\)</span> follows:
<span class="math display">\[
\mathbf{y}_t = \mathbf{c} + F \mathbf{y}_{t-1} + \boldsymbol\xi_t
\]</span>
with
<span class="math display">\[
\mathbf{c} =
\left[\begin{array}{c}
c\\
0\\
\vdots\\
0
\end{array}\right],
\quad
\boldsymbol\xi_t =
\left[\begin{array}{c}
\varepsilon_t\\
0\\
\vdots\\
0
\end{array}\right],
\quad
\mathbf{y}_t =
\left[\begin{array}{c}
y_t\\
y_{t-1}\\
\vdots\\
y_{t-p+1}
\end{array}\right].
\]</span></p>
<!-- :::{.definition #dynmult name="Dynamic multiplier"} -->
<!-- The **dynamic multiplier** of $y_{t+j}$ w.r.t. $\varepsilon_{t}$ is $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}}$. -->
<!-- ::: -->
<!-- Eq. \@ref(eq:Fyt) implies that we have: $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = (F^j)_{[1,1]}$ -->
<!-- (for any $M,i,j$, $(M)_{[i,j]}$ denotes the $(i,j)$ element of matrix $M$). -->
<!-- Let us assume that the eigenvalues of $F$ (see Def. \@ref(def:determinant)), denoted by $\lambda_1,\dots,\lambda_p$, are distinct. -->
<!-- Then, there exists a nonsingular matrix $P$ such that: -->
<!-- $$ -->
<!-- F = P -->
<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- \lambda_1 & 0 & \dots& 0 \\ -->
<!-- 0 & \lambda_2 & 0 & \dots \\ -->
<!-- &&\ddots\\ -->
<!-- 0 & \dots & 0& \lambda_p\\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- P^{-1} = P D P^{-1}. -->
<!-- $$ -->
<!-- It can be seen that: -->
<!-- $$ -->
<!-- F^j = P -->
<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- \lambda_1^j & 0 & \dots& 0 \\ -->
<!-- 0 & \lambda_2^j & 0 & \dots \\ -->
<!-- &&\ddots\\ -->
<!-- 0 & \dots & 0& \lambda_p^j\\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- P^{-1}. -->
<!-- $$ -->
<!-- Hence, the dynamic multiplier of $y_{t+j}$ w.r.t. $\varepsilon_{t}$ is given by: -->
<!-- $$ -->
<!-- \dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = \sum_i (P)_{[1,i]}(P^{-1})_{[i,1]}\lambda_i^j. -->
<!-- $$ -->
<!-- Denoting by $c_i$ the scalar $(P)_{[1,i]}(P^{-1})_{[i,1]}$, we have $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = \sum_i c_i\lambda_i^j$. If the eigenvalues of $F$ are distinct and nonzero, then $c_i \ne 0$. Therefore, if $\exists i$ s.t. $|\lambda_i|>1$ then: -->
<!-- $$ -->
<!-- \left| \dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} \right| \underset{j \rightarrow \infty}{\rightarrow} \infty. -->
<!-- $$ -->
<div class="proposition">
<p><span id="prp:Feigen" class="proposition"><strong>Proposition 8.5  (The eigenvalues of matrix F) </strong></span>The eigenvalues of <span class="math inline">\(F\)</span> (defined by Eq. <a href="TS.html#eq:F">(8.9)</a>) are the solutions of:
<span class="math display" id="eq:Feigen">\[\begin{equation}
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_{p-1}\lambda - \phi_p = 0.\tag{8.10}
\end{equation}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:stability" class="proposition"><strong>Proposition 8.6  (Covariance-stationarity of an AR(p) process) </strong></span>These four statements are equivalent:</p>
<ol style="list-style-type: lower-roman">
<li>Process <span class="math inline">\(\{y_t\}\)</span>, defined in Def. <a href="TS.html#def:ARp">8.14</a>, is covariance-stationary.</li>
<li>The eigenvalues of <span class="math inline">\(F\)</span> (as defined Eq. <a href="TS.html#eq:F">(8.9)</a>) lie strictly within the unit circle.</li>
<li>The roots of Eq. <a href="TS.html#eq:outside">(8.11)</a> (below) lie strictly outside the unit circle.
<span class="math display" id="eq:outside">\[\begin{equation}
1 - \phi_1 z - \dots - \phi_{p-1}z^{p-1} - \phi_p z^p = 0.\tag{8.11}
\end{equation}\]</span>
</li>
<li>The roots of Eq. <a href="TS.html#eq:inside">(8.12)</a> (below) lie strictly inside the unit circle.
<span class="math display" id="eq:inside">\[\begin{equation}
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_{p-1}\lambda - \phi_p = 0.\tag{8.12}
\end{equation}\]</span>
</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-39" class="proof"><em>Proof</em>. </span>We consider the case where the eigenvalues of <span class="math inline">\(F\)</span> are distinct; Jordan decomposition can be used in the general case. When the eigenvalues of <span class="math inline">\(F\)</span> are distinct, <span class="math inline">\(F\)</span> admits the following spectral decomposition: <span class="math inline">\(F = PDP^{-1}\)</span>, where <span class="math inline">\(D\)</span> is diagonal. Using the notations introduced in Eq. <a href="TS.html#eq:F">(8.9)</a>, we have:
<span class="math display">\[
\mathbf{y}_{t} = \mathbf{c} + F \mathbf{y}_{t-1} + \boldsymbol\xi_{t}.
\]</span>
Let’s introduce <span class="math inline">\(\mathbf{d} = P^{-1}\mathbf{c}\)</span>, <span class="math inline">\(\mathbf{z}_t = P^{-1}\mathbf{y}_t\)</span> and <span class="math inline">\(\boldsymbol\eta_t = P^{-1}\boldsymbol\xi_t\)</span>. We have:
<span class="math display">\[
\mathbf{z}_{t} = \mathbf{d} + D \mathbf{z}_{t-1} + \boldsymbol\eta_{t}.
\]</span>
Because <span class="math inline">\(D\)</span> is diagonal, the different component of <span class="math inline">\(\mathbf{z}_t\)</span>, denoted by <span class="math inline">\(z_{i,t}\)</span>, follow AR(1) processes. The (scalar) autoregressive parameters of these AR(1) processes are the diagonal entries of <span class="math inline">\(D\)</span> –which also are the eigenvalues of <span class="math inline">\(F\)</span>– that we denote by <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Process <span class="math inline">\(y_t\)</span> is covariance-stationary iff <span class="math inline">\(\mathbf{y}_{t}\)</span> also is covariance-stationary, which is the case iff all <span class="math inline">\(z_{i,t}\)</span>, <span class="math inline">\(i \in [1,p]\)</span>, are covariance-stationary. By Prop. <a href="TS.html#prp:statioAR1">8.4</a>, process <span class="math inline">\(z_{i,t}\)</span> is covariance-stationary iff <span class="math inline">\(|\lambda_i|&lt;1\)</span>. This proves that (i) is equivalent to (ii). Prop. <a href="TS.html#prp:Feigen">8.5</a> further proves that (ii) is equivalent to (iv). Finally, it is easily seen that (iii) is equivalent to (iv) (as long as <span class="math inline">\(\phi_p \ne 0\)</span>).</p>
</div>
<!-- Note that we have: -->
<!-- \begin{equation} -->
<!-- \bv{y}_{t+j} = \boldsymbol\xi_{t+j} + F \boldsymbol\xi_{t+j-1} + F^2 \boldsymbol\xi_{t+j-2} + \dots + F^{j-1} \boldsymbol\xi_{t+1} + F^{j} \bv{y}_{t} -->
<!-- \end{equation} -->
<!-- or -->
<!-- \begin{equation} -->
<!-- \bv{y}_{t+j} = \boldsymbol\xi_{t+j} + F \boldsymbol\xi_{t+j-1} + F^2 \boldsymbol\xi_{t+j-2} + \dots + F^{j} \boldsymbol\xi_{t} + F^{j+1} \bv{y}_{t-1}.(\#eq:Fyt) -->
<!-- \end{equation} -->
<p>Using the lag operator (see Eq <a href="TS.html#eq:lagOp">(8.1)</a>), if <span class="math inline">\(y_t\)</span> is a covariance-stationary AR(p) process (Def. <a href="TS.html#def:ARp">8.14</a>), we can write:
<span class="math display">\[
y_t = \mu + \psi(L)\varepsilon_t,
\]</span>
where
<span class="math display">\[\begin{equation}
\psi(L) = (1 - \phi_1 L - \dots - \phi_p L^p)^{-1},
\end{equation}\]</span>
and
<span class="math display" id="eq:EAR">\[\begin{equation}
\mu = \mathbb{E}(y_t) = \dfrac{c}{1-\phi_1 -\dots - \phi_p}.\tag{8.13}
\end{equation}\]</span></p>
<p>In the following lines of codes, we compute the eigenvalues of the <span class="math inline">\(F\)</span> matrices associated with the following processes (where <span class="math inline">\(\varepsilon_t\)</span> is a white noise):
<span class="math display">\[\begin{eqnarray*}
x_t &amp;=&amp; 0.9 x_{t-1} -0.2 x_{t-2} + \varepsilon_t\\
y_t &amp;=&amp; 1.1 y_{t-1} -0.3 y_{t-2} + \varepsilon_t\\
w_t &amp;=&amp; 1.4 w_{t-1} -0.7 w_{t-2} + \varepsilon_t\\
z_t &amp;=&amp; 0.9 z_{t-1} +0.2 z_{t-2} + \varepsilon_t
\end{eqnarray*}\]</span></p>
<div class="sourceCode" id="cb129"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">F</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.9</span>,<span class="fl">1</span>,<span class="op">-</span><span class="fl">.2</span>,<span class="fl">0</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">lambda_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.1</span>,<span class="op">-</span><span class="fl">.3</span><span class="op">)</span></span>
<span><span class="va">lambda_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.4</span>,<span class="op">-</span><span class="fl">.7</span><span class="op">)</span></span>
<span><span class="va">lambda_w</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.9</span>,<span class="fl">.2</span><span class="op">)</span></span>
<span><span class="va">lambda_z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">lambda_x</span>,<span class="va">lambda_y</span>,<span class="va">lambda_w</span>,<span class="va">lambda_z</span><span class="op">)</span></span></code></pre></div>
<pre><code>##                         [,1]                  [,2]
## lambda_x 0.500000+0.0000000i  0.4000000+0.0000000i
## lambda_y 0.600000+0.0000000i  0.5000000+0.0000000i
## lambda_w 0.700000+0.4582576i  0.7000000-0.4582576i
## lambda_z 1.084429+0.0000000i -0.1844289+0.0000000i</code></pre>
<p>The absolute values of the eigenvalues associated with process <span class="math inline">\(w_t\)</span> are both equal to 0.837. Therefore, according to Proposition <a href="TS.html#prp:stability">8.6</a>, processes <span class="math inline">\(x_t\)</span>, <span class="math inline">\(y_t\)</span>, and <span class="math inline">\(w_t\)</span> are covariance-stationary, but not <span class="math inline">\(z_t\)</span> (because the absolute value of one of the eigenvalues of the <span class="math inline">\(F\)</span> matrix associated with this process is larger than 1).</p>
<p>The computation of the autocovariances of <span class="math inline">\(y_t\)</span> is based on the so-called <strong>Yule-Walker equations</strong> (Eq. <a href="TS.html#eq:gammas">(8.14)</a>). Let’s rewrite Eq. <a href="TS.html#eq:AR">(8.8)</a>:
<span class="math display">\[
(y_t-\mu) = \phi_1 (y_{t-1}-\mu) + \phi_2 (y_{t-2}-\mu) + \dots + \phi_p (y_{t-p}-\mu) + \varepsilon_t.
\]</span>
Multiplying both sides by <span class="math inline">\(y_{t-j}-\mu\)</span> and taking expectations leads to the (Yule-Walker) equations:
<span class="math display" id="eq:gammas">\[\begin{equation}
\gamma_j = \left\{
\begin{array}{l}
\phi_1 \gamma_{j-1}+\phi_2 \gamma_{j-2}+ \dots + \phi_p \gamma_{j-p} \quad if \quad j&gt;0\\
\phi_1 \gamma_{1}+\phi_2 \gamma_{2}+ \dots + \phi_p \gamma_{p} + \sigma^2 \quad for \quad j=0.
\end{array}
\right.\tag{8.14}
\end{equation}\]</span>
Using <span class="math inline">\(\gamma_j = \gamma_{-j}\)</span> (Prop. <a href="TS.html#prp:gammaMinus">8.1</a>), one can express <span class="math inline">\((\gamma_0,\gamma_1,\dots,\gamma_{p})\)</span> as functions of <span class="math inline">\((\sigma^2,\phi_1,\dots,\phi_p)\)</span>.</p>
</div>
<div id="PACFapproach" class="section level3" number="8.2.3">
<h3>
<span class="header-section-number">8.2.3</span> PACF approach to identify AR/MA processes<a class="anchor" aria-label="anchor" href="#PACFapproach"><i class="fas fa-link"></i></a>
</h3>
<p>We have seen that the <span class="math inline">\(k^{th}\)</span>-order auto-correlation of a MA(q) process is null if <span class="math inline">\(k&gt;q\)</span>. This is exploited, in practice, to determine the order of a MA process. Moreover, since this is not the case for an AR process, this can be used to distinguish an AR from an MA process.</p>
<p>There exists an equivalent approach to determine whether a process can be modeled as an AR process; it is based on partial auto-correlations:</p>
<!-- :::{.definition #partialC name="Partial correlation"} -->
<!-- The partial correlation between $X$ and $Y$, given $Z$, is the correlation between: -->
<!-- a. the residuals of the linear regression of $X$ on $Z$ and -->
<!-- b. the residuals of the linear regression of $Y$ on $Z$. -->
<!-- ::: -->
<div class="definition">
<p><span id="def:partialAC" class="definition"><strong>Definition 8.15  (Partial auto-correlation) </strong></span>In a time series context, the partial auto-correlation (<span class="math inline">\(\phi_{h,h}\)</span>) of process <span class="math inline">\(\{y_t\}\)</span> is defined as the partial correlation of <span class="math inline">\(y_{t+h}\)</span> and <span class="math inline">\(y_t\)</span> given <span class="math inline">\(y_{t+h-1},\dots,y_{t+1}\)</span>. (see Def. <a href="append.html#def:partialcorrel">9.5</a> for the definition of partial correlation.)</p>
</div>
<p>If <span class="math inline">\(h&gt;p\)</span>, the regression of <span class="math inline">\(y_{t+h}\)</span> on <span class="math inline">\(y_{t+h-1},\dots,y_{t+1}\)</span> is:
<span class="math display">\[
y_{t+h} = c + \phi_1 y_{t+h-1}+\dots+ \phi_p  y_{t+h-p} + \varepsilon_{t+h}.
\]</span>
The residuals of the latter regressions (<span class="math inline">\(\varepsilon_{t+h}\)</span>) are uncorrelated to <span class="math inline">\(y_t\)</span>. Then the partial autocorrelation is zero for <span class="math inline">\(h&gt;p\)</span>.</p>
<p>Besides, it can be shown that <span class="math inline">\(\phi_{p,p}=\phi_p\)</span>. Hence <span class="math inline">\(\phi_{p,p}=\phi_p\)</span> but <span class="math inline">\(\phi_{h,h}=0\)</span> for <span class="math inline">\(h&gt;p\)</span>. This can be used to determine the order of an AR process. By contrast (importantly) if <span class="math inline">\(y_t\)</span> follows an MA(q) process, then <span class="math inline">\(\phi_{k,k}\)</span> asymptotically approaches zero instead of cutting off abruptly.</p>
<p>As illustrated below, functions <code>acf</code> and <code>pacf</code> can be conveniently used to employ the (P)ACF approach. (Note also the use of function <code>sim.arma</code> to simulate ARMA processes.)</p>
<div class="sourceCode" id="cb131"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.2</span>,<span class="fl">.9</span>,<span class="fl">.2</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span><span class="op">=</span><span class="fl">0</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,T<span class="op">=</span><span class="fl">1000</span>,y.0<span class="op">=</span><span class="fl">0</span>,nb.sim<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span><span class="op">=</span><span class="fl">0.9</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,T<span class="op">=</span><span class="fl">1000</span>,y.0<span class="op">=</span><span class="fl">0</span>,nb.sim<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:pacf"></span>
<img src="EcoStat_files/figure-html/pacf-1.png" alt="ACF/PACF analysis of two processes (MA process on the left, AR on the right)." width="100%"><p class="caption">
Figure 8.7: ACF/PACF analysis of two processes (MA process on the left, AR on the right).
</p>
</div>
</div>
</div>
<div id="forecasting" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Forecasting<a class="anchor" aria-label="anchor" href="#forecasting"><i class="fas fa-link"></i></a>
</h2>
<p>Forecasting has always been an important part of the time series field (<span class="citation">De Gooijer and Hyndman (<a href="references.html#ref-DEGOOIJER2006443">2006</a>)</span>). Macroeconomic forecasts are done in many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. These institutions are interested in the <strong>point estimates</strong> (<span class="math inline">\(\sim\)</span> most likely value) of the variable of interest. They also sometimes need to measure the <strong>uncertainty</strong> (<span class="math inline">\(\sim\)</span> dispersion of likely outcomes) associated to the point estimates.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;In its inflation report, the Bank of England displays charts showing the conditional distribution of future inflation, called fan charts. This fan charts show the uncertainty associated with future inflation. See &lt;a href="https://www.bankofengland.co.uk/quarterly-bulletin/1998/q1/the-inflation-report-projections-understanding-the-fan-chart"&gt;this page&lt;/a&gt;.&lt;/p&gt;'><sup>19</sup></a></p>
<p>Forecasts produced by professional forecasters are available on these web pages:</p>
<ul>
<li>
<a href="https://www.philadelphiafed.org/research-and-data/real-time-center/survey-of-professional-forecasters/">Philly Fed Survey of Professional Forecasters</a>.</li>
<li>
<a href="http://www.ecb.europa.eu/stats/prices/indic/forecast/html/index.en.html">ECB Survey of Professional Forecasters</a>.</li>
<li>
<a href="https://www.imf.org/external/pubs/ft/weo/2016/update/01/">IMF World Economic Outlook</a>.</li>
<li>
<a href="http://www.oecd.org/eco/economicoutlook.htm">OECD Global Economic Outlook</a>.</li>
<li>
<a href="http://ec.europa.eu/economy_finance/eu/forecasts/index_en.htm">European Commission Economic Forecasts</a>.</li>
</ul>
<p>How to formalize the forecasting problem? Assume the current date is <span class="math inline">\(t\)</span>. We want to forecast the value that variable <span class="math inline">\(y_t\)</span> will take on date <span class="math inline">\(t+1\)</span> (i.e., <span class="math inline">\(y_{t+1}\)</span>) based on the observation of a set of variables gathered in vector <span class="math inline">\(x_t\)</span> (<span class="math inline">\(x_t\)</span> may contain lagged values of <span class="math inline">\(y_t\)</span>).</p>
<p>The forecaster aims at minimizing (a function of) the forecast error. It is usal to consider the following (quadratic) loss function:
<span class="math display">\[
\underbrace{\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)}_{\mbox{Mean square error (MSE)}}
\]</span>
where <span class="math inline">\(y^*_{t+1}\)</span> is the forecast of <span class="math inline">\(y_{t+1}\)</span> (function of <span class="math inline">\(x_t\)</span>).</p>
<div class="proposition">
<p><span id="prp:smallestMSE" class="proposition"><strong>Proposition 8.7  (Smallest MSE) </strong></span>The smallest MSE is obtained with the expectation of <span class="math inline">\(y_{t+1}\)</span> conditional on <span class="math inline">\(x_t\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-40" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">9.5</a>.</p>
</div>
<div class="proposition" names="Smallest MSE for linear forecasts">
<p><span id="prp:smallestMSElinear" class="proposition"><strong>Proposition 8.8  </strong></span>Among the class of linear forecasts, the smallest MSE is obtained with the linear projection of <span class="math inline">\(y_{t+1}\)</span> on <span class="math inline">\(x_t\)</span>.
This projection, denoted by <span class="math inline">\(\hat{P}(y_{t+1}|x_t):=\boldsymbol\alpha'x_t\)</span>, satisfies:
<span class="math display" id="eq:proj">\[\begin{equation}
\mathbb{E}\left( [y_{t+1} - \boldsymbol\alpha'x_t]x_t \right)=\mathbf{0}.\tag{8.15}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-41" class="proof"><em>Proof</em>. </span>Consider the function <span class="math inline">\(f:\)</span> <span class="math inline">\(\boldsymbol\alpha \rightarrow \mathbb{E}\left( [y_{t+1} - \boldsymbol\alpha'x_t]^2 \right)\)</span>. We have:
<span class="math display">\[
f(\boldsymbol\alpha) = \mathbb{E}\left( y_{t+1}^2 - 2 y_t x_t'\boldsymbol\alpha + \boldsymbol\alpha'x_t x_t'\boldsymbol\alpha] \right).
\]</span>
We have <span class="math inline">\(\partial f(\boldsymbol\alpha)/\partial \boldsymbol\alpha = \mathbb{E}(-2 y_{t+1} x_t + 2 x_t x_t'\boldsymbol\alpha)\)</span>. The function is minimised for <span class="math inline">\(\partial f(\boldsymbol\alpha)/\partial \boldsymbol\alpha =0\)</span>.</p>
</div>
<p>Eq. <a href="TS.html#eq:proj">(8.15)</a> implies that <span class="math inline">\(\mathbb{E}\left( y_{t+1}x_t \right)=\mathbb{E}\left(x_tx_t' \right)\boldsymbol\alpha\)</span>. (Note that <span class="math inline">\(x_t x_t'\boldsymbol\alpha=x_t (x_t'\boldsymbol\alpha)=(\boldsymbol\alpha'x_t) x_t'\)</span>.)</p>
<p>Hence, if <span class="math inline">\(\mathbb{E}\left(x_tx_t' \right)\)</span> is nonsingular,
<span class="math display" id="eq:linproj">\[\begin{equation}
\boldsymbol\alpha=[\mathbb{E}\left(x_tx_t' \right)]^{-1}\mathbb{E}\left( y_{t+1}x_t \right).\tag{8.16}
\end{equation}\]</span></p>
<p>The MSE then is:
<span class="math display">\[
\mathbb{E}([y_{t+1} - \boldsymbol\alpha'x_t]^2) = \mathbb{E}{(y_{t+1}^2)} - \mathbb{E}\left( y_{t+1}x_t' \right)[\mathbb{E}\left(x_tx_t' \right)]^{-1}\mathbb{E}\left(x_ty_{t+1} \right).
\]</span></p>
<p>Consider the regression <span class="math inline">\(y_{t+1} = \boldsymbol\beta'\mathbf{x}_t + \varepsilon_{t+1}\)</span>. The OLS estimate is:
<span class="math display">\[
\mathbf{b} = \left[ \underbrace{ \frac{1}{T} \sum_{i=1}^T \mathbf{x}_t\mathbf{x}_t'}_{\mathbf{m}_1} \right]^{-1}\left[  \underbrace{ \frac{1}{T} \sum_{i=1}^T \mathbf{x}_t'y_{t+1}}_{\mathbf{m}_2} \right].
\]</span>
If <span class="math inline">\(\{x_t,y_t\}\)</span> is covariance-stationary and ergodic for the second moments then the sample moments (<span class="math inline">\(\mathbf{m}_1\)</span> and <span class="math inline">\(\mathbf{m}_2\)</span>) converges in probability to the associated population moments and <span class="math inline">\(\mathbf{b} \overset{p}{\rightarrow} \boldsymbol\alpha\)</span> (where <span class="math inline">\(\boldsymbol\alpha\)</span> is defined in Eq. <a href="TS.html#eq:linproj">(8.16)</a>).</p>
<div class="example">
<p><span id="exm:fcstMAq" class="example"><strong>Example 8.3  (Forecasting an MA(q) process) </strong></span>Consider the MA(q) process:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q},
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise sequence (Def. <a href="TS.html#def:whitenoise">8.1</a>).</p>
<p>We have:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The present reasoning relies on the assumption that the &lt;span class="math inline"&gt;\(\varepsilon_t\)&lt;/span&gt;’s are observed. But this is generally not the case in practice, where the &lt;span class="math inline"&gt;\(\varepsilon_t\)&lt;/span&gt;’s generally have to be estimated.&lt;/p&gt;'><sup>20</sup></a>
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots) =\\
&amp;&amp;\left\{
\begin{array}{lll}
\mu + \theta_h \varepsilon_{t} + \dots + \theta_q \varepsilon_{t-q+h}  \quad &amp;for&amp; \quad h \in [1,q]\\
\mu \quad &amp;for&amp; \quad h &gt; q
\end{array}
\right.
\end{eqnarray*}\]</span>
and
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{V}ar(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots)= \mathbb{E}\left( [y_{t+h} - \mathbb{E}(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots)]^2 \right) =\\
&amp;&amp;\left\{
\begin{array}{lll}
\sigma^2(1+\theta_1^2+\dots+\theta_{h-1}^2) \quad &amp;for&amp; \quad h \in [1,q]\\
\sigma^2(1+\theta_1^2+\dots+\theta_q^2) \quad &amp;for&amp; \quad h&gt;q.
\end{array}
\right.
\end{eqnarray*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:fcstARp" class="example"><strong>Example 8.4  (Forecasting an AR(p) process) </strong></span>(See <a href="https://jrenne.shinyapps.io/ARpFcst">this web interface</a>.) Consider the AR(p) process:
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t,
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise sequence (Def. <a href="TS.html#def:whitenoise">8.1</a>).</p>
<p>Using the notation of Eq. <a href="TS.html#eq:F">(8.9)</a>, we have:
<span class="math display">\[
\mathbf{y}_t - \boldsymbol\mu = F (\mathbf{y}_{t-1}- \boldsymbol\mu) + \boldsymbol\xi_t,
\]</span>
with <span class="math inline">\(\boldsymbol\mu = [\mu,\dots,\mu]'\)</span> (<span class="math inline">\(\mu\)</span> is defined in Eq. <a href="TS.html#eq:EAR">(8.13)</a>). Hence:
<span class="math display">\[
\mathbf{y}_{t+h} - \boldsymbol\mu = \boldsymbol\xi_{t+h} + F \boldsymbol\xi_{t+h-1} + \dots + F^{h-1} \boldsymbol\xi_{t+1} + F^h (\mathbf{y}_{t}- \mu).
\]</span>
Therefore:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\mathbf{y}_{t+h}|y_{t},y_{t-1},\dots) &amp;=&amp; \boldsymbol\mu + F^{h}(\mathbf{y}_t - \boldsymbol\mu)\\
\mathbb{V}ar\left([\mathbf{y}_{t+h} - \mathbb{E}(\mathbf{y}_{t+h}|y_{t},y_{t-1},\dots)] \right) &amp;=&amp; \Sigma + F\Sigma F' + \dots + F^{h-1}\Sigma (F^{h-1})',
\end{eqnarray*}\]</span>
where:
<span class="math display">\[
\Sigma = \left[
\begin{array}{ccc}
\sigma^2  &amp; 0&amp; \dots\\
0  &amp; 0 &amp; \\
\vdots  &amp; &amp; \ddots \\
\end{array}
\right].
\]</span></p>
<p>Alternative approach: Taking the (conditional) expectations of both sides of
<span class="math display">\[
y_{t+h} - \mu = \phi_1 (y_{t+h-1} - \mu) + \phi_2 (y_{t+h-2} - \mu) + \dots + \phi_p (y_{t-p} - \mu) + \varepsilon_{t+h},
\]</span>
we obtain:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(y_{t+h}|y_{t},y_{t-1},\dots) &amp;=&amp; \mu + \phi_1\left(\mathbb{E}[y_{t+h-1}|y_{t},y_{t-1},\dots] - \mu\right)+\\
&amp;&amp;\phi_2\left(\mathbb{E}[y_{t+h-2}|y_{t},y_{t-1},\dots] - \mu\right) + \dots +\\
&amp;&amp; \phi_p\left(\mathbb{E}[y_{t+h-p}|y_{t},y_{t-1},\dots] - \mu\right),
\end{eqnarray*}\]</span>
which can be exploited recursively.</p>
<p>The recursion begins with <span class="math inline">\(\mathbb{E}(y_{t-k}|y_{t},y_{t-1},\dots)=y_{t-k}\)</span> (for any <span class="math inline">\(k \ge 0\)</span>).</p>
</div>
<p><strong>Assessing the performances of a forecasting model</strong></p>
<p>Once one has fitted a model on a given dataset (of length <span class="math inline">\(T\)</span>, say), one compute MSE (mean square errors) to evaluate the performance of the model. But this MSE is the <strong>in-sample</strong> one. It is easy to reduce in-sample MSE. Typically, if the model is estimated by OLS, adding covariates mechanically reduces the MSE (see Props. <a href="ChapterLS.html#prp:chgeR2">4.4</a> and <a href="ChapterLS.html#prp:chgeInR2">4.5</a>). That is, even if additional data are irrelevant, the <span class="math inline">\(R^2\)</span> of the regression increases. Adding irrelevant variables increases the (in-sample) <span class="math inline">\(R^2\)</span> but is bound to increase the <strong>out-of-sample</strong> MSE.</p>
<p>Therefore, it is important to analyse <strong>out-of-sample</strong> performances of the forecasting model:</p>
<ol style="list-style-type: lower-alpha">
<li>Estimate a model on a sample of reduced size (<span class="math inline">\(1,\dots,T^*\)</span>, with <span class="math inline">\(T^*&lt;T\)</span>)</li>
<li>Use the remaining available periods (<span class="math inline">\(T^*+1,\dots,T\)</span>) to compute <strong>out-of-sample</strong> forecasting errors (and compute their MSE). In an out-of-sample exercise, it is important to make sure that the data used to produce a forecasts (as of date <span class="math inline">\(T^*\)</span>) where indeed available on date <span class="math inline">\(T^*\)</span>.</li>
</ol>
<p><strong>Diebold-Mariano test</strong></p>
<p>How to compare different forecasting approaches? <span class="citation">Diebold and Mariano (<a href="references.html#ref-Diebold_Mariano_1995">1995</a>)</span> have proposed a simple test to address this question.</p>
<p>Assume that you want to compare approaches A and B. You have historical data sets and you have implemented both approaches in the past, providing you with two sets of forecasting errors: <span class="math inline">\(\{e^{A}_t\}_{t=1,\dots,T}\)</span> and <span class="math inline">\(\{e^{B}_t\}_{t=1,\dots,T}\)</span>.</p>
<p>It may be the case that your forecasts serve a specific purpose and that, for instance, you dislike positive forecasting errors and you care less about negative errors. We assume you are able to formalise this by means of a <strong>loss function <span class="math inline">\(L(e)\)</span></strong>. For instance:</p>
<ul>
<li>If you dislike large positive errors, you may set <span class="math inline">\(L(e)=\exp(e)\)</span>.</li>
<li>If you are concerned about both positive and negative errors (indifferently), you may set <span class="math inline">\(L(e)=e^2\)</span> (standard approach).</li>
</ul>
<p>Let us define the sequence <span class="math inline">\(\{d_t\}_{t=1,\dots,T} \equiv \{L(e^{A}_t)-L(e^{B}_t)\}_{t=1,\dots,T}\)</span> and assume that this sequence is covariance stationary. We consider the following null hypothesis: <span class="math inline">\(H_0:\)</span> <span class="math inline">\(\bar{d}=0\)</span>, where <span class="math inline">\(\bar{d}\)</span> denotes the population mean of the <span class="math inline">\(d_t\)</span>s. Under <span class="math inline">\(H_0\)</span> and under the assumption of covariance-stationarity of <span class="math inline">\(d_t\)</span>, we have (Theorem <a href="TS.html#thm:CLTcovstat">8.1</a>):
<span class="math display">\[
\sqrt{T} \bar{d}_T \overset{d}{\rightarrow} \mathcal{N}\left(0,\sum_{j=-\infty}^{+\infty} \gamma_j \right),
\]</span><br>
where the <span class="math inline">\(\gamma_j\)</span>s are the autocovariances of <span class="math inline">\(d_t\)</span>.</p>
<p>Hence, assuming that <span class="math inline">\(\hat{\sigma}^2\)</span> is a consistent estimate of <span class="math inline">\(\sum_{j=-\infty}^{+\infty} \gamma_j\)</span> (for instance the one given by <span class="citation">Newey and West (<a href="references.html#ref-Newey_West_1987">1987</a>)</span>), we have, under <span class="math inline">\(H_0\)</span>:
<span class="math display">\[
DM_T := \sqrt{T}\frac{\bar{d}_T}{\sqrt{\hat{\sigma}^2}} \overset{d}{\rightarrow}  \mathcal{N}(0,1).
\]</span>
<span class="math inline">\(DM_T\)</span> is the test statistics. For a test of size <span class="math inline">\(\alpha\)</span>, the critical region is:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;This &lt;a href="https://jrenne.shinyapps.io/tests/"&gt;ShinyApp application&lt;/a&gt; illustrates the notion of statistical test (illustrating the p-value and the cirtical region, in particular).&lt;/p&gt;'><sup>21</sup></a>
<span class="math display">\[
]-\infty,-\Phi^{-1}(1-\alpha/2)] \cup [\Phi^{-1}(1-\alpha/2),+\infty[,
\]</span>
where <span class="math inline">\(\Phi\)</span> is the c.d.f. of the standard normal distribution.</p>
<div class="example">
<p><span id="exm:SwissOutOfSample" class="example"><strong>Example 8.5  (Forecasting Swiss GDP growth) </strong></span>We use a long historical time series of the Swiss GDP growth taken from the <span class="citation">Jordà, Schularick, and Taylor (<a href="references.html#ref-JST_2017">2017</a>)</span> dataset.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See &lt;a href="https://www.macrohistory.net/database/"&gt;this website&lt;/a&gt;.&lt;/p&gt;'><sup>22</sup></a></p>
<p>We want to forecast this GDP growth. We envision two specifications : an AR(1) specification (the one advocated by the AIC criteria), and an ARMA(2,2) specification. We are interested in 2-year-ahead forecasts (i.e., <span class="math inline">\(h=2\)</span> since the data are yearly).</p>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://pkg.robjhyndman.com/forecast/">forecast</a></span><span class="op">)</span></span></code></pre></div>
<pre><code>## Registered S3 method overwritten by 'quantmod':
##   method            from
##   as.zoo.data.frame zoo</code></pre>
<div class="sourceCode" id="cb134"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">JST</span>,<span class="va">iso</span><span class="op">==</span><span class="st">"CHE"</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">first.date</span> <span class="op">&lt;-</span> <span class="cn">T</span><span class="op">-</span><span class="fl">50</span></span>
<span><span class="va">e1</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>; <span class="va">e2</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>;<span class="va">h</span><span class="op">&lt;-</span><span class="fl">2</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">T.star</span> <span class="kw">in</span> <span class="va">first.date</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">h</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">estim.model.1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">T.star</span><span class="op">]</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">estim.model.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">T.star</span><span class="op">]</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">0</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">e1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">e1</span>,<span class="va">y</span><span class="op">[</span><span class="va">T.star</span><span class="op">+</span><span class="va">h</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">estim.model.1</span>,n.ahead<span class="op">=</span><span class="va">h</span><span class="op">)</span><span class="op">$</span><span class="va">pred</span><span class="op">[</span><span class="va">h</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">e2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">e2</span>,<span class="va">y</span><span class="op">[</span><span class="va">T.star</span><span class="op">+</span><span class="va">h</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">estim.model.2</span>,n.ahead<span class="op">=</span><span class="va">h</span><span class="op">)</span><span class="op">$</span><span class="va">pred</span><span class="op">[</span><span class="va">h</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">res.DM</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://pkg.robjhyndman.com/forecast/reference/dm.test.html">dm.test</a></span><span class="op">(</span><span class="va">e1</span>,<span class="va">e2</span>,h <span class="op">=</span> <span class="va">h</span>,alternative <span class="op">=</span> <span class="st">"greater"</span><span class="op">)</span></span>
<span><span class="va">res.DM</span></span></code></pre></div>
<pre><code>## 
##  Diebold-Mariano Test
## 
## data:  e1e2
## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =
## 0.7946
## alternative hypothesis: greater</code></pre>
<p>With <code>alternative = "greater"</code> The alternative hypothesis is that method 2 is more accurate than method 1. Since we do not reject the null (the p-value being of 0.795), we are not led to use the more sophisticated model (ARMA(2,2)) and we keep the simple AR(1) model.</p>
<p>Assume now that we want to compare the AR(1) process to a multivariate (VAR) model. We consider a bivariate VAR, where GDP growth is complemented with CPI-based inflation rate.</p>
<div class="sourceCode" id="cb136"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span></span>
<span><span class="va">infl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">cpi</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">cpi</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">y_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">y</span>,<span class="va">infl</span><span class="op">)</span></span>
<span><span class="va">e3</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">T.star</span> <span class="kw">in</span> <span class="va">first.date</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">h</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">estim.model.3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/vars/man/VAR.html">VAR</a></span><span class="op">(</span><span class="va">y_var</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="va">T.star</span>,<span class="op">]</span>,p<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="va">e3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">e3</span>,<span class="va">y</span><span class="op">[</span><span class="va">T.star</span><span class="op">+</span><span class="va">h</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">estim.model.3</span>,n.ahead<span class="op">=</span><span class="va">h</span><span class="op">)</span><span class="op">$</span><span class="va">fcst</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="va">h</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">res.DM</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://pkg.robjhyndman.com/forecast/reference/dm.test.html">dm.test</a></span><span class="op">(</span><span class="va">e1</span>,<span class="va">e2</span>,h <span class="op">=</span> <span class="va">h</span>,alternative <span class="op">=</span> <span class="st">"greater"</span><span class="op">)</span></span>
<span><span class="va">res.DM</span></span></code></pre></div>
<pre><code>## 
##  Diebold-Mariano Test
## 
## data:  e1e2
## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =
## 0.7946
## alternative hypothesis: greater</code></pre>
<p>Again, we do not find that the alternative model (here the VAR(1) model) is better than the AR(1) model to forecast GDP growth.</p>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="binary-choice-models.html"><span class="header-section-number">7</span> Binary-choice models</a></div>
<div class="next"><a href="append.html"><span class="header-section-number">9</span> Appendix</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#TS"><span class="header-section-number">8</span> Time Series</a></li>
<li><a class="nav-link" href="#introduction-to-time-series"><span class="header-section-number">8.1</span> Introduction to time series</a></li>
<li>
<a class="nav-link" href="#univariate-processes"><span class="header-section-number">8.2</span> Univariate processes</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#moving-average-ma-processes"><span class="header-section-number">8.2.1</span> Moving Average (MA) processes</a></li>
<li><a class="nav-link" href="#ARsection"><span class="header-section-number">8.2.2</span> Auto-Regressive (AR) processes</a></li>
<li><a class="nav-link" href="#PACFapproach"><span class="header-section-number">8.2.3</span> PACF approach to identify AR/MA processes</a></li>
</ul>
</li>
<li><a class="nav-link" href="#forecasting"><span class="header-section-number">8.3</span> Forecasting</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Econometrics and Statistics</strong>" was written by Jean-Paul Renne. It was last built on 2025-02-13.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
