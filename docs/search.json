[{"path":"index.html","id":"intro","chapter":"Econometrics and statistics","heading":"Econometrics and statistics","text":"course covers various econometric topics, including linear regression models, discrete-choice models, introduction time series analysis. provides examples simulations based R codes. course developed Jean-Paul Renne.R codes use various packages can obtained CRAN. Several pieces code also involve procedures data companion package (AEC). AEC package available GitHub. install , one need employ devtools library:Useful (R) links:Download R:\nR software: https://cran.r-project.org (basic R software)\nRStudio: https://www.rstudio.com (convenient R editor)\nDownload R:R software: https://cran.r-project.org (basic R software)RStudio: https://www.rstudio.com (convenient R editor)Tutorials:\nRstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)\nR: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)\ntutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/\nTutorials:Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/","code":"\ninstall.packages(\"devtools\") # This library allows to use \"install_github\".\nlibrary(devtools)\ninstall_github(\"jrenne/AEC\")\nlibrary(AEC)"},{"path":"Basics.html","id":"Basics","chapter":"1 Basic statistical results","heading":"1 Basic statistical results","text":"","code":""},{"path":"Basics.html","id":"cumulative-and-probability-density-funtions-c.d.f.-and-p.d.f.","chapter":"1 Basic statistical results","heading":"1.1 Cumulative and probability density funtions (c.d.f. and p.d.f.)","text":"Definition 1.1  (Cumulative distribution function (c.d.f.)) random variable (r.v.) \\(X\\) admits cumulative distribution function \\(F\\) , \\(\\):\n\\[\nF()=\\mathbb{P}(X \\le ).\n\\]Definition 1.2  (Probability distribution function (p.d.f.)) continuous random variable \\(X\\) admits probability density function \\(f\\) , \\(\\) \\(b\\) \\(<b\\):\n\\[\n\\mathbb{P}(< X \\le b) = \\int_{}^{b}f(x)dx,\n\\]\n\\(f(x) \\ge 0\\) \\(x\\).particular, :\n\\[\\begin{equation}\nf(x) = \\lim_{\\varepsilon \\rightarrow 0} \\frac{\\mathbb{P}(x < X \\le x + \\varepsilon)}{\\varepsilon} = \\lim_{\\varepsilon \\rightarrow 0} \\frac{F(x + \\varepsilon)-F(x)}{\\varepsilon}.\\tag{1.1}\n\\end{equation}\\]\n\n\\[\nF() = \\int_{-\\infty}^{}f(x)dx.\n\\]\nweb interface illustrates link p.d.f. c.d.f. Nonparametric estimates p.d.f. obtained kernel-based methods (see extra material).Definition 1.3  (Joint cumulative distribution function (c.d.f.)) random variables \\(X\\) \\(Y\\) admit joint cumulative distribution function \\(F_{XY}\\) , \\(\\) \\(b\\):\n\\[\nF_{XY}(,b)=\\mathbb{P}(X \\le ,Y \\le b).\n\\]\nFigure 1.1: volume horizontal plane (\\(z=0\\)) surface equal \\(F_{XY}(0.5,1)=\\mathbb{P}(X<0.5,Y<1)\\).\nDefinition 1.4  (Joint probability density function (p.d.f.)) continuous random variables \\(X\\) \\(Y\\) admit joint p.d.f. \\(f_{XY}\\), \\(f_{XY}(x,y) \\ge 0\\) \\(x\\) \\(y\\), :\n\\[\n\\mathbb{P}(< X \\le b,c < Y \\le d) = \\int_{}^{b}\\int_{c}^{d}f_{XY}(x,y)dx dy, \\quad \\forall \\le b,\\;c \\le d.\n\\]particular, :\n\\[\\begin{eqnarray*}\n&&f_{XY}(x,y)\\\\\n&=& \\lim_{\\varepsilon \\rightarrow 0} \\frac{\\mathbb{P}(x < X \\le x + \\varepsilon,y < Y \\le y + \\varepsilon)}{\\varepsilon^2} \\\\\n&=& \\lim_{\\varepsilon \\rightarrow 0} \\frac{F_{XY}(x + \\varepsilon,y + \\varepsilon)-F_{XY}(x,y + \\varepsilon)-F_{XY}(x + \\varepsilon,y)+F_{XY}(x,y)}{\\varepsilon^2}.\n\\end{eqnarray*}\\]\nFigure 1.2: Assume basis black column defined points whose \\(x\\)-coordinates \\(x\\) \\(x+\\varepsilon\\) \\(y\\)-coordinates \\(y\\) \\(y+\\varepsilon\\). volume black column equal \\(\\mathbb{P}(x < X \\le x+\\varepsilon,y < Y \\le y+\\varepsilon)\\), approximately equal \\(f_{XY}(x,y)\\varepsilon^2\\) \\(\\varepsilon\\) small.\nDefinition 1.5  (Conditional probability distribution function) \\(X\\) \\(Y\\) continuous r.v., distribution \\(X\\) conditional \\(Y=y\\), denote \\(f_{X|Y}(x,y)\\), satisfies:\n\\[\nf_{X|Y}(x,y)=\\lim_{\\varepsilon \\rightarrow 0} \\frac{\\mathbb{P}(x < X \\le x + \\varepsilon|Y=y)}{\\varepsilon}.\n\\]Proposition 1.1  (Conditional probability distribution function) \n\\[\nf_{X|Y}(x,y)=\\frac{f_{XY}(x,y)}{f_Y(y)}.\n\\]Proof. :\n\\[\\begin{eqnarray*}\nf_{X|Y}(x,y)&=&\\lim_{\\varepsilon \\rightarrow 0} \\frac{\\mathbb{P}(x < X \\le x + \\varepsilon|Y=y)}{\\varepsilon}\\\\\n&=&\\lim_{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\mathbb{P}(x < X \\le x + \\varepsilon|y<Y\\le y+\\varepsilon)\\\\\n&=&\\lim_{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\frac{\\mathbb{P}(x < X \\le x + \\varepsilon,y<Y\\le y+\\varepsilon)}{\\mathbb{P}(y<Y\\le y+\\varepsilon)}\\\\\n&=&\\lim_{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon} \\frac{\\varepsilon^2f_{XY}(x,y)}{\\varepsilon f_{Y}(y)}.\n\\end{eqnarray*}\\]Definition 1.6  (Independent random variables) Consider two r.v., \\(X\\) \\(Y\\), respective c.d.f. \\(F_X\\) \\(F_Y\\), respective p.d.f. \\(f_X\\) \\(f_Y\\).random variables independent (iff) joint c.d.f. \\(X\\) \\(Y\\) (see Def. 1.3) given :\n\\[\nF_{XY}(x,y) = F_{X}(x) \\times F_{Y}(y),\n\\]\n, equivalently, iff joint p.d.f. \\((X,Y)\\) (see Def. 1.4) given :\n\\[\nf_{XY}(x,y) = f_{X}(x) \\times f_{Y}(y).\n\\]following:\\(X\\) \\(Y\\) independent, \\(f_{X|Y}(x,y)=f_{X}(x)\\). implies, particular, \\(\\mathbb{E}(g(X)|Y)=\\mathbb{E}(g(X))\\), \\(g\\) function.\\(X\\) \\(Y\\) independent, \\(\\mathbb{E}(g(X)h(Y))=\\mathbb{E}(g(X))\\mathbb{E}(h(Y))\\) \\(\\mathbb{C}ov(g(X),h(Y))=0\\), \\(g\\) \\(h\\) functions.important note absence correlation two variables sufficient condition independence. Consider instance case \\(X=Y^2\\), \\(Y \\sim\\mathcal{N}(0,1)\\). case, \\(\\mathbb{C}ov(X,Y)=0\\), \\(X\\) \\(Y\\) independent. Indeed, instance \\(\\mathbb{E}(Y^2 \\times X)=3\\), equal \\(\\mathbb{E}(Y^2) \\times \\mathbb{E}(X)=1\\). (\\(X\\) \\(Y\\) independent, \\(\\mathbb{E}(Y^2 \\times X)=\\mathbb{E}(Y^2) \\times \\mathbb{E}(X)\\) according point 2 .)","code":""},{"path":"Basics.html","id":"law-of-iterated-expectations","chapter":"1 Basic statistical results","heading":"1.2 Law of iterated expectations","text":"Proposition 1.2  (Law iterated expectations) \\(X\\) \\(Y\\) two random variables \\(\\mathbb{E}(|X|)<\\infty\\), :\n\\[\n\\boxed{\\mathbb{E}(X) = \\mathbb{E}(\\mathbb{E}(X|Y)).}\n\\]Proof. (case p.d.f. \\((X,Y)\\) exists) Let us denote \\(f_X\\), \\(f_Y\\) \\(f_{XY}\\) probability distribution functions (p.d.f.) \\(X\\), \\(Y\\) \\((X,Y)\\), respectively. :\n\\[\nf_{X}(x) = \\int f_{XY}(x,y) dy.\n\\]\nBesides, (Bayes equality, Prop. 1.1):\n\\[\nf_{XY}(x,y) = f_{X|Y}(x,y)f_{Y}(y).\n\\]\nTherefore:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(X) &=& \\int x f_X(x)dx = \\int x  \\underbrace{ \\int  f_{XY}(x,y) dy}_{=f_X(x)} dx =\\int  \\int x f_{X|Y}(x,y)f_{Y}(y) dydx \\\\\n& = & \\int \\underbrace{\\left(\\int x f_{X|Y}(x,y)dx\\right)}_{\\mathbb{E}[X|Y=y]}f_{Y}(y) dy = \\mathbb{E} \\left( \\mathbb{E}[X|Y] \\right).\n\\end{eqnarray*}\\]Example 1.1  (Mixture Gaussian distributions) definition, \\(X\\) drawn mixture Gaussian distributions :\n\\[\nX = \\color{blue}{B \\times Y_1} + \\color{red}{(1-B) \\times Y_2},\n\\]\n\\(B\\), \\(Y_1\\) \\(Y_2\\) three independent variables drawn follows:\n\\[\nB \\sim \\mbox{Bernoulli}(p),\\quad Y_1 \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2), \\quad \\mbox{}\\quad Y_2 \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2).\n\\]Figure 1.3 displays pdfs associated three different mixtures Gaussian distributions. (web-interface allows produce pdf associated parameterization.)\nFigure 1.3: Example pdfs mixtures Gaussian distribututions.\nlaw iterated expectations gives:\n\\[\n\\mathbb{E}(X) = \\mathbb{E}(\\mathbb{E}(X|B)) = \\mathbb{E}(B\\mu_1+(1-B)\\mu_2)=p\\mu_1 + (1-p)\\mu_2.\n\\]Example 1.2  (Buffon (1733)'s needles) Suppose floor made parallel strips wood, width [\\(w=1\\)]. drop needle, length \\(1/2\\), onto floor. probability needle crosses grooves floor?Let’s define random variable \\(X\\) \n\\[\nX = \\left\\{\n\\begin{array}{cl}\n1 & \\mbox{needle crosses line}\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\n\\right.\n\\]\nConditionally \\(\\theta\\), can seen \\(\\mathbb{E}(X|\\theta)=\\cos(\\theta)/2\\) (see Figure 1.4).\nFigure 1.4: Schematic representation problem.\nreasonable assume \\(\\theta\\) uniformly distributed \\([-\\pi/2,\\pi/2]\\), therefore:\n\\[\n\\mathbb{E}(X)=\\mathbb{E}(\\mathbb{E}(X|\\theta))=\\mathbb{E}(\\cos(\\theta)/2)=\\int_{-\\pi/2}^{\\pi/2}\\frac{1}{2}\\cos(\\theta)\\left(\\frac{d\\theta}{\\pi}\\right)=\\frac{1}{\\pi}.\n\\][web-interface allows simulate present experiment (select Worksheet “Buffon’s needles”).]","code":""},{"path":"Basics.html","id":"law-of-total-variance","chapter":"1 Basic statistical results","heading":"1.3 Law of total variance","text":"Proposition 1.3  (Law total variance) \\(X\\) \\(Y\\) two random variables variance \\(X\\) finite, :\n\\[\n\\boxed{\\mathbb{V}ar(X) = \\mathbb{E}(\\mathbb{V}ar(X|Y)) + \\mathbb{V}ar(\\mathbb{E}(X|Y)).}\n\\]Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{V}ar(X) &=& \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\\\\\n&=& \\mathbb{E}(\\mathbb{E}(X^2|Y)) - \\mathbb{E}(X)^2\\\\\n&=& \\mathbb{E}(\\mathbb{E}(X^2|Y) \\color{blue}{- \\mathbb{E}(X|Y)^2}) +  \\color{blue}{\\mathbb{E}(\\mathbb{E}(X|Y)^2)}  - \\color{red}{\\mathbb{E}(X)^2}\\\\\n&=& \\mathbb{E}(\\underbrace{\\mathbb{E}(X^2|Y) - \\mathbb{E}(X|Y)^2}_{\\mathbb{V}ar(X|Y)}) + \\underbrace{\\mathbb{E}(\\mathbb{E}(X|Y)^2) - \\color{red}{\\mathbb{E}(\\mathbb{E}(X|Y))^2}}_{\\mathbb{V}ar(\\mathbb{E}(X|Y))}.\n\\end{eqnarray*}\\]Example 1.3  (Mixture Gaussian distributions (cont'd)) Consider case mixture Gaussian distributions (Example 1.1). :\n\\[\\begin{eqnarray*}\n\\mathbb{V}ar(X) &=& \\color{blue}{\\mathbb{E}(\\mathbb{V}ar(X|B))} + \\color{red}{\\mathbb{V}ar(\\mathbb{E}(X|B))}\\\\\n&=&  \\color{blue}{p\\sigma_1^2+(1-p)\\sigma_2^2} + \\color{red}{p(1-p)(\\mu_1 - \\mu_2)^2}.\n\\end{eqnarray*}\\]","code":""},{"path":"Basics.html","id":"about-consistent-estimators","chapter":"1 Basic statistical results","heading":"1.4 About consistent estimators","text":"objective econometrics estimate parameters data observations (samples). Examples parameters interest include, among many others: causal effect variable another, elasticities, parameters defining distribution interest, preference parameters (risk aversion)…Except degenerate cases, estimates different “true” (population) value. good estimator expected converge true value sample size increases. , interested consistency estimator.Denote \\(\\hat\\theta_n\\) estimate \\(\\theta\\) based sample length \\(n\\). say \\(\\hat\\theta\\) consistent estimator \\(\\theta\\) , \\(\\varepsilon>0\\) (even small), probability \\(\\hat\\theta_n\\) \\([\\theta - \\varepsilon,\\theta + \\varepsilon]\\) goes 1 \\(n\\) goes \\(\\infty\\). Formally:\n\\[\n\\lim_{n \\rightarrow + \\infty} \\mathbb{P}\\left(\\hat\\theta_n \\[\\theta - \\varepsilon,\\theta + \\varepsilon]\\right) = 1.\n\\], \\(\\hat\\theta\\) consistent estimator \\(\\hat\\theta_n\\) converges probability (Def. 9.14) \\(\\theta\\). Note exist different types stochastic convergence.1Example 1.4  (Example non-convergent estimator) Assume \\(X_i \\sim ..d. \\mbox{Cauchy}\\) location parameter 1 scale parameter 1 (Def. 9.12). sample mean \\(\\bar{X}_n = \\frac{1}{n}\\sum_{=1}^{n} X_i\\) converge probability. Cauchy distribution mean; hence law large numbers (Theorem 2.1) apply.\nFigure 1.5: Simulation \\(\\bar{X}_n\\) \\(X_i \\sim ..d. \\mbox{Cauchy}\\).\n","code":"\nN <- 5000\nX <- rcauchy(N)\nX.bar <- cumsum(X)/(1:N)\npar(plt=c(.1,.95,.3,.85),mfrow=c(1,2))\nplot(X,type=\"l\",xlab=\"sample size (n)\",\n     ylab=\"\",main=expression(X[n]),lwd=2)\nplot(X.bar,type=\"l\",xlab=\"sample size (n)\",\n     ylab=\"\",main=expression(bar(X)[n]),lwd=2)\nabline(h=0,lty=2,col=\"grey\")"},{"path":"TCL.html","id":"TCL","chapter":"2 Central Limit Theorem","heading":"2 Central Limit Theorem","text":"law large numbers (LLN) central limit theorem (CLT) two fundamental theorems probability. former states sample mean converges probability population mean (exists). latter states distribution sum (average) large number independent, identically distributed (..d.) variables finite variance approximately normal, regardless underlying distribution (long features finite variance).","code":""},{"path":"TCL.html","id":"law-of-large-numbers","chapter":"2 Central Limit Theorem","heading":"2.1 Law of large numbers","text":"Definition 2.1  (Sample Population) statistics, sample refers finite set observations drawn population.time, necessary use samples research, impractical study whole population. Typically, rare observe population mean, often use instead sample means (averages). sample average \\(\\{x_1,\\dots,x_n\\}\\) given :\n\\[\n\\overline{x}_n = \\frac{1}{n}\\sum_{=1}^n x_i.\n\\]\nsample mean estimator true (population) mean \\(\\mathbb{E}(x_i)\\) (assuming latter exists). According law large numbers (Theorem 2.1), estimator consistent. (see Def. 9.14 definition convergence probability.)Theorem 2.1  (Law large numbers) sample mean consistent estimator population mean. words, sample mean converges probability population mean.Proof. See Appendix 9.3.4.Example 2.1  Suppose one interested average number doctor consultations 12 months, Swiss people. One can get estimate computing sample average large dataset, instance using Swiss Household Panel (SHP):Figure 2.1 shows empirical distribution number doctor visits.\nFigure 2.1: Distribution number doctor visits. Source: Swiss Household Panel.\nLet us compute sample mean, variance, standard deviation:know whether sample mean good estimate true (population) average? words, measure accuracy estimator?first answer provided Chebychev inequality. Assume \\(x_i\\)’s independently drawn distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). :\n\\[\n\\mathbb{E}(\\bar{x}_n) = \\mu \\quad \\mbox{ } \\quad \\mathbb{V}ar(\\mu - \\bar{x}_n) = \\frac{1}{n}\\sigma^2 \\underset{n \\rightarrow \\infty}{\\rightarrow} 0.\n\\]\nChebychev inequality (Proposition 9.11) states , \\(\\varepsilon > 0\\) (even small), :\n\\[\n\\mathbb{P}(|\\bar{x}_n-\\mu|>\\varepsilon) \\le \\frac{\\mathbb{V}ar(\\mu - \\bar{x}_n)}{\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\underset{n \\rightarrow \\infty}{\\rightarrow} 0.\n\\]\nConsider instance estimation average number doctor visits (Example 2.1). example, sample length \\(n=\\) 4389, \\(\\bar{x}_n=\\) 5.25, variance \\(\\sigma^2\\) approximately equal 79.51. Therefore, taking \\(\\varepsilon=0.5\\), \\(\\mathbb{P}(\\bar{x}_n-0.5<\\mu<\\bar{x}_n+0.5)\\) lower \\(\\frac{\\sigma^2}{n\\varepsilon^2} \\approx\\)\n0.072.However, gives bounds probabilities. central limit theorem provides richer information, gives approximate distribution estimation error.","code":"\nlibrary(AEC) # to load the shp data-frame.\nNb.doct.visits <- shp$p19c15\nNb.doct.visits <- Nb.doct.visits[Nb.doct.visits>=0] # remove irrelevant observations\npar(plt=c(.15,.95,.2,.95))\nbarplot(table(Nb.doct.visits),xlim=c(0,25))\ncbind(mean(Nb.doct.visits),var(Nb.doct.visits),sd(Nb.doct.visits))##          [,1]     [,2]   [,3]\n## [1,] 5.253816 79.50575 8.9166"},{"path":"TCL.html","id":"central-limit-theorem-clt","chapter":"2 Central Limit Theorem","heading":"2.2 Central Limit Theorem (CLT)","text":"Theorem 2.2  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) (finite) variance, :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]\n[“\\(\\overset{d}{\\rightarrow}\\)” denotes convergence distribution (see Def. 9.17)]Proof. See Appendix 9.3.4.According CLT, \\(n\\) large whatever distribution \\(x_i\\)’s (long features average \\(\\mu\\) variance \\(\\sigma^2\\)), error \\(\\bar{x}_n\\) \\(\\mu\\) can seen random variable normally distributed; mean zero standard deviation \\(\\sigma/\\sqrt{n}\\). (say convergence rate \\(\\sqrt{n}\\).) knowledge quantiles normal distribution can used compute approximate confidence intervals \\(\\mu\\) (based sample \\(\\{x_1,\\dots,x_n\\}\\)).web interface illustrates CLT simulations (select “CLT” block).CL theorem first postulated mathematician Abraham de Moivre. article published 1733, used normal distribution approximate distribution number heads resulting many tosses fair coin. finding nearly forgotten Pierre-Simon Laplace recalled Théorie analytique des probabilités, published 1812. Laplace expanded De Moivre’s finding approximating binomial distribution normal distribution.Exercise 2.1  Consider Buffon’s experiment (Example 1.2). seen \\(\\pi\\) can estimated counting fraction times needles cross grooves floor.2 Using CLT, determine (approximate) minimal number \\(n\\) needles one throw get estimate \\(\\hat\\pi\\) (\\(= 1/\\bar{X}_n\\)) \\(\\pi\\) 95% chance \\(\\pi \\[\\hat\\pi-0.01,\\hat\\pi+0.01]\\)?Exercise 2.2  Assume lot time coin. approximate 100 percentiles \\(\\mathcal{N}(0,1)\\) distribution?CLT can extended case \\(x_i\\) vector. (case, dimension \\(x_i\\) \\(m\\), \\(\\mathbb{V}ar(x_i)\\) \\(m\\times m\\) matrix.) multivariate CLT:Theorem 2.3  (Multivariate Central limit theorem, CLT) \\(\\mathbf{x}_n\\) ..d. sequence \\(m\\)-dimensional random variables mean \\(\\boldsymbol\\mu\\) variance \\(\\Sigma\\), \\(\\Sigma\\) positive definite matrix, :\n\\[\n\\boxed{\\sqrt{n} (\\bar{\\mathbf{x}}_n - \\boldsymbol\\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(\\mathbf{0},\\Sigma), \\quad \\mbox{} \\quad \\bar{\\mathbf{x}}_n = \\frac{1}{n} \\sum_{=1}^{n} \\mathbf{x}_i.}\n\\]","code":""},{"path":"TCL.html","id":"comparison-of-sample-means","chapter":"2 Central Limit Theorem","heading":"2.3 Comparison of sample means","text":"said beginning chapter, CLT key statistical result numerous applications. important one statistical comparision sample means:Consider two samples \\(\\{m_1,\\dots,m_{n_m}\\}\\) \\(\\{w_1,\\dots,w_{n_w}\\}\\). Assume \\(m_i\\)’s (respectively \\(w_i\\)’s) ..d., mean \\(\\mu_m\\) variance \\(\\sigma^2_m\\) \\(w_i\\)’s (resp. mean \\(\\mu_w\\) variance \\(\\sigma^2_w\\)).\\(n\\) large, according CLT, approximately :\n\\[\n\\sqrt{n_m}(\\bar{m} - \\mu_m) \\sim \\mathcal{N}(0,\\sigma^2_m) \\quad \\quad \\sqrt{n_w}(\\bar{w} - \\mu_w) \\sim \\mathcal{N}(0,\\sigma^2_w),\n\\]\n\n\\[\n\\bar{m} \\sim \\mathcal{N}\\left(\\mu_m,\\frac{\\sigma^2_m}{n_m}\\right) \\quad \\quad \\bar{w} \\sim \\mathcal{N}\\left(\\mu_w,\\frac{\\sigma^2_w}{n_w}\\right).\n\\]\\(m_i\\)’s \\(w_i\\)’s independent, \\(\\bar{m}\\) \\(\\bar{w}\\) also , get:\n\\[\n\\bar{m} - \\bar{w} \\sim \\mathcal{N}\\left(\\mu_m-\\mu_w,\\sigma^2\\right) \\quad \\quad \\sigma^2 = \\frac{\\sigma^2_m}{n_m}+\\frac{\\sigma^2_w}{n_w}.\n\\]\ncan used test null hypothesis \\(H_0: \\; \\mu_m-\\mu_w = 0\\) (see Section 3). Indeed assumption, probability \\(\\bar{m} - \\bar{w}\\) \\([-1.96\\sigma,1.96\\sigma]\\) (respectively \\([-2.58\\sigma,2.58\\sigma]\\)) approximately 0.95 (respectively 0.99) [see Table 9.1].instance, find, using sample, \\(\\bar{m} - \\bar{w}\\) \\([-2.58\\sigma,2.58\\sigma]\\), reject null hypothesis 1% significance level.Example 2.2  (Comparison sample means) Let us use Swiss Household Panel (SHP) dataset test equality men women incomes, Swiss people age 35.next lines codes construct two samples incomes; one men (income.m), one women (income.w).Figure 2.2 displays empirical densities incomes men women. Vertical dashed lines indicate sample means:\nFigure 2.2: Distribution yearly gross incomes Swiss residents age 35. Source: SHP. Vertical dashed lines indicates sample mean.\nLet us compute \\(\\bar{m}\\), \\(\\bar{w}\\), \\(\\sigma\\):“equality assumption”, probability 5% (respectively 1%) \\(\\bar{m} - \\bar{w}\\) \\([\\)-4758,4758\\(]\\) (respectively \\([\\)-6263,6263\\(]\\)). Since \\(\\bar{m} - \\bar{w}\\) equal 10327, reject null hypothesis 5%, even 1%, significance levels.","code":"\nlibrary(AEC);library(logKDE)\ndataset.m <- subset(shp,(sex19==1)&(age19<=35))\ndataset.w <- subset(shp,(sex19==2)&(age19<=35))\nincome.m <- dataset.m$i19ptotg\nincome.w <- dataset.w$i19ptotg\nincome.m <- income.m[income.m>=0] # remove irrelevant data\nincome.w <- income.w[income.w>=0] # remove irrelevant data\npar(plt=c(.1,.95,.2,.95)) # adjust margins\nplot(logdensity(income.w),lwd=2,xlim=c(0,200000),main=\"\",\n     xlab=\"Yearly gross income (in CHF)\",ylab=\"\") # x and y labels\nlines(logdensity(income.m),col=\"red\",lwd=2)\nabline(v=mean(income.m),col=\"red\",lty=3,lwd=2)\nabline(v=mean(income.w),col=\"black\",lty=3,lwd=2)\nlegend(\"topright\",c(\"men yearly income\",\"women\"),lty=1,lwd=2,col=c(\"red\",\"black\"))\nm.bar <- mean(income.m)\nw.bar <- mean(income.w)\nsigma <- sqrt(var(income.m)/length(income.m) + var(income.w)/length(income.w))\nprint(c(m.bar,w.bar,sigma))## [1] 53035.023 42708.227  2427.425"},{"path":"Tests.html","id":"Tests","chapter":"3 Statistical tests","heading":"3 Statistical tests","text":"run statistical test want know whether hypothesis vector parameters \\(\\theta\\) —imperfectly observed— consistent data seen random, whose randomness depend \\(\\theta\\).Typically, assume observe sample \\(\\mathbf{x}=\\{x_1,\\dots,x_n\\}\\) \\(x_i\\)’s ..d., mean \\(\\mu\\) variance \\(\\sigma^2\\) (two parameters unobseved). One may want know whether \\(\\mu=0\\) , maybe, whether \\(\\sigma = 1\\).hypothesis researcher wants test called null hypothesis. often denoted \\(H_0\\). conjecture given property population. Without loss generality, can stated :\n\\[\nH_0:\\;\\{\\theta \\\\Theta\\}.\n\\]\ncan also defined function \\(h\\) (say):3\n\\[\nH_0:\\;h(\\theta)=0.\n\\]alternative hypothesis, often denoted \\(H_1\\) defined \\(H_1:\\;\\{\\theta \\\\Theta^c\\}\\).4The ingredients statistical test :vector (unknown) parameters (\\(\\theta\\)),test statistic, function sample components (\\(S(\\mathbf{x})\\), say), anda critical region (\\(\\Omega\\), say), defined set implausible values \\(S\\) \\(H_0\\).implement test statistic, need know distribution \\(S\\) null hypothesis \\(H_0\\). Equipped, distribution, compute \\(S(\\mathbf{x})\\) look location; specifically, look whether lies within critical region \\(\\Omega\\). latter corresponds “implausible” regions distribution (typically tails). \\(S \\\\Omega\\), reject null hypothesis. Looselly speaking; amounts saying: \\(H_0\\) true, unlikely get large (small) draw \\(S\\).Hence, two possible outcomes statisitcal test:\\(H_0\\) rejected \\(S \\\\Omega\\);\\(H_0\\) rejected \\(S \\\\\\Omega\\).Except extreme cases, always non-zero probability reject \\(H_0\\) true (Type error, false positive), fail reject false (Type II error, false negative).vocabulary widely used. instance, notions false positive false negative used context Early Warning Signals (see Example 3.1).Example 3.1  (Early Warning Signals) approaches aim detect occurrence crises advance. See, e.g., ECB (2014), applications financial crises.implement approaches, researchers look signals/indices forecasting crises. Suppose one index (\\(W\\), say) tends large financial crises; one may define EWS predicting crisis \\(W>\\), \\(\\) given threshold. easliy seen lower (respectively higeher) \\(\\), larger fraction FP (resp. FN).","code":""},{"path":"Tests.html","id":"size-and-power-of-a-test","chapter":"3 Statistical tests","heading":"3.1 Size and power of a test","text":"Definition 3.1  (Size Power test) given test,probability type-errors, denoted \\(\\alpha\\), called size, significance level, test,power test equal \\(1 - \\beta\\), \\(\\beta\\) probability type-II errors.Formally, previous definitions can written follows:\n\\[\\begin{eqnarray}\n\\alpha &=& \\mathbb{P}(S \\\\Omega|H_0) \\quad \\mbox{(Proba. false positive)}\\\\\n\\beta &=& \\mathbb{P}(S \\\\\\Omega|H_1) \\quad \\mbox{(Proba. false negative)}.\n\\end{eqnarray}\\]power probability test lead rejection null hypothesis latter false. Therefore, given size, prefer tests high power.cases, trade-size power, whch easily understood EWS context (Example 3.1): increasing threshold \\(\\) reduces fraction FP (thereby reducing size test), increases fraction FN (thereby reducing power test).","code":""},{"path":"Tests.html","id":"the-different-types-of-statistical-tests","chapter":"3 Statistical tests","heading":"3.2 The different types of statistical tests","text":"determine critical region? Loosely speaking, want critical region set “implausible” values test statistic \\(S\\) null hypothesis \\(H_0\\). lower size test (\\(\\alpha\\)), implausible values. Recall , definition size test, \\(\\alpha = \\mathbb{P}(S \\\\Omega|H_0)\\). , \\(\\alpha\\) small, small probability \\(S\\) lies \\(\\Omega\\) \\(H_0\\).Consider case , \\(H_0\\), distribution test statistic symmetrical (e.g., normal distribution Student-t distribution). case, critical region usually defined union two tails distribution. test said two-tailed test two-sided test. situation illustrated Figures 3.1 3.2. (Use web interface explore alternative situations.)Figure 3.2 also illustrates notion p-value (case two-sided test). p-value can defined value size test \\(\\alpha\\) computed test statistic, \\(S\\), “frontier” critical region. Given definition, p-value smaller (respectively larger) size test, reject (resp. rejet) null hypothesis \\(\\alpha\\) significance level.\nFigure 3.1: Two-sided test. \\(H_0\\), \\(S \\sim t(5)\\). \\(\\alpha\\) size test.\n\nFigure 3.2: Two-sided test. \\(H_0\\), \\(S \\sim t(5)\\). \\(\\alpha\\) size test.\nFigures 3.3 3.4 illustrate one-tailed, one-sided situation. tests typically employed distribution test statistic null hypothesis support \\(\\mathbb{R}^+\\) (e.g., chi-square distribution \\(\\chi^2\\), see Def. 9.11). Figure 3.4 also illustrates notion p-value associated one-sided statistical test.\nFigure 3.3: One-sided test. \\(H_0\\), \\(S \\sim \\chi^2(5)\\). \\(\\alpha\\) size test.\n\nFigure 3.4: One-sided test. \\(H_0\\), \\(S \\sim \\chi^2(5)\\). \\(\\alpha\\) size test.\nExample 3.2  (practical illustration size power) Consider factory produces metal cylinders whose diameter equal 1,cm. tolerance \\(=0.01\\) cm. precisely, 90% pieces satisfy tolerance whole production (say 1.000.000 pieces) bought client.production technology proportion \\(\\theta\\) (imperfectly known) pieces satisfy tolerance. (population) parameter \\(\\theta\\) computed measuring pieces costly. Instead, decided \\(n \\ll 1.000.000\\) pieces measured. context, null hypothesis \\(H_0\\) \\(\\theta < 10\\%\\). producing firm like \\(H_0\\) true.Let us denote \\(d_i\\) binary indicator defined :\n\\[\nd_i = \\left\\{\n\\begin{array}{cll}\n0 & \\mbox{size $^{th}$ cylinder $[1-,1+]$;}\\\\\n1 & \\mbox{otherwise.}\n\\end{array}\n\\right.\n\\]set \\(x_n=\\sum_{=1}^n d_i\\). , \\(x_n\\) number measured pieces satisfy tolerance (\\(n\\)).decision rule : accept \\(H_0\\) \\(\\dfrac{x_n}{n} \\le b\\), reject otherwise. natural choice \\(b\\) \\(b=0.1\\). However, conservative choice, since remains likely \\(x_n<0.1\\) even \\(\\mathbb{E}(x_n)=\\theta>0.1\\) (especially \\(n\\) small). Hence, one chooses \\(b=0.1\\), probability false negative may high.simple example, size power test can computed analytically. test statistic \\(S_n=\\frac{x_n}{n}\\) critical region \\(\\Omega = [b,1]\\). probability reject \\(H_0\\) :\n\\[\\begin{eqnarray*}\n\\mathbb{P}_\\theta(S_n \\\\Omega) = \\sum_{=b \\times n+1}^{n}C_{n}^\\theta^(1-\\theta)^{n-}.\n\\end{eqnarray*}\\]\\(\\theta<0.1\\), previous expression gives size test, gives power test \\(\\theta>0.1\\). Figure 3.5 shows probability depends \\(\\theta\\) two sample sizes: \\(n=100\\) (upper plot) \\(n=500\\) (lower plot).\nFigure 3.5: Factory example.\n(Alternative situations can explored using web interface.)","code":""},{"path":"Tests.html","id":"asymptotic-properties-of-statistical-tests","chapter":"3 Statistical tests","heading":"3.3 Asymptotic properties of statistical tests","text":"cases, distribution test statistic known asymptotically. instance, may case distribution given test statistic becomes normal large samples (e.g., related CLT, see Section 2). level test know small sample sizes, asymptotically, .e., \\(n\\) becomes infinitely large. defines asymptotic level test:Definition 3.2  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) asymptotic level equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha.\n\\]Example 3.3  (factory example) Let us come back factory example (Example 3.2). \\(S_n =\\bar{d}_n\\), since \\(\\mathbb{E}(d_i)=\\theta\\) \\(\\mathbb{V}ar(d_i)=\\theta(1-\\theta)\\), CLT (Theorem 2.2) leads :\n\\[\nS_n \\sim \\mathcal{N}\\left(\\theta,\\frac{1}{n}\\theta(1-\\theta)\\right) \\quad \\quad \\frac{\\sqrt{n}(S_n-\\theta)}{\\sqrt{\\theta(1-\\theta)}} \\sim \\mathcal{N}(0,1).\n\\]Hence, \\(\\mathbb{P}_\\theta (S_n \\\\Omega_n)=\\mathbb{P}_\\theta (S_n > b) \\approx 1-\\Phi\\left(\\frac{\\sqrt{n}(b-\\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right)\\). Since function \\(\\theta \\rightarrow 1-\\Phi\\left(\\frac{\\sqrt{n}(b-\\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right)\\) increases w.r.t. \\(\\theta\\), :\n\\[\n\\underset{\\theta \\\\Theta=[0,0.1]}{\\mbox{sup}} \\quad \\mathbb{P}_\\theta (S_n > b_n) = \\mathbb{P}_{\\theta=0.1} (S_n \\\\Omega_n)\\approx\\]\n\\[\n1-\\Phi\\left(\\frac{\\sqrt{n}(b_n-0.1)}{0.3}\\right).\n\\]\nHence, set \\(b_n = 0.1 + 0.3\\Phi^{-1}(1-\\alpha)/\\sqrt{n}\\), \\({\\mbox{sup}}_{\\theta \\\\Theta=[0,0.1]} \\quad \\mathbb{P}_\\theta (S_n > b_n) \\approx \\alpha\\) large values \\(n\\).Let us now turn power test. Although often difficult compute power test, sometimes feasible demonstrate power test converges one \\(n\\) goes \\(+\\infty\\). case, \\(H_0\\) false, probability reject tends close one large samples.Definition 3.3  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1.\n\\]Example 3.4  (factory example) Let us come back factory example (Example 3.2). proceed assumption \\(\\theta>0.1\\) consider \\(b_n = b = 0.1\\). still :\n\\[\n\\mathbb{P}_\\theta (S_n \\\\Omega_n)=\\mathbb{P}_\\theta (S_n > b) \\approx 1-\\Phi\\left(\\frac{\\sqrt{n}(b-\\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right).\n\\]\\(\\frac{\\sqrt{n}(b-\\theta)}{\\sqrt{\\theta(1-\\theta)}} \\underset{n \\rightarrow \\infty}{\\rightarrow} -\\infty\\), \n\\[\n\\mathbb{P}_\\theta (S_n > b) \\approx 1- \\underbrace{\\Phi\\left(\\frac{\\sqrt{n}(b-\\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right)}_{\\underset{n \\rightarrow \\infty}{\\rightarrow} 0} \\underset{n \\rightarrow \\infty}{\\rightarrow} 1.\n\\]\nTherefore, \\(b_n=b=0.1\\), test consistent.","code":""},{"path":"Tests.html","id":"example-normality-tests","chapter":"3 Statistical tests","heading":"3.4 Example: Normality tests","text":"many statistical results valid underlying data normally distributed, researchers often tu conduct normality tests. null hypothesis, data hand (\\(\\mathbf{y}=\\{y_1,\\dots,y_n\\}\\), say) drawn Gaussian distribution. popular normality test Jarque-Bera test. consists verifying sample skewness kurtosis \\(y_i\\)’s consistent normal distribution. Let us first define skewness kurtosis random variable.Let \\(f\\) p.d.f. \\(Y\\). \\(k^{th}\\) standardized moment \\(Y\\) defined :\n\\[\n\\psi_k = \\frac{\\mu_k}{\\left(\\sqrt{\\mathbb{V}ar(Y)}\\right)^k},\n\\]\n\\(\\mathbb{E}(Y)=\\mu\\) \n\\[\n\\mu_k = \\mathbb{E}[(Y-\\mu)^k]= \\int_{-\\infty}^{\\infty} (y-\\mu)^k f(y) dy\n\\]\n\\(k^{th}\\) central moment \\(Y\\). particular, \\(\\mu_2 = \\mathbb{V}ar(Y)\\). Therefore:\n\\[\n\\psi_k = \\frac{\\mu_k}{\\left(\\mu_2^{1/2}\\right)^k},\n\\]skewness \\(Y\\) corresponds \\(\\psi_3\\) kurtosis \\(\\psi_4\\) (Def. 9.6).Proposition 3.1  (Skewness kurtosis normal distribution) Gaussian var., skewness (\\(\\psi_3\\)) 0 kurtosis (\\(\\psi_4\\)) 3.Proof. centered Gaussian distribution, \\((-y)^3f(-y)=-y^3f(y)\\). implies \n\\[\\begin{eqnarray*}\n\\int_{-\\infty}^{\\infty}y^3f(y)dy&=&\\int_{-\\infty}^{0}y^3f(y)dy+\\int_{0}^{\\infty}y^3f(y)dy\\\\\n&=&-\\int_{0}^{\\infty}y^3f(y)dy+\\int_{0}^{\\infty}y^3f(y)dy=0,\n\\end{eqnarray*}\\]\nleads skewness result.Moreover, Gaussian distribution, \\(df(y)/dy=-yf(y)\\) therefore \\(\\frac{d}{dy}(y^3f(y))=3y^2f(y)-y^4f(y)\\). Partial integration leads kurtosis result.Let us now introduce sample analog standardized moments. \\(k^{th}\\) central sample moment \\(Y\\) given :\n\\[\nm_k = \\frac{1}{n}\\sum_{=1}^n(y_i - \\bar{y})^k,\n\\]\n\\(k^{th}\\) standardized sample moment \\(Y\\) given :\n\\[\ng_k = \\frac{m_k}{m_2^{k/2}}.\n\\]Proposition 3.2  (Consistency central sample moments) \\(k^{th}\\) central moment \\(Y\\), exists \\(y_i\\)’s ..d., sample central moment \\(m_k\\) consistent estimate central moment \\(\\mu_k\\).Proposition 3.3  (Asymptotic distribution 3rd-order sample central moment normal distribution) \\(y_i\\sim\\,..d.\\,\\mathcal{N}(\\mu,\\sigma^2)\\), \\(\\sqrt{n}g_3 \\overset{d}{\\rightarrow} \\mathcal{N}(0,6)\\).Proof. See, e.g. Lehmann (1999).Proposition 3.4  (Asymptotic distribution 4th-order sample central moment normal distribution) \\(y_i\\sim\\,..d.\\,\\mathcal{N}(\\mu,\\sigma^2)\\), \\(\\sqrt{n}(g_4-3) \\overset{d}{\\rightarrow} \\mathcal{N}(0,24)\\).Proposition 3.5  (Joint asymptotic distribution 3rd 4th-order sample central moments normal distribution) \\(y_i\\sim\\,..d.\\,\\mathcal{N}(\\mu,\\sigma^2)\\), vector \\((\\sqrt{n}g_3,\\sqrt{n}(g_4-3))\\) asymptotically bivariate Gaussian. elements uncorrelated therefore independent.Jarque-Bera statistic defined :\n\\[\nJB = n \\left( \\frac{g_3^2}{6}+\\frac{(g_4-3)^2}{24} \\right) = \\frac{n}{6}\\left(g_3^2 + \\frac{(g_4-3)^2}{4}\\right).\n\\]Proposition 3.6  (Jarque-Bera asympt. distri.) \\(y_i\\sim\\,..d.\\,\\mathcal{N}(\\mu,\\sigma^2)\\), \\(JB \\overset{d}{\\rightarrow} \\chi^2(2)\\).Proof. directly derives Proposition 3.5.Example 3.5  (Consistency Jarque-Bera normality test) example illustrates consistency JB test (see Def. 3.3).row matrix x, function JB (defined ) computes Jarque-Bera test statistic. (, row considered given sample.)Let us first consider case \\(H_0\\) satisfied. Figure 3.6 displays, different sample sizes \\(n\\), distribution JB statistics \\(y_i\\)’s normal, consistently \\(H_0\\). appears \\(n\\) grows, distribution indeed converges \\(\\chi^2(2)\\) distribution (stated Proposition 3.6).\nFigure 3.6: Distribution JB test statistic \\(H_0\\) (normality).\nNow, replace rnorm runif. \\(y_i\\)’s drawn uniform distribution. \\(H_0\\) satisfied. Figure 3.7 shows , \\(n\\) grows, distributions JB statistic shift right. results consistency JB test (see Def. 3.3).\nFigure 3.7: Distribution JB test statistic \\(y_i\\)’s drawn uniform distribution (hence \\(H_0\\) satisfied).\n","code":"\nJB <- function(x){\n  N <- dim(x)[1] # number of samples\n  n <- dim(x)[2] # sample size\n  x.bar <- apply(x,1,mean)\n  x.x.bar <- x - matrix(x.bar,N,n)\n  m.2 <- apply(x.x.bar,1,function(x){mean(x^2)})\n  m.3 <- apply(x.x.bar,1,function(x){mean(x^3)})\n  m.4 <- apply(x.x.bar,1,function(x){mean(x^4)})\n  g.3 <- m.3/m.2^(3/2)\n  g.4 <- m.4/m.2^(4/2)\n  return(n*(g.3^2/6 + (g.4-3)^2/24))\n}\nall.n <- c(5,10,20,100)\nnb.sim <- 10000\ny <- matrix(rnorm(nb.sim*max(all.n)),nb.sim,max(all.n))\n\npar(mfrow=c(2,2));par(plt=c(.1,.95,.15,.8))\nfor(i in 1:length(all.n)){\n  n <- all.n[i]\n  hist(JB(y[,1:n]),nclass = 200,freq = FALSE,\n       main=paste(\"n = \",toString(n),sep=\"\"),xlim=c(0,10))\n  xx <- seq(0,10,by=.01)\n  lines(xx,dchisq(xx,df = 2),col=\"red\")\n}"},{"path":"ChapterLS.html","id":"ChapterLS","chapter":"4 Linear Regressions","heading":"4 Linear Regressions","text":"Definition 4.1  linear regression model form:\n\\[\\begin{equation}\ny_i = \\boldsymbol\\beta'\\mathbf{x}_{} + \\varepsilon_i,\\tag{4.1}\n\\end{equation}\\]\n\\(\\mathbf{x}_{}=[x_{,1},\\dots,x_{,K}]'\\) vector dimension \\(K \\times 1\\).entity \\(\\), \\(x_{,k}\\)’s, \\(k \\\\{1,\\dots,K\\}\\), explanatory variables, regressors, covariates. variable interest, \\(y_i\\), often called dependent variable, regressand. last term specification, namely \\(\\varepsilon_i\\), called error, disturbance.researcher usually interested components vector \\(\\boldsymbol\\beta\\), denoted \\(\\beta_k\\), \\(k \\\\{1,\\dots,K\\}\\). usually aims estimating coefficients based observations \\(\\{y_i,\\mathbf{x}_{}\\}\\), \\(\\\\{1,\\dots,n\\}\\), constitutes sample. following, denote sample length \\(n\\).intercept specification (4.1), one set \\(x_{,1}=1\\) \\(\\); \\(\\beta_1\\) corresponds intercept.","code":""},{"path":"ChapterLS.html","id":"linearHyp","chapter":"4 Linear Regressions","heading":"4.1 Hypotheses","text":"section, introduce different assumptions regarding covariates /errors. properties estimators used researcher depend assumptions satisfied.Hypothesis 4.1  (Full rank) exact linear relationship among independent variables (\\(x_{,k}\\)’s, given \\(\\\\{1,\\dots,n\\}\\)).Intuitively, Hypothesis 4.1 satisfied, estimation model parameters unfeasible since, value \\(\\boldsymbol\\beta\\), changes explanatory variables exactly compensated changes another set explanatory variables, preventing identification effects.Let us denote \\(\\mathbf{X}\\) matrix containing explanatory variables, dimension \\(n \\times K\\). (, row \\(\\) \\(\\mathbf{X}\\) \\(\\mathbf{x}_i'\\).) following hypothesis concerns relationship errors (gathered \\(\\boldsymbol\\varepsilon\\), \\(n\\)-dimensional vector) explanatory variables \\(\\mathbf{X}\\):Hypothesis 4.2  (Conditional mean-zero assumption) \\[\\begin{equation}\n\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X}) = 0.\n\\end{equation}\\]Hypothesis 4.2 important implications:Proposition 4.1  Hypothesis 4.2:\\(\\mathbb{E}(\\varepsilon_{})=0\\);\\(x_{ij}\\)’s \\(\\varepsilon_{}\\)’s uncorrelated, .e. \\(\\forall ,\\,j \\quad \\mathbb{C}orr(x_{ij},\\varepsilon_{})=0\\).Proof. Let us prove () (ii):law iterated expectations:\n\\[\n\\mathbb{E}(\\boldsymbol\\varepsilon)=\\mathbb{E}(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X}))=\\mathbb{E}(0)=0.\n\\]\\(\\mathbb{E}(x_{ij}\\varepsilon_i)=\\mathbb{E}(\\mathbb{E}(x_{ij}\\varepsilon_i|\\mathbf{X}))=\\mathbb{E}(x_{ij}\\underbrace{\\mathbb{E}(\\varepsilon_i|\\mathbf{X})}_{=0})=0\\).next two hypotheses (4.3 4.4) concern stochastic properties errors \\(\\varepsilon_i\\):Hypothesis 4.3  (Homoskedasticity) \\[\n\\forall , \\quad \\mathbb{V}ar(\\varepsilon_i|\\mathbf{X}) = \\sigma^2.\n\\]following lines code generate figure comparing two situations: Panel () Figure 4.1 corresponds situation homoskedasticity, Panel (b) corresponds situation heteroskedasticity. Let us specific. two plots, \\(X_i \\sim \\mathcal{N}(0,1)\\) \\(\\varepsilon^*_i \\sim \\mathcal{N}(0,1)\\). Panel () (homoskedasticity):\n\\[\nY_i = 2 + 2X_i + \\varepsilon^*_i.\n\\]\nPanel (b) (heteroskedasticity):\n\\[\nY_i = 2 + 2X_i + \\left(2\\mathbb{}_{\\{X_i<0\\}}+0.2\\mathbb{}_{\\{X_i\\ge0\\}}\\right)\\varepsilon^*_i\\].\nFigure 4.1: Homoskedasticity vs heteroskedasticity. See text exact specifications.\nFigure 4.2 shows real-data situation heteroskedasticity, based data taken Swiss Household Panel. sample restricted persons () younger 35 year 2019, (ii) completed least 19 years study. figure shows dispersion yearly income increases age.\nFigure 4.2: Income versus age. Data Swiss Household Panel. sample restricted persons completed least 19 years study. figure shows dispersion yearly income increases age.\nnext assumption concerns correlation errors across entities.Hypothesis 4.4  (Uncorrelated errors) \\[\n\\forall \\ne j, \\quad \\mathbb{C}ov(\\varepsilon_i,\\varepsilon_j|\\mathbf{X})=0.\n\\]often need work covariance matrix errors. Proposition 4.2 give specific form covariance matrix errors —conditional \\(\\mathbf{X}\\)— Hypotheses 4.3 4.4 satisfied:Proposition 4.2  Hypotheses 4.3 4.4 hold, :\n\\[\n\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})= \\sigma^2 Id,\n\\]\n\\(Id\\) \\(n \\times n\\) identity matrix.sometimes assume errors Gaussian—normal. invoke Hypothesis 4.5:Hypothesis 4.5  (Normal distribution) \\[\n\\forall , \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2).\n\\]","code":"\nN <- 200\nX <- rnorm(N);eps <- rnorm(N)\npar(mfrow=c(1,2),plt=c(.2,.95,.2,.8))\nY <- 2 + 2*X + eps\nplot(X,Y,pch=19,main=\"(a) Homoskedasticity\",\n     las=1,cex.lab=.8,cex.axis=.8,cex.main=.8,)\nY <- 2 + 2*X + eps*( (X<0)*2 + (X>=0)*.2 )\nplot(X,Y,pch=19,main=\"(b) Heteroskedasticity\",\n     las=1,cex.lab=.8,cex.axis=.8,cex.main=.8,)\nlibrary(AEC)\ntable(shp$edyear19)## \n##    8    9   10   12   13   14   16   19   21 \n##   70  325  350 1985  454  117  990 1263  168\nshp_higherEd <- subset(shp,(edyear19>18)&age19<35)\nplot(i19wyg/1000~age19,data=shp_higherEd,pch=19,las=1,\n     xlab=\"Age\",ylab=\"Yearly work income\")\nabline(lm(i19wyg/1000~age19,data=shp_higherEd),col=\"red\",lwd=2)"},{"path":"ChapterLS.html","id":"LSquares","chapter":"4 Linear Regressions","heading":"4.2 Least square estimation","text":"","code":""},{"path":"ChapterLS.html","id":"derivation-of-the-ols-formula","chapter":"4 Linear Regressions","heading":"4.2.1 Derivation of the OLS formula","text":"section, present study properties popular estimation approach, namely Ordinary Least Squares (OLS) approach. suggested name, OLS estimator \\(\\boldsymbol\\beta\\) defined vector \\(\\mathbf{b}\\) minimizes sum squared residuals. (residuals estimates errors \\(\\varepsilon_i\\).)given vector coefficients \\(\\mathbf{b}=[b_1,\\dots,b_K]'\\), sum squared residuals :\n\\[\nf(\\mathbf{b}) =\\sum_{=1}^n \\left(y_i - \\sum_{j=1}^K x_{,j} b_j \\right)^2 = \\sum_{=1}^n (y_i - \\mathbf{x}_i' \\mathbf{b})^2.\n\\]\nMinimizing sum amounts minimizing:\n\\[\nf(\\mathbf{b}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{b})'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}).\n\\]Since:5\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = - 2 \\mathbf{X}'\\mathbf{y} + 2 \\mathbf{X}'\\mathbf{X}\\mathbf{b},\n\\]\ncomes necessary first-order condition (FOC) :\n\\[\\begin{equation}\n\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}.\\tag{4.2}\n\\end{equation}\\]\nAssumption 4.1, \\(\\mathbf{X}'\\mathbf{X}\\) invertible. Hence:\n\\[\n\\boxed{\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y}.}\n\\]\nVector \\(\\mathbf{b}\\) minimizes sum squared residuals. (\\(f\\) non-negative quadratic function, therefore admits minimum.):\n\\[\n\\mathbf{y} = \\underbrace{\\mathbf{X}\\mathbf{b}}_{\\mbox{fitted values } (\\hat{\\mathbf{y}})} + \\underbrace{\\mathbf{e}}_{\\mbox{residuals}}\n\\]estimated residuals :\n\\[\\begin{equation}\n\\mathbf{e} = \\mathbf{y} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y} = \\mathbf{M} \\mathbf{y},\\tag{4.3}\n\\end{equation}\\]\n\\(\\mathbf{M} := \\mathbf{} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\) called residual maker matrix.Moreover, fitted values \\(\\hat{\\mathbf{y}}\\) given :\n\\[\\begin{equation}\n\\hat{\\mathbf{y}}=\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y} = \\mathbf{P} \\mathbf{y},\\tag{4.4}\n\\end{equation}\\]\n\\(\\mathbf{P}=\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\) projection matrix.matrices \\(\\mathbf{M}\\) \\(\\mathbf{P}\\) :\\(\\mathbf{M} \\mathbf{X} = \\mathbf{0}\\): one regresses one explanatory variables \\(\\mathbf{X}\\), residuals null.\\(\\mathbf{M}\\mathbf{y}=\\mathbf{M}\\boldsymbol\\varepsilon\\) (\\(\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon\\) \\(\\mathbf{M} \\mathbf{X} = \\mathbf{0}\\)).additional properties \\(\\mathbf{M}\\) \\(\\mathbf{P}\\):\\(\\mathbf{M}\\) symmetric (\\(\\mathbf{M} = \\mathbf{M}'\\)) idempotent (\\(\\mathbf{M} = \\mathbf{M}^2 = \\mathbf{M}^k\\) \\(k>0\\)).\\(\\mathbf{P}\\) symmetric idempotent.\\(\\mathbf{P}\\mathbf{X} = \\mathbf{X}\\).\\(\\mathbf{P} \\mathbf{M} = \\mathbf{M} \\mathbf{P} = 0\\).\\(\\mathbf{y} = \\mathbf{P}\\mathbf{y} + \\mathbf{M}\\mathbf{y}\\) (decomposition \\(\\mathbf{y}\\) two orthogonal parts).easily checked \\(\\mathbf{X}'\\mathbf{e}=0\\). column \\(\\mathbf{X}\\) therefore orthogonal \\(\\mathbf{e}\\). particular, intercept included regression (\\(x_{,1} \\equiv 1\\) \\(\\)’s, .e., first column \\(\\mathbf{X}\\) filled ones), average residuals null.Example 4.1  (Bivariate case) Consider bivariate situation, regress \\(y_i\\) constant explanatory variable \\(w_i\\). \\(K=2\\), \\(\\mathbf{X}\\) \\(n \\times 2\\) matrix whose \\(^{th}\\) row \\([x_{,1},x_{,2}]\\), \\(x_{,1}=1\\) (account intercept) \\(w_i = x_{,2}\\) (say).:\n\\[\\begin{eqnarray*}\n\\mathbf{X}'\\mathbf{X} &=&\n\\left[\\begin{array}{cc}\nn & \\sum_i w_i \\\\\n\\sum_i w_i & \\sum_i w_i^2\n\\end{array}\n\\right],\\\\\n(\\mathbf{X}'\\mathbf{X})^{-1} &=&\n\\frac{1}{n\\sum_i w_i^2-(\\sum_i w_i)^2}\n\\left[\\begin{array}{cc}\n\\sum_i w_i^2 & -\\sum_i w_i \\\\\n-\\sum_i w_i & n\n\\end{array}\n\\right],\\\\\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} &=&\n\\frac{1}{n\\sum_i w_i^2-(\\sum_i w_i)^2}\n\\left[\\begin{array}{c}\n\\sum_i w_i^2\\sum_i y_i -\\sum_i w_i \\sum_i w_iy_i \\\\\n-\\sum_i w_i \\sum_i y_i + n \\sum_i w_i y_i\n\\end{array}\n\\right]\\\\\n&=& \\frac{1}{\\frac{1}{n}\\sum_i(w_i - \\bar{w})^2}\n\\left[\\begin{array}{c}\n\\frac{\\bar{y}}{n}\\sum_i w_i^2 -\\frac{\\bar{w}}{n}\\sum_i w_iy_i \\\\\n\\frac{1}{n}\\sum_i (w_i-\\bar{w})(y_i-\\bar{y})\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]can seen second element \\(\\mathbf{b}=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\) :\n\\[\nb_2 = \\frac{\\overline{\\mathbb{C}ov(W,Y)}}{\\overline{\\mathbb{V}ar(W)}},\n\\]\n\\(\\overline{\\mathbb{C}ov(W,Y)}\\) \\(\\overline{\\mathbb{V}ar(W)}\\) sample estimates.Since constant regression, \\(b_1 = \\bar{y} - b_2 \\bar{w}\\).","code":""},{"path":"ChapterLS.html","id":"properties-of-the-ols-estimate-small-sample","chapter":"4 Linear Regressions","heading":"4.2.2 Properties of the OLS estimate (small sample)","text":"OLS properties stated Proposition 4.3 valid sample size \\(n\\):Proposition 4.3  (Properties OLS estimator) :Assumptions 4.1 4.2, OLS estimator linear unbiased.Assumptions 4.1 4.2, OLS estimator linear unbiased.Hypotheses 4.1 4.4, conditional covariance matrix \\(\\mathbf{b}\\) : \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\).Hypotheses 4.1 4.4, conditional covariance matrix \\(\\mathbf{b}\\) : \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\).Proof. Hypothesis 4.1, \\(\\mathbf{X}'\\mathbf{X}\\) can inverted. :\n\\[\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' {\\boldsymbol\\varepsilon}.\n\\]Let us consider expectation last term, .e. \\(\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' {\\boldsymbol\\varepsilon})\\). Using law iterated expectations, obtain:\n\\[\n\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' {\\boldsymbol\\varepsilon}) = \\mathbb{E}(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' {\\boldsymbol\\varepsilon}|\\mathbf{X}]) = \\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbb{E}[{\\boldsymbol\\varepsilon}|\\mathbf{X}]).\n\\]\nHypothesis 4.2, \\(\\mathbb{E}[{\\boldsymbol\\varepsilon}|\\mathbf{X}]=0\\). Hence \\(\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' {\\boldsymbol\\varepsilon}) =0\\) result () follows.\\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbb{E}(\\boldsymbol\\varepsilon\\boldsymbol\\varepsilon'|\\mathbf{X}) \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1}\\).\nProp. 4.2, 4.3 4.4 hold, \\(\\mathbb{E}(\\boldsymbol\\varepsilon\\boldsymbol\\varepsilon'|\\mathbf{X})=\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})=\\sigma^2 Id\\).Together, Hypotheses 4.1 4.4 form -called Gauss-Markov set assumptions. assumptions, OLS estimator feature lowest possible variance within family linear unbiased estimates \\(\\boldsymbol\\beta\\):Theorem 4.1  (Gauss-Markov Theorem) Assumptions 4.1 4.4, vector \\(w\\), minimum-variance linear unbiased estimator \\(w' \\boldsymbol\\beta\\) \\(w' \\mathbf{b}\\), \\(\\mathbf{b}\\) least squares estimator. (BLUE: Best Linear Unbiased Estimator.)Proof. Consider \\(\\mathbf{b}^* = C \\mathbf{y}\\), another linear unbiased estimator \\(\\boldsymbol\\beta\\). Since unbiased, must \\(\\mathbb{E}(C\\mathbf{y}|\\mathbf{X}) = \\mathbb{E}(C\\mathbf{X}\\boldsymbol\\beta + C\\boldsymbol\\varepsilon|\\mathbf{X}) = \\boldsymbol\\beta\\). \\(\\mathbb{E}(C\\boldsymbol\\varepsilon|\\mathbf{X})=C\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X})=0\\) (4.2). Therefore \\(\\mathbf{b}^*\\) unbiased \\(\\mathbb{E}(C\\mathbf{X})\\boldsymbol\\beta=\\boldsymbol\\beta\\). case \\(\\boldsymbol\\beta\\), implies must \\(C\\mathbf{X}=\\mathbf{}\\). Let us compute \\(\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X})\\). , introduce \\(D = C - (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\), \\(D\\mathbf{y}=\\mathbf{b}^*-\\mathbf{b}\\). fact \\(C\\mathbf{X}=\\mathbf{}\\) implies \\(D\\mathbf{X} = \\mathbf{0}\\). \\(\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X}) = \\mathbb{V}ar(C \\mathbf{y}|\\mathbf{X}) =\\mathbb{V}ar(C \\boldsymbol\\varepsilon|\\mathbf{X}) = \\sigma^2CC'\\) (Assumptions 4.3 4.4, see Prop. 4.2). Using \\(C=D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) exploiting fact \\(D\\mathbf{X} = \\mathbf{0}\\) leads :\n\\[\n\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X}) =\\sigma^2\\left[(D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')(D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')'\\right] = \\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) + \\sigma^2 \\mathbf{D}\\mathbf{D}'.\n\\]\nTherefore, \n\\[\\begin{eqnarray*}\n&&\\mathbb{V}ar(w'\\mathbf{b^*}|\\mathbf{X})=w'\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})w + \\sigma^2 w'\\mathbf{D}\\mathbf{D}'w\\\\\n&\\ge& w'\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})w=\\mathbb{V}ar(w'\\mathbf{b}|\\mathbf{X}).\n\\end{eqnarray*}\\]Frish-Waugh theorem (Theorem 4.2) reveals relationship OLS estimator notion partial correlation coefficient. Consider linear least square regression \\(\\mathbf{y}\\) \\(\\mathbf{X}\\). introduce notations:\\(\\mathbf{b}^{\\mathbf{y}/\\mathbf{X}}\\): OLS estimates \\(\\boldsymbol\\beta\\),\\(\\mathbf{M}^{\\mathbf{X}}\\): residual-maker matrix regression \\(\\mathbf{X}\\) (given \\(\\mathbf{} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\)),\\(\\mathbf{P}^{\\mathbf{X}}\\): projection matrix regression \\(\\mathbf{X}\\) (given \\(\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\)).Let us split set explanatory variables two: \\(\\mathbf{X} = [\\mathbf{X}_1,\\mathbf{X}_2]\\). obvious notations: \\(\\mathbf{b}^{\\mathbf{y}/\\mathbf{X}}=[\\mathbf{b}_1',\\mathbf{b}_2']'\\).Theorem 4.2  (Frisch-Waugh Theorem) :\n\\[\n\\mathbf{b}_2 = \\mathbf{b}^{\\mathbf{M^{\\mathbf{X}_1}y}/\\mathbf{M^{\\mathbf{X}_1}\\mathbf{X}_2}}.\n\\]Proof. minimization least squares leads (first-order conditions, see Eq. (4.2)):\n\\[\n\\left[ \\begin{array}{cc} \\mathbf{X}_1'\\mathbf{X}_1 & \\mathbf{X}_1'\\mathbf{X}_2 \\\\ \\mathbf{X}_2'\\mathbf{X}_1 & \\mathbf{X}_2'\\mathbf{X}_2\\end{array}\\right]\n\\left[ \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2\\end{array}\\right] =\n\\left[ \\begin{array}{c} \\mathbf{X}_1' \\mathbf{y} \\\\ \\mathbf{X}_2' \\mathbf{y} \\end{array}\\right].\n\\]\nUse first-row block equations solve \\(\\mathbf{b}_1\\) first; comes function \\(\\mathbf{b}_2\\). use second set equations solve \\(\\mathbf{b}_2\\), leads :\n\\[\\begin{eqnarray*}\n\\mathbf{b}_2 &=& [\\mathbf{X}_2'\\mathbf{X}_2 - \\mathbf{X}_2'\\mathbf{X}_1(\\mathbf{X}_1'\\mathbf{X}_1)\\mathbf{X}_1'\\mathbf{X}_2]^{-1}\\mathbf{X}_2'(Id - \\mathbf{X}_1(\\mathbf{X}_1'\\mathbf{X}_1)\\mathbf{X}_1')\\mathbf{y}\\\\\n&=& [\\mathbf{X}_2' \\mathbf{M}^{\\mathbf{X}_1}\\mathbf{X}_2]^{-1}\\mathbf{X}_2'\\mathbf{M}^{\\mathbf{X}_1}\\mathbf{y}.\n\\end{eqnarray*}\\]\nUsing fact \\(\\mathbf{M}^{\\mathbf{X}_1}\\) idempotent symmetric leads result.suggests second way estimating \\(\\mathbf{b}_2\\):Regress \\(Y\\) \\(X_1\\), regress \\(X_2\\) \\(X_1\\).Regress residuals associated former regression associated latter regressions.illustrated following code, run different regressions involving number Google searches “parapluie” (umbrella French). broad specification, regress French precipitations month dummies. Next, deseasonalize dependent variable precipitations regressing month dummies. stated Theorem 4.2, regressing deseasonalized Google searches deseasonalized precipitations give coefficient baseline regression.\\(b_2\\) scalar (\\(\\mathbf{X}_2\\) dimension \\(n \\times 1\\)), Theorem 4.2 gives expression partial regression coefficient \\(b_2\\):\n\\[\nb_2 = \\frac{\\mathbf{X}_2'M^{\\mathbf{X}_1}\\mathbf{y}}{\\mathbf{X}_2'M^{\\mathbf{X}_1}\\mathbf{X}_2}.\n\\]","code":"\nlibrary(AEC)\ndummies <- as.matrix(parapluie[,4:14])\neq_all <- lm(parapluie~dummies+precip,data=parapluie)\ndeseas_parapluie <- lm(parapluie~dummies,data=parapluie)$residuals\ndeseas_precip    <- lm(precip~dummies,data=parapluie)$residuals\neq_frac <- lm(deseas_parapluie~deseas_precip-1)\nstargazer::stargazer(eq_all,eq_frac,omit=c(1:11,\"Constant\"),type=\"text\",\n                     omit.stat = c(\"f\",\"ser\"),digits=5,\n                     add.lines=list(c('Monthly dummy','Yes','No')))## \n## ==========================================\n##                   Dependent variable:     \n##               ----------------------------\n##                parapluie  deseas_parapluie\n##                   (1)           (2)       \n## ------------------------------------------\n## precip        0.13001***                  \n##                (0.03594)                  \n##                                           \n## deseas_precip                0.13001***   \n##                              (0.03277)    \n##                                           \n## ------------------------------------------\n## Monthly dummy     Yes            No       \n## Observations      72             72       \n## R2              0.51793       0.18148     \n## Adjusted R2     0.41988       0.16995     \n## ==========================================\n## Note:          *p<0.1; **p<0.05; ***p<0.01"},{"path":"ChapterLS.html","id":"goodness-of-fit","chapter":"4 Linear Regressions","heading":"4.2.3 Goodness of fit","text":"Define total variation \\(y\\) sum squared deviations (sample mean):\n\\[\nTSS = \\sum_{=1}^{n} (y_i - \\bar{y})^2.\n\\]\n:\n\\[\n\\mathbf{y} = \\mathbf{X}\\mathbf{b} + \\mathbf{e} = \\hat{\\mathbf{y}} + \\mathbf{e}\n\\]\nfollowing, assume regression includes constant (.e. \\(\\), \\(x_{,1}=1\\)). Denote \\(\\mathbf{M}^0\\) matrix transforms observations deviations sample means. Using \\(\\mathbf{M}^0 \\mathbf{e} = \\mathbf{e}\\) \\(\\mathbf{X}' \\mathbf{e}=0\\), :\n\\[\\begin{eqnarray*}\n\\underbrace{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}}_{\\mbox{Total sum sq.}} &=& (\\mathbf{X}\\mathbf{b} + \\mathbf{e})' \\mathbf{M}^0 (\\mathbf{X}\\mathbf{b} + \\mathbf{e})\\\\\n&=& \\underbrace{\\mathbf{b}' \\mathbf{X}' \\mathbf{M}^0 \\mathbf{X}\\mathbf{b}}_{\\mbox{\"Explained\" sum sq.}} + \\underbrace{\\mathbf{e}'\\mathbf{e}}_{\\mbox{Sum sq. residuals}}\\\\\nTSS &=& Expl.SS + SSR.\n\\end{eqnarray*}\\]can now define coefficient determination:\n\\[\\begin{equation}\n\\boxed{\\mbox{Coefficient determination} = \\frac{Expl.SS}{TSS} = 1 - \\frac{SSR}{TSS} = 1 - \\frac{\\mathbf{e}'\\mathbf{e}}{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}}.}\\tag{4.5}\n\\end{equation}\\]can shown (Greene (2003), Section 3.5) :\n\\[\n\\mbox{Coefficient determination} = \\frac{[\\sum_{=1}^n(y_i - \\bar{y})(\\hat{y_i} - \\bar{y})]^2}{\\sum_{=1}^n(y_i - \\bar{y})^2 \\sum_{=1}^n(\\hat{y_i} - \\bar{y})^2}.\n\\]\n, \\(R^2\\) sample squared correlation \\(y\\) (regression-implied) \\(y\\)’s predictions.hgher \\(R^2\\), higher goodness fit model. One however cautious \\(R^2\\). Indeed, easy increase : suffices add explanatory variables. stated Proposition 4.5, adding explanatory variable (even truly relate dependent variable) mechanically results increase \\(R^2\\). limit, taking set \\(n\\) non-linearly-dependent explanatory variables (.e., variables satisfying Hypothesis 4.1) results \\(R^2\\) equal one.Proposition 4.4  (Change SSR variable added) :\n\\[\\begin{equation}\n\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e} - c^2(\\mathbf{z^*}'\\mathbf{z^*}) \\qquad (\\le \\mathbf{e}'\\mathbf{e}) \\tag{4.6}\n\\end{equation}\\]\n() \\(\\mathbf{u}\\) \\(\\mathbf{e}\\) residuals regressions \\(\\mathbf{y}\\) \\([\\mathbf{X},\\mathbf{z}]\\) \\(\\mathbf{y}\\) \\(\\mathbf{X}\\), respectively, (ii) \\(c\\) regression coefficient \\(\\mathbf{z}\\) former regression \\(\\mathbf{z}^*\\) residuals regression \\(\\mathbf{z}\\) \\(\\mathbf{X}\\).Proof. OLS estimates \\([\\mathbf{d}',\\mathbf{c}]'\\) regression \\(\\mathbf{y}\\) \\([\\mathbf{X},\\mathbf{z}]\\) satisfies (first-order cond., Eq. (4.2)):\n\\[\n\\left[ \\begin{array}{cc} \\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{z} \\\\ \\mathbf{z}'\\mathbf{X} & \\mathbf{z}'\\mathbf{z}\\end{array}\\right]\n\\left[ \\begin{array}{c} \\mathbf{d} \\\\ c\\end{array}\\right] =\n\\left[ \\begin{array}{c} \\mathbf{X}' \\mathbf{y} \\\\ \\mathbf{z}' \\mathbf{y} \\end{array}\\right].\n\\]\nHence, particular \\(\\mathbf{d} = \\mathbf{b} - (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{z}c\\), \\(\\mathbf{b}\\) OLS \\(\\mathbf{y}\\) \\(\\mathbf{X}\\). Substituting \\(\\mathbf{u} = \\mathbf{y} - \\mathbf{X}\\mathbf{d} - \\mathbf{z}c\\), get \\(\\mathbf{u} = \\mathbf{e} - \\mathbf{z}^*c\\). therefore :\n\\[\\begin{equation}\n\\mathbf{u}'\\mathbf{u} = (\\mathbf{e} - \\mathbf{z}^*c)(\\mathbf{e} - \\mathbf{z}^*c)= \\mathbf{e}'\\mathbf{e} + c^2(\\mathbf{z^*}'\\mathbf{z^*}) - 2 c\\mathbf{z^*}'\\mathbf{e}.\\tag{4.7}\n\\end{equation}\\]\nNow \\(\\mathbf{z^*}'\\mathbf{e} = \\mathbf{z^*}'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}) = \\mathbf{z^*}'\\mathbf{y}\\) \\(\\mathbf{z}^*\\) residuals OLS regression \\(\\mathbf{X}\\). Since \\(c = (\\mathbf{z^*}'\\mathbf{z^*})^{-1}\\mathbf{z^*}'\\mathbf{y^*}\\) (application Theorem 4.2), \\((\\mathbf{z^*}'\\mathbf{z^*})c = \\mathbf{z^*}'\\mathbf{y^*}\\) , therefore, \\(\\mathbf{z^*}'\\mathbf{e} = (\\mathbf{z^*}'\\mathbf{z^*})c\\). Inserting Eq. (4.7) leads results.Proposition 4.5  (Change coefficient determination variable added) Denoting \\(R_W^2\\) coefficient determination regression \\(\\mathbf{y}\\) variable \\(\\mathbf{W}\\), :\n\\[\nR_{\\mathbf{X},\\mathbf{z}}^2 = R_{\\mathbf{X}}^2 + (1-R_{\\mathbf{X}}^2)(r_{yz}^\\mathbf{X})^2,\n\\]\n\\(r_{yz}^\\mathbf{X}\\) coefficient partial correlation (see Definition 9.5).Proof. Let’s use notations Prop. 4.4. Theorem 4.2 implies \\(c = (\\mathbf{z^*}'\\mathbf{z^*})^{-1}\\mathbf{z^*}'\\mathbf{y^*}\\). Using Eq. (4.6) gives \\(\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e} - (\\mathbf{z^*}'\\mathbf{y^*})^2/(\\mathbf{z^*}'\\mathbf{z^*})\\). Using definition partial correlation (Eq. (9.2)), get \\(\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e}\\left(1 - (r_{yz}^\\mathbf{X})^2\\right)\\). results obtained dividing sides previous equation \\(\\mathbf{y}'\\mathbf{M}_0\\mathbf{y}\\).Figure 4.3, , illustrates fact one can obtain \\(R^2\\) one regressing sample length \\(n\\) set \\(n\\) linearly-independent variables.\nFigure 4.3: figure illustrates monotonous increase \\(R^2\\) function number explanatory variables. true model, explanatory variables, .e., \\(y_i = \\varepsilon_i\\). take (independent) regressors regress \\(y\\) latter, progressively increasing set regressors.\norder address risk adding irrelevant explanatory variables, measures adjusted \\(R^2\\) proposed. Compared standard \\(R^2\\), measures add penalties depend number covariates employed regression. common adjusted \\(R^2\\) measure, denoted \\(\\bar{R}^2\\), following:\n\\[\\begin{equation*}\n\\boxed{\\bar{R}^2 = 1 - \\frac{\\mathbf{e}'\\mathbf{e}/(n-K)}{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}/(n-1)} = 1 - \\frac{n-1}{n-K}(1-R^2).}\n\\end{equation*}\\]","code":"\nn <- 30;Y <- rnorm(n);X <- matrix(rnorm(n^2),n,n)\nall_R2 <- NULL;all_adjR2 <- NULL\nfor(j in 0:(n-1)){\n  if(j==0){eq <- lm(Y~1)}else{eq <- lm(Y~X[,1:j])}\n  all_R2 <- c(all_R2,summary(eq)$r.squared)\n  all_adjR2 <- c(all_adjR2,summary(eq)$adj.r.squared)\n}\npar(plt=c(.15,.95,.25,.95))\nplot(all_R2,pch=19,ylim=c(min(all_adjR2,na.rm = TRUE),1),\n     xlab=\"number of regressors\",ylab=\"R2\")\npoints(all_adjR2,pch=3);abline(h=0,col=\"light grey\",lwd=2)\nlegend(\"topleft\",c(\"R2\",\"Adjusted R2\"),\n       lty=NaN,col=c(\"black\"),pch=c(19,3),lwd=2)"},{"path":"ChapterLS.html","id":"inference-and-confidence-intervals-in-small-sample","chapter":"4 Linear Regressions","heading":"4.2.4 Inference and confidence intervals (in small sample)","text":"normality assumption (Assumption 4.5), know distribution \\(\\mathbf{b}\\) (conditional \\(\\mathbf{X}\\)). Indeed, \\(\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\boldsymbol\\varepsilon\\). Therefore, conditional \\(\\mathbf{X}\\), vector \\(\\mathbf{b}\\) affine combination Gaussian variables—components \\(\\boldsymbol\\varepsilon\\). result, also Gaussian. distribution therefore completely characterized mean variance, :\n\\[\\begin{equation}\n\\mathbf{b}|\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol\\beta,\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}).\\tag{4.8}\n\\end{equation}\\]Eq. (4.8) can used conduct inference tests. However, practice, know \\(\\sigma^2\\) (population parameter). following proposition gives unbiased estimate \\(\\sigma^2\\).Proposition 4.6  4.1 4.4, unbiased estimate \\(\\sigma^2\\) given :\n\\[\\begin{equation}\ns^2 = \\frac{\\mathbf{e}'\\mathbf{e}}{n-K}.\\tag{4.9}\n\\end{equation}\\]\n(sometimes denoted \\(\\sigma^2_{OLS}\\).)Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{e}'\\mathbf{e}|\\mathbf{X})&=&\\mathbb{E}(\\boldsymbol{\\varepsilon}'\\mathbf{M}\\boldsymbol{\\varepsilon}|\\mathbf{X})=\\mathbb{E}(\\mbox{Tr}(\\boldsymbol{\\varepsilon}'\\mathbf{M}\\boldsymbol{\\varepsilon})|\\mathbf{X}))\\\\\n&=&\\mbox{Tr}(\\mathbf{M}\\mathbb{E}(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'|\\mathbf{X}))=\\sigma^2 \\mbox{Tr}(\\mathbf{M}).\n\\end{eqnarray*}\\]\n(Note \\(\\mathbb{E}(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'|\\mathbf{X})=\\sigma^2Id\\) Assumptions 4.3 4.4, see Prop. 4.2.) Moreover:\n\\[\\begin{eqnarray*}\n\\mbox{Tr}(\\mathbf{M})&=&n-\\mbox{Tr}(\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')\\\\\n&=&n-\\mbox{Tr}((\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X})=n-\\mbox{Tr}(Id_{K \\times K}),\n\\end{eqnarray*}\\]\nleads result.Two results prove important produce inference:know conditional distribution \\(s^2\\) (Prop. 4.7).\\(s^2\\) \\(\\mathbf{b}\\) independent random variables (Prop. 4.8).Proposition 4.7  4.1 4.5, : \\(\\dfrac{s^2}{\\sigma^2} | \\mathbf{X} \\sim \\frac{1}{n-K}\\chi^2(n-K)\\).Proof. \\(\\mathbf{e}'\\mathbf{e}=\\boldsymbol\\varepsilon'\\mathbf{M}\\boldsymbol\\varepsilon\\). \\(\\mathbf{M}\\) idempotent symmetric matrix. Therefore can decomposed \\(PDP'\\) \\(D\\) diagonal matrix \\(P\\) orthogonal matrix. result \\(\\mathbf{e}'\\mathbf{e} = (P'\\boldsymbol\\varepsilon)'D(P'\\boldsymbol\\varepsilon)\\), .e. \\(\\mathbf{e}'\\mathbf{e}\\) weighted sum independent squared Gaussian variables (entries \\(P'\\boldsymbol\\varepsilon\\) independent Gaussian —4.5— uncorrelated). variance ..d. Gaussian variable \\(\\sigma^2\\). \\(\\mathbf{M}\\) idempotent symmetric matrix, eigenvalues either 0 1, rank equals trace (see Propositions 9.3 9.4). , trace equal \\(n-K\\) (see proof Eq. (4.9)). Therefore \\(D\\) \\(n-K\\) entries equal 1 \\(K\\) equal 0. Hence, \\(\\mathbf{e}'\\mathbf{e} = (P'\\boldsymbol\\varepsilon)'D(P'\\boldsymbol\\varepsilon)\\) sum \\(n-K\\) squared independent Gaussian variables variance \\(\\sigma^2\\). Therefore \\(\\frac{\\mathbf{e}'\\mathbf{e}}{\\sigma^2} = (n-K)\\frac{s^2}{\\sigma^2}\\) sum \\(n-k\\) squared ..d. standard normal variables. result follows definition chi-square distribution (see Def. 9.11).Proposition 4.8  Hypotheses 4.1 4.5, \\(\\mathbf{b}\\) \\(s^2\\) independent.Proof. \\(\\mathbf{b}=\\boldsymbol\\beta + [\\mathbf{X}'{\\mathbf{X}}]^{-1}\\mathbf{X}\\boldsymbol\\varepsilon\\) \\(s^2 = \\boldsymbol\\varepsilon' \\mathbf{M} \\boldsymbol\\varepsilon/(n-K)\\). Hence \\(\\mathbf{b}\\) affine combination \\(\\boldsymbol\\varepsilon\\) \\(s^2\\) quadratic combination Gaussian shocks. One can write \\(s^2\\) \\(s^2 = (\\mathbf{M}\\boldsymbol\\varepsilon)' \\mathbf{M} \\boldsymbol\\varepsilon/(n-K)\\) \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta + \\mathbf{T}\\boldsymbol\\varepsilon\\). Since \\(\\mathbf{T}\\mathbf{M}=0\\), \\(\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{M}\\boldsymbol\\varepsilon\\) independent (two uncorrelated Gaussian variables independent), therefore \\(\\mathbf{b}\\) \\(s^2\\), functions two sets independent variables, independent.Consistently Eq. (4.8), Hypotheses 4.1 4.5, \\(k^{th}\\) entry \\(\\mathbf{b}\\) satisfies:\n\\[\nb_k | \\mathbf{X} \\sim \\mathcal{N}(\\beta_k,\\sigma^2 v_k),\n\\]\n\\(v_k\\) k\\(^{th}\\) component diagonal \\((\\mathbf{X}'\\mathbf{X})^{-1}\\).Moreover, (Prop. 4.7):\n\\[\n\\frac{(n-K)s^2}{\\sigma^2} | \\mathbf{X} \\sim \\chi ^2 (n-K).\n\\]result (using Propositions 4.7 4.8), :\n\\[\\begin{equation}\n\\boxed{t_k = \\frac{\\frac{b_k - \\beta_k}{\\sqrt{\\sigma^2 v_k}}}{\\sqrt{\\frac{(n-K)s^2}{\\sigma^2(n-K)}}} = \\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} \\sim t(n-K),}\\tag{4.10}\n\\end{equation}\\]\n\\(t(n-K)\\) denotes Student \\(t\\) distribution \\(n-K\\) degrees freedom (see Def. 9.10).6Note \\(s^2 v_k\\) exactly conditional variance \\(b_k\\): variance \\(b_k\\) conditional \\(\\mathbf{X}\\) \\(\\sigma^2 v_k\\). However \\(s^2 v_k\\) unbiased estimate \\(\\sigma^2 v_k\\) (Prop. 4.6).previous result (Eq. (4.10)) can extended linear combinations elements \\(\\mathbf{b}\\). (Eq. (4.10) \\(k^{th}\\) component .) Let us consider \\(\\boldsymbol\\alpha'\\mathbf{b}\\), OLS estimate \\(\\boldsymbol\\alpha'\\boldsymbol\\beta\\). Eq. (4.8), :\n\\[\n\\boldsymbol\\alpha'\\mathbf{b} | \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol\\alpha'\\boldsymbol\\beta,\\sigma^2 \\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha).\n\\]\nTherefore:\n\\[\n\\frac{\\boldsymbol\\alpha'\\mathbf{b} - \\boldsymbol\\alpha'\\boldsymbol\\beta}{\\sqrt{\\sigma^2 \\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha}} | \\mathbf{X} \\sim \\mathcal{N}(0,1).\n\\]\nUsing approach one used derive Eq. (4.10), one can show Props. 4.7 4.8 also imply :\n\\[\\begin{equation}\n\\boxed{\\frac{\\boldsymbol\\alpha'\\mathbf{b} - \\boldsymbol\\alpha'\\boldsymbol\\beta}{\\sqrt{s^2\\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha}} \\sim t(n-K).}\\tag{4.11}\n\\end{equation}\\]\nFigure 4.4: higher degree freedom, closer distribution \\(t(\\nu)\\) gets normal distribution. (Convergence distribution.)\nprecedes widely exploited statistical inference context linear regressions. Indeed, Eq. (4.10) gives sense distances \\(b_k\\) \\(\\beta_k\\) can deemed “likely” (, conversely, “unlikely”). instance, implies , \\(\\sqrt{v_k s^2}\\) equal 1 (say), probability obtain \\(b_k\\) smaller \\(\\beta_k-\\) 4.587 \\(\\times \\sqrt{v_k s^2}\\) larger \\(\\beta_k+\\) 4.587 \\(\\times \\sqrt{v_k s^2}\\) equal 0.1% \\(n-K=10\\).means instance , assumption \\(\\beta_k=0\\), extremely unlikely obtained \\(b_k/\\sqrt{v_k s^2}\\) smaller -4.587 larger 4.587. generally, shows t-statistic, .e., ratio \\(b_k/\\sqrt{v_k s^2}\\), test statistic associated null hypothesis:\n\\[\nH_0: \\beta_k=0.\n\\]\nnull hypothesis, test statistic follows Student-t distribution \\(n-K\\) degrees freedom. t-statistic therefore particular importance, , result, routinely reported regression outputs (see Example 4.2).Example 4.2  (Education income) Consider regression aims determining covariates households’ income. example makes use data Swiss Household Panel (SHP); edyear19 number years education age19 age respondent, 2019.last two columns previous table give t-statistic p-values associated t-tests, whose size-\\(\\alpha\\) critical region :\n\\[\n\\left]-\\infty,-\\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\right] \\cup \\left[\\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right),+\\infty\\right[.\n\\]recall p-value defined probability \\(|Z| > |t|\\), \\(t\\) (computed) t-statistics \\(Z \\sim t(n-K)\\). , context t-test, p-value given \\(2(1 - \\Phi_{t(n-K)}(|t_k|))\\). See webpage details regarding link critical regions, p-value, test outcomes.Now, suppose want compute (symmetrical) confidence interval \\([I_{d,1-\\alpha},I_{u,1-\\alpha}]\\) \\(\\mathbb{P}(\\beta_k \\[I_{d,1-\\alpha},I_{u,1-\\alpha}])=1-\\alpha\\). , want : \\(\\mathbb{P}(\\beta_k < I_{d,1-\\alpha})=\\frac{\\alpha}{2}\\) \\(\\mathbb{P}(\\beta_k > I_{u,1-\\alpha})=\\frac{\\alpha}{2}\\). Let us focus \\(I_{d,1-\\alpha}\\) start . Using Eq. (4.10), .e., \\(t_k = \\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} \\sim t(n-K)\\), :\n\\[\\begin{eqnarray*}\n\\mathbb{P}(\\beta_k < I_{d,1-\\alpha})=\\frac{\\alpha}{2} &\\Leftrightarrow& \\\\\n\\mathbb{P}\\left(\\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} > \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} &\\Leftrightarrow& \\mathbb{P}\\left(t_k > \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} \\Leftrightarrow\\\\\n1 - \\mathbb{P}\\left(t_k \\le \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} &\\Leftrightarrow& \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}} = \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right),\n\\end{eqnarray*}\\]\n\\(\\Phi_{t(n-K)}(\\alpha)\\) c.d.f. \\(t(n-K)\\) distribution (Table 9.2).\\(I_{u,1-\\alpha}\\), obtain:\n\\[\\begin{eqnarray*}\n&&[I_{d,1-\\alpha},I_{u,1-\\alpha}] =\\\\\n&&\\left[b_k - \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\sqrt{s^2v_k},b_k + \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\sqrt{s^2v_k}\\right].\n\\end{eqnarray*}\\]Using results presented Example 4.2, can compute lower upper bounds 95% confidence intervals estimated parameters follows:","code":"\nlibrary(AEC)\nlibrary(sandwich)\nshp$income <- shp$i19ptotn/1000\nshp$female <- 1*(shp$sex19==2)\neq <- lm(income ~ edyear19 + age19 + I(age19^2) + female,data=shp)\nlmtest::coeftest(eq)## \n## t test of coefficients:\n## \n##                Estimate  Std. Error t value  Pr(>|t|)    \n## (Intercept) -71.9738073   5.7082456 -12.609 < 2.2e-16 ***\n## edyear19      4.8442661   0.2172320  22.300 < 2.2e-16 ***\n## age19         3.2386215   0.2183812  14.830 < 2.2e-16 ***\n## I(age19^2)   -0.0289498   0.0020915 -13.842 < 2.2e-16 ***\n## female      -31.8089006   1.4578004 -21.820 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nn <- length(eq$residuals); K <- length(eq$coefficients)\nlower.b <- eq$coefficients + qt(.025,df=n-K)*sqrt(diag(vcov(eq)))\nupper.b <- eq$coefficients + qt(.975,df=n-K)*sqrt(diag(vcov(eq)))\ncbind(lower.b,upper.b)##                  lower.b      upper.b\n## (Intercept) -83.16413225 -60.78348237\n## edyear19      4.41840914   5.27012310\n## age19         2.81051152   3.66673148\n## I(age19^2)   -0.03304986  -0.02484977\n## female      -34.66674188 -28.95105932"},{"path":"ChapterLS.html","id":"Ftest","chapter":"4 Linear Regressions","heading":"4.2.5 Testing a set of linear restrictions","text":"sometimes want test set restrictions jointly consistent data hand. Let us formalize set (\\(J\\)) linear restrictions:\n\\[\\begin{equation}\\label{eq:restrictions}\n\\begin{array}{ccc}\nr_{1,1} \\beta_1 + \\dots + r_{1,K} \\beta_K &=& q_1\\\\\n\\vdots && \\vdots\\\\\nr_{J,1} \\beta_1 + \\dots + r_{J,K} \\beta_K &=& q_J.\n\\end{array}\n\\end{equation}\\]\nmatrix form, get:\n\\[\\begin{equation}\n\\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}.\n\\end{equation}\\]Define discrepancy vector \\(\\mathbf{m} = \\mathbf{R}\\mathbf{b} - \\mathbf{q}\\). null hypothesis:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{m}|\\mathbf{X}) &=& \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} = 0 \\quad \\mbox{} \\\\\n\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X}) &=& \\mathbf{R} \\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\mathbf{R}'.\n\\end{eqnarray*}\\]notations, assumption test :\n\\[\\begin{equation}\n\\boxed{H_0: \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} = 0 \\mbox{ } H_1: \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} \\ne 0.}\\tag{4.12}\n\\end{equation}\\]Hypotheses 4.1 4.4, \\(\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X}) = \\sigma^2 \\mathbf{R} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\) (see Prop. 4.3). add normality assumption (Hypothesis 4.5), :\n\\[\\begin{equation}\nW = \\mathbf{m}'\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X})^{-1}\\mathbf{m} \\sim \\chi^2(J). \\tag{4.13}\n\\end{equation}\\]\\(\\sigma^2\\) known, conduct Wald test (directly exploiting Eq. (4.13)). case practice compute \\(W\\). can, however, approximate replacing \\(\\sigma^2\\) \\(s^2\\) (given Eq. (4.9)). distribution new statistic \\(\\chi^2(J)\\) ; \\(\\mathcal{F}\\) distribution (whose quantiles shown Table 9.4), test called \\(F\\) test.Proposition 4.9  Hypotheses 4.1 4.5 Eq. (4.12) holds, :\n\\[\\begin{equation}\nF = \\frac{W}{J}\\frac{\\sigma^2}{s^2} = \\frac{\\mathbf{m}'(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\mathbf{m}}{s^2J} \\sim \\mathcal{F}(J,n-K),\\tag{4.14}\n\\end{equation}\\]\n\\(\\mathcal{F}\\) distribution F-statistic (see Def. 9.9).Proof. According Eq. (4.13), \\(W/J \\sim \\chi^2(J)/J\\). Moreover, denominator (\\(s^2/\\sigma^2\\)) \\(\\sim \\chi^2(n-K)\\). Therefore, \\(F\\) ratio r.v. distributed \\(\\chi^2(J)/J\\) another distributed \\(\\chi^2(n-K)/(n-K)\\). remains verify r.v. independent. \\(H_0\\), \\(\\mathbf{m} = \\mathbf{R}(\\mathbf{b}-\\boldsymbol\\beta) = \\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon\\).\nTherefore \\(\\mathbf{m}'(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\mathbf{m}\\) form \\(\\boldsymbol\\varepsilon'\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{T}=\\mathbf{D}'\\mathbf{C}\\mathbf{D}\\) \\(\\mathbf{D}=\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) \\(\\mathbf{C}=(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\). Hypotheses 4.1 4.4, covariance \\(\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{M}\\boldsymbol\\varepsilon\\) \\(\\sigma^2\\mathbf{T}\\mathbf{M} = \\mathbf{0}\\). Therefore, 4.5, variables Gaussian variables 0 covariance. Hence independent.large \\(n-K\\), \\(\\mathcal{F}_{J,n-K}\\) distribution converges \\(\\mathcal{F}_{J,\\infty}=\\chi^2(J)/J\\). implies , large samples, F-statistic approximately \\(\\chi^2\\) distribution. words, one can approximately employ Eq. (4.13) perform Wald test (one just replace \\(\\sigma^2\\) \\(s^2\\) computing \\(\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X})\\)).following proposition proposes another equivalent computation F-statistic, based \\(R^2\\) restricted unrestricted linear models.Proposition 4.10  F-statistic defined Eq. (4.14) also equal :\n\\[\\begin{equation}\nF = \\frac{(R^2-R_*^2)/J}{(1-R^2)/(n-K)} =  \\frac{(SSR_{restr}-SSR_{unrestr})/J}{SSR_{unrestr}/(n-K)},\\tag{4.15}\n\\end{equation}\\]\n\\(R_*^2\\) coef. determination (Eq. (4.5)) “restricted regression” (SSR: sum squared residuals.)Proof. Let’s denote \\(\\mathbf{e}_*=\\mathbf{y}-\\mathbf{X}\\mathbf{b}_*\\) vector residuals associated restricted regression (.e. \\(\\mathbf{R}\\mathbf{b}_*=\\mathbf{q}\\)).\n\\(\\mathbf{e}_*=\\mathbf{e} - \\mathbf{X}(\\mathbf{b}_*-\\mathbf{b})\\). Using \\(\\mathbf{e}'\\mathbf{X}=0\\), get \\(\\mathbf{e}_*'\\mathbf{e}_*=\\mathbf{e}'\\mathbf{e} + (\\mathbf{b}_*-\\mathbf{b})'\\mathbf{X}'\\mathbf{X}(\\mathbf{b}_*-\\mathbf{b}) \\ge \\mathbf{e}'\\mathbf{e}\\).Proposition 9.5 (Appendix 9.2), : \\(\\mathbf{b}_*-\\mathbf{b}=-(\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\mathbf{b} - \\mathbf{q})\\). Therefore:\n\\[\n\\mathbf{e}_*'\\mathbf{e}_* - \\mathbf{e}'\\mathbf{e} = (\\mathbf{R}\\mathbf{b} - \\mathbf{q})'[\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}']^{-1}(\\mathbf{R}\\mathbf{b} - \\mathbf{q}).\n\\]\nimplies F statistic defined Prop. 4.9 also equal :\n\\[\n\\frac{(\\mathbf{e}_*'\\mathbf{e}_* - \\mathbf{e}'\\mathbf{e})/J}{\\mathbf{e}'\\mathbf{e}/(n-K)},\n\\]\nleads result.null hypothesis \\(H_0\\) (Eq. (4.12)) F-test rejected \\(F\\) —defined Eq. (4.14) (4.15)— higher \\(\\mathcal{F}_{1-\\alpha}(J,n-K)\\). (Hence, test one-sided test.)","code":""},{"path":"ChapterLS.html","id":"largeSample","chapter":"4 Linear Regressions","heading":"4.2.6 Large Sample Properties","text":"Even relax normality assumption (Hypothesis 4.5), can approximate finite-sample behavior estimators using large-sample asymptotic properties.begin , proceed Hypothesis 4.1 4.4. (see, later , deal —partial— relaxations Hypothesis 4.3 4.4.)regularity assumptions, Hypotheses 4.1 4.4, even residuals normally-distributed, least square estimators can asymptotically normal inference can performed way small samples Hypotheses 4.1 4.5 hold. derives Prop. 4.11 (). F-test (Prop. 4.10) t-test (Eq. (4.10)) can performed.Proposition 4.11  Assumptions 4.1 4.4, assuming :\n\\[\\begin{equation}\nQ = \\mbox{plim}_{n \\rightarrow \\infty} \\frac{\\mathbf{X}'\\mathbf{X}}{n},\\tag{4.16}\n\\end{equation}\\]\n\\((\\mathbf{x}_i,\\varepsilon_i)\\)’s independent (across entities \\(\\)), :\n\\[\\begin{equation}\n\\sqrt{n}(\\mathbf{b} - \\boldsymbol\\beta)\\overset{d} {\\rightarrow} \\mathcal{N}\\left(0,\\sigma^2Q^{-1}\\right).\\tag{4.17}\n\\end{equation}\\]Proof. Since \\(\\mathbf{b} = \\boldsymbol\\beta + \\left( \\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1}\\left(\\frac{\\mathbf{X}'\\boldsymbol\\varepsilon}{n}\\right)\\), : \\(\\sqrt{n}(\\mathbf{b} - \\boldsymbol\\beta) = \\left( \\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1} \\left(\\frac{1}{\\sqrt{n}}\\right)\\mathbf{X}'\\boldsymbol\\varepsilon\\). Since \\(f:\\rightarrow ^{-1}\\) continuous function (\\(\\ne \\mathbf{0}\\)), \\(\\mbox{plim}_{n \\rightarrow \\infty} \\left(\\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1} = \\mathbf{Q}^{-1}\\) (see Prop. 9.12). Let us denote \\(V_i\\) vector \\(\\mathbf{x}_i \\varepsilon_i\\). \\((\\mathbf{x}_i,\\varepsilon_i)\\)’s independent, \\(V_i\\)’s independent well. covariance matrix \\(\\sigma^2\\mathbb{E}(\\mathbf{x}_i \\mathbf{x}_i')=\\sigma^2Q\\). Applying multivariate central limit theorem vectors \\(V_i\\) gives \\(\\sqrt{n}\\left(\\frac{1}{n}\\sum_{=1}^n \\mathbf{x}_i \\varepsilon_i\\right) = \\left(\\frac{1}{\\sqrt{n}}\\right)\\mathbf{X}'\\boldsymbol\\varepsilon \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2Q)\\). application Slutsky’s theorem (Prop. 9.12) leads results.practice, \\(\\sigma^2\\) approximated \\(s^2=\\frac{\\mathbf{e}'\\mathbf{e}}{n-K}\\) (Eq. (4.9)) \\(\\mathbf{Q}^{-1}\\) \\(\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1}\\). , covariance matrix estimator approximated :\n\\[\\begin{equation}\n\\boxed{\\widehat{\\mathbb{V}ar}(\\mathbf{b}) = s^2 (\\mathbf{X}'\\mathbf{X})^{-1}.}\\tag{4.18}\n\\end{equation}\\]Eqs. (4.16) (4.17) respectively correspond convergences probability distribution (see Definitions 9.14 9.17, respectively).","code":""},{"path":"ChapterLS.html","id":"CommonPitfalls","chapter":"4 Linear Regressions","heading":"4.3 Common pitfalls in linear regressions","text":"","code":""},{"path":"ChapterLS.html","id":"multicollinearity","chapter":"4 Linear Regressions","heading":"4.3.1 Multicollinearity","text":"Consider model: \\(y_i = \\beta_1 x_{,1} + \\beta_2 x_{,2} + \\varepsilon_i\\), variables zero-mean \\(\\mathbb{V}ar(\\varepsilon_i)=\\sigma^2\\). :\n\\[\n\\mathbf{X}'\\mathbf{X} = \\left[ \\begin{array}{cc}\n\\sum_i x_{,1}^2 & \\sum_i x_{,1} x_{,2} \\\\\n\\sum_i x_{,1} x_{,2} & \\sum_i x_{,2}^2\n\\end{array}\\right],\n\\]\ntherefore:\n\\[\\begin{eqnarray*}\n(\\mathbf{X}'\\mathbf{X})^{-1} &=& \\frac{1}{\\sum_i x_{,1}^2\\sum_i x_{,2}^2 - (\\sum_i x_{,1} x_{,2})^2} \\left[ \\begin{array}{cc}\n\\sum_i x_{,2}^2 & -\\sum_i x_{,1} x_{,2} \\\\\n-\\sum_i x_{,1} x_{,2} & \\sum_i x_{,1}^2\n\\end{array}\\right].\n\\end{eqnarray*}\\]\ninverse upper-left parameter \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) :\n\\[\\begin{equation}\n\\sum_i x_{,1}^2 - \\frac{(\\sum_i x_{,1} x_{,2})^2}{\\sum_i x_{,2}^2} = \\sum_i x_{,1}^2(1 - correl_{1,2}^2),\\tag{4.19}\n\\end{equation}\\]\n\\(correl_{1,2}\\) sample correlation \\(\\mathbf{x}_{1}\\) \\(\\mathbf{x}_{2}\\).Hence, closer one \\(correl_{1,2}\\), higher variance \\(b_1\\) (recall variance \\(b_1\\) upper-left component \\(\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}\\)). , regressors close linear conbination ones, confidence intervals tend wide, typically reduces power t-test (tend fail reject null hypothesis coefficients different zero).","code":""},{"path":"ChapterLS.html","id":"Omitted","chapter":"4 Linear Regressions","heading":"4.3.2 Omitted variables","text":"Consider following model (“True model”):\n\\[\n\\mathbf{y} = \\underbrace{\\mathbf{X}_1}_{n \\times K_1}\\underbrace{\\boldsymbol\\beta_1}_{K_1 \\times 1} + \\underbrace{\\mathbf{X}_2}_{n\\times K_2}\\underbrace{\\boldsymbol\\beta_2}_{K_2 \\times 1} + \\boldsymbol\\varepsilon\n\\]\none computes \\(\\mathbf{b}_1\\) regressing \\(\\mathbf{y}\\) \\(\\mathbf{X}_1\\) , one gets:\n\\[\n\\mathbf{b}_1 = (\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\mathbf{y} = \\boldsymbol\\beta_1 + (\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\mathbf{X}_2\\boldsymbol\\beta_2 +\n(\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\boldsymbol\\varepsilon.\n\\]results omitted-variable formula:\n\\[\n\\mathbb{E}(\\mathbf{b}_1|\\mathbf{X}) = \\boldsymbol\\beta_1 + \\underbrace{(\\mathbf{X}_1'\\mathbf{X}_1)^{-1}(\\mathbf{X}_1'\\mathbf{X}_2)}_{K_1 \\times K_2}\\boldsymbol\\beta_2.\n\\]\n(column \\((\\mathbf{X}_1'\\mathbf{X}_1)^{-1}(\\mathbf{X}_1'\\mathbf{X}_2)\\) OLS regressors obtained regressing columns \\(\\mathbf{X}_2\\) \\(\\mathbf{X}_1\\).) Unless variables included \\(\\mathbf{X}_1\\) orthogonal \\(\\mathbf{X}_2\\), obtain bias. way address potential pitfall introduce “controls” specification.Example 4.3  Let us use California Test Score dataset (package AER). Assume want measure effect students--teacher ratio (str) student test scores (testscr). following regressions show effect lower controls added.","code":"\nlibrary(AER); data(\"CASchools\")\nCASchools$str <- CASchools$students/CASchools$teachers\nCASchools$testscr <- .5 * (CASchools$math + CASchools$read)\neq1 <- lm(testscr~str,data=CASchools)\neq2 <- lm(testscr~str+lunch,data=CASchools)\neq3 <- lm(testscr~str+lunch+english,data=CASchools)\nstargazer::stargazer(eq1,eq2,eq3,type=\"text\",\n                     no.space = TRUE,omit.stat=c(\"f\",\"ser\"))## \n## =============================================\n##                    Dependent variable:       \n##              --------------------------------\n##                          testscr             \n##                 (1)        (2)        (3)    \n## ---------------------------------------------\n## str          -2.280***  -1.117***  -0.998*** \n##               (0.480)    (0.240)    (0.239)  \n## lunch                   -0.600***  -0.547*** \n##                          (0.017)    (0.022)  \n## english                            -0.122*** \n##                                     (0.032)  \n## Constant     698.933*** 702.911*** 700.150***\n##               (9.467)    (4.700)    (4.686)  \n## ---------------------------------------------\n## Observations    420        420        420    \n## R2             0.051      0.767      0.775   \n## Adjusted R2    0.049      0.766      0.773   \n## =============================================\n## Note:             *p<0.1; **p<0.05; ***p<0.01"},{"path":"ChapterLS.html","id":"irrelevant","chapter":"4 Linear Regressions","heading":"4.3.3 Irrelevant variable","text":"Consider true model:\n\\[\n\\mathbf{y} = \\mathbf{X}_1\\boldsymbol\\beta_1 + \\boldsymbol\\varepsilon,\n\\]\nestimated model :\n\\[\n\\mathbf{y} = \\mathbf{X}_1\\boldsymbol\\beta_1 + \\mathbf{X}_2\\boldsymbol\\beta_2 + \\boldsymbol\\varepsilon\n\\]estimates unbiased. However, adding irrelevant explanatory variables increases variance estimate \\(\\boldsymbol\\beta_1\\) (compared case one uses correct explanatory variables). case unless correlation \\(\\mathbf{X}_1\\) \\(\\mathbf{X}_2\\) null, see Eq. (4.19).words, estimator inefficient, .e., exists alternative consistent estimator whose variance lower. inefficiency problem can serious consequences testing hypotheses \\(H_0: \\beta_1 = 0\\). Due loss power, might wrongly infer \\(\\mathbf{X}_1\\) variables “relevant” (Type-II error, False Negative).","code":""},{"path":"ChapterLS.html","id":"IV","chapter":"4 Linear Regressions","heading":"4.4 Instrumental Variables","text":"conditional mean zero assumption (Hypothesis 4.2), according \\(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X})=0\\) —implies particular \\(\\mathbf{x}_i\\) \\(\\varepsilon_i\\) uncorrelated— sometimes consistent considered economic framework. case, parameters interest may still estimated consistently resorting instrumental variable techniques.Consider following model:\n\\[\\begin{equation}\ny_i = \\mathbf{x_i}'\\boldsymbol\\beta + \\varepsilon_i, \\quad \\mbox{} \\mathbb{E}(\\varepsilon_i)=0  \\mbox{ } \\mathbf{x_i}\\\\perp \\varepsilon_i.\\tag{4.20}\n\\end{equation}\\]Let us illustrate situation may result biased OLS estimate. Consider instance situation :\n\\[\\begin{equation}\n\\mathbb{E}(\\varepsilon_i)=0 \\quad \\mbox{} \\quad \\mathbb{E}(\\varepsilon_i \\mathbf{x_i})=\\boldsymbol\\gamma,\\tag{4.21}\n\\end{equation}\\]\ncase \\(\\mathbf{x}_i\\\\perp \\varepsilon_i\\) (consistently Eq. (4.20)).law large numbers, \\(\\mbox{plim}_{n \\rightarrow \\infty} \\mathbf{X}'\\boldsymbol\\varepsilon / n = \\boldsymbol\\gamma\\). \\(\\mathbf{Q}_{xx} := \\mbox{plim } \\mathbf{X}'\\mathbf{X}/n\\), OLS estimator consistent \n\\[\n\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon \\overset{p}{\\rightarrow} \\boldsymbol\\beta + \\mathbf{Q}_{xx}^{-1}\\boldsymbol\\gamma \\ne \\boldsymbol\\beta.\n\\]Let us now introduce notion instruments.Definition 4.2  (Instrumental variables) \\(L\\)-dimensional random variable \\(\\mathbf{z}_i\\) valid set instruments :\\(\\mathbf{z}_i\\) correlated \\(\\mathbf{x}_i\\);\\(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{Z})=0\\) andthe orthogonal projections \\(\\mathbf{x}_i\\)’s \\(\\mathbf{z}_i\\)’s multicollinear.Point c implies particular dimension \\(\\mathbf{z}_i\\) least large \\(\\mathbf{x}_i\\). \\(\\mathbf{z}_i\\) valid set instruments, :\n\\[\n\\mbox{plim}\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right) =\\mbox{plim}\\left( \\frac{\\mathbf{Z}'(\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon)}{n} \\right) = \\mbox{plim}\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\boldsymbol\\beta.\n\\]\nIndeed, law large numbers, \\(\\frac{\\mathbf{Z}'\\boldsymbol\\varepsilon}{n} \\overset{p}{\\rightarrow}\\mathbb{E}(\\mathbf{z}_i\\varepsilon_i)=0\\).\\(L = K\\), matrix \\(\\frac{\\mathbf{Z}'\\mathbf{X}}{n}\\) dimension \\(K \\times K\\) :\n\\[\n\\left[\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\right]^{-1}\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right) = \\boldsymbol\\beta.\n\\]\ncontinuity inverse function (everywhere 0): \\(\\left[\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\right]^{-1}=\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1}\\).\nSlutsky Theorem (Prop. 9.12) implies :\n\\[\n\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1} \\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right)  = \\mbox{plim }\\left( \\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1} \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right).\n\\]\nHence \\(\\mathbf{b}_{iv}\\) consistent defined :\n\\[\n\\boxed{\\mathbf{b}_{iv} = (\\mathbf{Z}'\\mathbf{X})^{-1}\\mathbf{Z}'\\mathbf{y}.}\n\\]Proposition 4.12  (Asymptotic distribution IV estimator) \\(\\mathbf{z}_i\\) \\(L\\)-dimensional random variable constitutes valid set instruments (see Def. 4.2) \\(L=K\\), asymptotic distribution \\(\\mathbf{b}_{iv}\\) :\n\\[\n\\mathbf{b}_{iv} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(\\boldsymbol\\beta,\\frac{\\sigma^2}{n}\\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\\right]^{-1}\\right)\n\\]\n\\(\\mbox{plim } \\mathbf{Z}'\\mathbf{Z}/n =: \\mathbf{Q}_{zz}\\), \\(\\mbox{plim } \\mathbf{Z}'\\mathbf{X}/n =: \\mathbf{Q}_{zx}\\), \\(\\mbox{plim } \\mathbf{X}'\\mathbf{Z}/n =: \\mathbf{Q}_{xz}\\).Proof. proof similar Prop. 4.11, starting point \\(\\mathbf{b}_{iv} = \\boldsymbol\\beta + (\\mathbf{Z}'\\mathbf{X})^{-1}\\mathbf{Z}'\\boldsymbol\\varepsilon\\).\\(L=K\\), :\n\\[\n\\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\\right]^{-1}=Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}.\n\\]\npractice, estimate \\(\\mathbb{V}ar(\\mathbf{b}_{iv}) = \\frac{\\sigma^2}{n}Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}\\), replace \\(\\sigma^2\\) :\n\\[\ns_{iv}^2 = \\frac{1}{n}\\sum_{=1}^{n} (y_i - \\mathbf{x}_i'\\mathbf{b}_{iv})^2.\n\\]\\(L > K\\)? case, proceed follows:Regress \\(\\mathbf{X}\\) space spanned \\(\\mathbf{Z}\\) andRegress \\(\\mathbf{y}\\) fitted values \\(\\hat{\\mathbf{X}}:=\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}\\).two-step approach called Two-Stage Least Squares (2SLS). results :\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{iv} = [\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{Y}.} \\tag{4.22}\n\\end{equation}\\]case, Prop. 4.12 still holds, \\(\\mathbf{b}_{iv}\\) given Eq. (4.22).instruments properly satisfy Condition () Def. 4.2 (.e. \\(\\mathbf{x}_i\\) \\(\\mathbf{z}_i\\) loosely related), instruments said weak (see, e.g., J. H. Stock Yogo (2005), available Andrews, Stock, Sun (2019)). simple standard way test weak instruments consist looking F-statistic associated first stage estimation. easier reject null hypothesis (large test statistic), less weak —stronger— instruments.Durbin-Wu-Hausman test (Durbin (1954), Wu (1973), Hausman (1978)) can used test IV necessary. (IV techniques required \\(\\mbox{plim}_{n \\rightarrow \\infty} \\mathbf{X}'\\boldsymbol\\varepsilon / n \\ne 0\\).) Hausman (1978) proposes test efficiency estimators. null hypothesis two estimators, \\(\\mathbf{b}_0\\) \\(\\mathbf{b}_1\\), consistent \\(\\mathbf{b}_0\\) (asymptotically) efficient relative \\(\\mathbf{b}_1\\). alternative hypothesis, \\(\\mathbf{b}_1\\) (IV present case) remains consistent \\(\\mathbf{b}_0\\) (OLS present case). , reject null hypothesis, means OLS estimator consistent, potentially due endogeneity issue.test statistic :\n\\[\nH = (\\mathbf{b}_1 - \\mathbf{b}_0)' MPI(\\mathbb{V}ar(\\mathbf{b}_1) - \\mathbb{V}ar(\\mathbf{b}_0))(\\mathbf{b}_1 - \\mathbf{b}_0),\n\\]\n\\(MPI\\) Moore-Penrose pseudo-inverse. null hypothesis, \\(H \\sim \\chi^2(q)\\), \\(q\\) rank \\(\\mathbb{V}ar(\\mathbf{b}_1) - \\mathbb{V}ar(\\mathbf{b}_0)\\).Example 4.4  (Estimation price elasticity) See e.g. estimation tobacco price elasticity demand.want estimate effect demand exogenous increase prices cigarettes (say).model :\n\\[\\begin{eqnarray*}\n\\underbrace{q^d_t}_{\\mbox{log(demand)}} &=& \\alpha_0 + \\alpha_1 \\underbrace{\\times p_t}_{\\mbox{log(price)}} + \\alpha_2 \\underbrace{\\times w_t}_{\\mbox{income}} + \\varepsilon_t^d\\\\\n\\underbrace{q^s_t}_{\\mbox{log(supply)}} &=& \\gamma_0 + \\gamma_1 \\times p_t + \\gamma_2 \\underbrace{\\times \\mathbf{y}_t}_{\\mbox{cost factors}} + \\varepsilon_t^s,\n\\end{eqnarray*}\\]\n\\(\\mathbf{y}_t\\), \\(w_t\\), \\(\\varepsilon_t^s \\sim \\mathcal{N}(0,\\sigma^2_s)\\) \\(\\varepsilon_t^d \\sim \\mathcal{N}(0,\\sigma^2_d)\\) independent.Equilibrium: \\(q^d_t = q^s_t\\). implies prices endogenous:\n\\[\np_t = \\frac{\\alpha_0 + \\alpha_2 w_t + \\varepsilon_t^d - \\gamma_0 - \\gamma_2 \\mathbf{y}_t - \\varepsilon_t^s}{\\gamma_1 - \\alpha_1}.\n\\]\nparticular \\(\\mathbb{E}(p_t \\varepsilon_t^d) = \\frac{\\sigma^2_d}{\\gamma_1 - \\alpha_1} \\ne 0\\) \\(\\Rightarrow\\) Regressing OLS \\(q_t^d\\) \\(p_t\\) gives biased estimates (see Eq. (4.21)).\nFigure 4.5: figure illustrates situation prevailing estimating price-elasticity (price endogenous).\nLet us use IV regressions estimate price elasticity cigarette demand. purpose, use CigarettesSW dataset package AER (data used J. Stock Watson (2003)). panel dataset documents cigarette consumption 48 continental US States 1985–1995. instrument real tax cigarettes arising state’s general sales tax. rationale larger general sales tax drives cigarette prices , general tax determined forces affecting \\(\\varepsilon_t^d\\).last three tests interpreted follows:Since p-value first test small, reject null hypothesis according instrument weak.small p-value Wu-Hausman test implies reject null hypothesis according OLS estimates consistent (10% level , though).-identification (misspecification) detected Sargan test (large p-value).Example 4.5  (Education wage) example, make use another dataset proposed J. Stock Watson (2003), namely CollegeDistance dataset.7 objective estimate effect education wages. Education choice suspected endogenous variable, calls IV strategy. instrumental variable distance college (see, e.g., Dee (2004)).","code":"\ndata(\"CigarettesSW\", package = \"AER\")\nCigarettesSW$rprice  <- with(CigarettesSW, price/cpi)\nCigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)\nCigarettesSW$tdiff   <- with(CigarettesSW, (taxs - tax)/cpi)\n\n## model \neq.IV1 <- ivreg(log(packs) ~ log(rprice) + log(rincome) |\n                  log(rincome) + tdiff + I(tax/cpi),\n                data = CigarettesSW, subset = year == \"1995\")\neq.IV2 <- ivreg(log(packs) ~ log(rprice) | tdiff,\n                data = CigarettesSW, subset = year == \"1995\")\neq.no.IV <- lm(log(packs) ~ log(rprice) + log(rincome),\n               data = CigarettesSW, subset = year == \"1995\")\nstargazer::stargazer(eq.no.IV,eq.IV1,eq.IV2,type=\"text\",no.space = TRUE,\n                     omit.stat=c(\"f\",\"ser\"))## \n## ==========================================\n##                   Dependent variable:     \n##              -----------------------------\n##                       log(packs)          \n##                 OLS       instrumental    \n##                             variable      \n##                 (1)       (2)       (3)   \n## ------------------------------------------\n## log(rprice)  -1.407*** -1.277*** -1.084***\n##               (0.251)   (0.263)   (0.317) \n## log(rincome)   0.344     0.280            \n##               (0.235)   (0.239)           \n## Constant     10.342*** 9.895***  9.720*** \n##               (1.023)   (1.059)   (1.514) \n## ------------------------------------------\n## Observations    48        48        48    \n## R2             0.433     0.429     0.401  \n## Adjusted R2    0.408     0.404     0.388  \n## ==========================================\n## Note:          *p<0.1; **p<0.05; ***p<0.01\nsummary(eq.IV1,diagnostics = TRUE)$diagnostics##                  df1 df2   statistic      p-value\n## Weak instruments   2  44 244.7337536 1.444054e-24\n## Wu-Hausman         1  44   3.0678163 8.682505e-02\n## Sargan             1  NA   0.3326221 5.641191e-01\nlibrary(sem)\ndata(\"CollegeDistance\", package = \"AER\")\neq.1st.stage <- lm(education ~ urban + gender + ethnicity + unemp + distance,\n                   data = CollegeDistance)\nCollegeDistance$ed.pred<- predict(eq.1st.stage)\neq.2nd.stage <- lm(wage ~ urban + gender + ethnicity + unemp + ed.pred,\n                   data = CollegeDistance)\neqOLS <- lm(wage ~ urban + gender + ethnicity + unemp + education,\n            data=CollegeDistance)\neq2SLS <- ivreg(wage ~ urban + gender + ethnicity + unemp + education|\n                  urban + gender + ethnicity + unemp + distance,\n                data=CollegeDistance)\nstargazer::stargazer(eq.1st.stage,eq.2nd.stage,eq2SLS,eqOLS,\n                     type=\"text\",no.space = TRUE,omit.stat = c(\"f\",\"ser\"))## \n## ============================================================\n##                              Dependent variable:            \n##                   ------------------------------------------\n##                   education               wage              \n##                      OLS       OLS    instrumental    OLS   \n##                                         variable            \n##                      (1)       (2)        (3)         (4)   \n## ------------------------------------------------------------\n## urbanyes           -0.092     0.046      0.046       0.070  \n##                    (0.065)   (0.045)    (0.060)     (0.045) \n## genderfemale       -0.025    -0.071*     -0.071    -0.085** \n##                    (0.052)   (0.037)    (0.050)     (0.037) \n## ethnicityafam     -0.524*** -0.227***   -0.227**   -0.556***\n##                    (0.072)   (0.073)    (0.099)     (0.052) \n## ethnicityhispanic -0.275*** -0.351***  -0.351***   -0.544***\n##                    (0.068)   (0.057)    (0.077)     (0.049) \n## unemp               0.010   0.139***    0.139***   0.133*** \n##                    (0.010)   (0.007)    (0.009)     (0.007) \n## distance          -0.087***                                 \n##                    (0.012)                                  \n## ed.pred                     0.647***                        \n##                              (0.101)                        \n## education                               0.647***     0.005  \n##                                         (0.136)     (0.010) \n## Constant          14.061***  -0.359      -0.359    8.641*** \n##                    (0.083)   (1.412)    (1.908)     (0.157) \n## ------------------------------------------------------------\n## Observations        4,739     4,739      4,739       4,739  \n## R2                  0.023     0.117      -0.612      0.110  \n## Adjusted R2         0.022     0.116      -0.614      0.109  \n## ============================================================\n## Note:                            *p<0.1; **p<0.05; ***p<0.01"},{"path":"ChapterLS.html","id":"general-regression-model-grm-and-robust-covariance-matrices","chapter":"4 Linear Regressions","heading":"4.5 General Regression Model (GRM) and robust covariance matrices","text":"statistical inference presented relies strong assumptions regarding stochastic properties errors. Namely, assumed mutually uncorrelated (Hypothesis 4.4) homoskedastic (Hypothesis 4.3).objective section present approaches aimed adjusting estimate covariance matrix OLS estimator (\\((\\mathbf{X}'\\mathbf{X})^{-1}s^2\\), see Eq. (4.18)), previous hypotheses hold.","code":""},{"path":"ChapterLS.html","id":"presentation-of-the-general-regression-model-grm","chapter":"4 Linear Regressions","heading":"4.5.1 Presentation of the General Regression Model (GRM)","text":"prove useful introduce following notation:\n\\[\\begin{eqnarray}\n\\mathbb{V}ar(\\boldsymbol\\varepsilon | \\mathbf{X}) = \\mathbb{E}(\\boldsymbol\\varepsilon \\boldsymbol\\varepsilon'| \\mathbf{X}) &=& \\boldsymbol\\Sigma. \\tag{4.23}\n\\end{eqnarray}\\]Note Eq. (4.23) general Hypothesis 4.3 4.4 diagonal entries \\(\\boldsymbol\\Sigma\\) may different (opposed Hypothesis 4.3), non-diagonal entries \\(\\boldsymbol\\Sigma\\) can non-null (opposed Hypothesis 4.4).Definition 4.3  (General Regression Model (GRM)) Hypothesis 4.1 4.2, together Eq. (4.23), form General Regression Model (GRM) framework.Naturally, regression model Hypotheses 4.1 4.4 hold specific case GRM framework.GRM context notably encompasses situations heteroskedasticity autocorrelation:Heteroskedasticity:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]. \\tag{4.24}\n\\end{equation}\\]Heteroskedasticity:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]. \\tag{4.24}\n\\end{equation}\\]Autocorrelation:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc}\n1 & \\rho_{2,1} & \\dots & \\rho_{n,1} \\\\\n\\rho_{2,1} & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho_{n,n-1} \\\\\n\\rho_{n,1} & \\rho_{n,2} & \\dots & 1\n\\end{array} \\right]. \\tag{4.25}\n\\end{equation}\\]Autocorrelation:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc}\n1 & \\rho_{2,1} & \\dots & \\rho_{n,1} \\\\\n\\rho_{2,1} & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho_{n,n-1} \\\\\n\\rho_{n,1} & \\rho_{n,2} & \\dots & 1\n\\end{array} \\right]. \\tag{4.25}\n\\end{equation}\\]Example 4.6  (Auto-regressive processes) Autocorrelation common time-series contexts (see Section 8). time-series context, subscript \\(\\) refers date.Assume instance :\n\\[\\begin{equation}\ny_i = \\mathbf{x}_i' \\boldsymbol\\beta + \\varepsilon_i \\tag{4.26}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\varepsilon_i = \\rho \\varepsilon_{-1} + v_i, \\quad v_i \\sim \\mathcal{N}(0,\\sigma_v^2).\\tag{4.27}\n\\end{equation}\\]\ncase, GRM context, :\n\\[\\begin{equation}\n\\boldsymbol\\Sigma =\\frac{ \\sigma_v^2}{1 - \\rho^2} \\left[    \\begin{array}{cccc}\n1 & \\rho & \\dots & \\rho^{n-1} \\\\\n\\rho & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho \\\\\n\\rho^{n-1} & \\rho^{n-2} & \\dots & 1\n\\end{array} \\right].\\tag{4.28}\n\\end{equation}\\]cases —particular one assumes parametric formulation \\(\\boldsymbol\\Sigma\\)— one can determine better (accurate) estimator OLS one. approach called Generalized Least Squares (GLS), present .","code":""},{"path":"ChapterLS.html","id":"GLS","chapter":"4 Linear Regressions","heading":"4.5.2 Generalized Least Squares","text":"Assume \\(\\boldsymbol\\Sigma\\) known (“feasible GLS”). \\(\\boldsymbol\\Sigma\\) symmetric positive, admits spectral decomposition form \\(\\boldsymbol\\Sigma = \\mathbf{C} \\boldsymbol\\Lambda \\mathbf{C}'\\), \\(\\mathbf{C}\\) orthogonal matrix (.e. \\(\\mathbf{C}\\mathbf{C}'=Id\\)) \\(\\boldsymbol\\Lambda\\) diagonal matrix (diagonal entries eigenvalues \\(\\boldsymbol\\Sigma\\)).\\(\\boldsymbol\\Sigma = (\\mathbf{P}\\mathbf{P}')^{-1}\\) \\(\\mathbf{P} = \\mathbf{C}\\boldsymbol\\Lambda^{-1/2}\\). Consider transformed model:\n\\[\n\\mathbf{P}'\\mathbf{y} = \\mathbf{P}'\\mathbf{X}\\boldsymbol\\beta + \\mathbf{P}'\\boldsymbol\\varepsilon \\quad \\mbox{} \\quad \\mathbf{y}^* = \\mathbf{X}^*\\boldsymbol\\beta + \\boldsymbol\\varepsilon^*.\n\\]\nvariance \\(\\boldsymbol\\varepsilon^*\\) identity matrix \\(Id\\). transformed model, OLS BLUE (Gauss-Markow Theorem 4.1).Generalized least squares estimator \\(\\boldsymbol\\beta\\) :\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{GLS} = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}.}\\tag{4.29}\n\\end{equation}\\]\n:\n\\[\n\\mathbb{V}ar(\\mathbf{b}_{GLS}|\\mathbf{X}) = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}.\n\\]However, general, \\(\\boldsymbol\\Sigma\\) unknown. GLS estimator said infeasible. structure required. Assume \\(\\boldsymbol\\Sigma\\) admits parametric form \\(\\boldsymbol\\Sigma(\\theta)\\). estimation becomes feasible (FGLS) one replaces \\(\\boldsymbol\\Sigma(\\theta)\\) \\(\\boldsymbol\\Sigma(\\hat\\theta)\\), \\(\\hat\\theta\\) consistent estimator \\(\\theta\\). case, FGLS asymptotically efficient (see Example 4.7).\\(\\boldsymbol\\Sigma\\) obvious structure: OLS (IV) estimator available. regularity assumptions, remains unbiased, consistent, asymptotically normally distributed, efficient. Standard inference procedures longer appropriate.Example 4.7  (GLS auto-correlation case) Consider case presented Example 4.6. OLS estimate \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta\\) consistent, estimates \\(e_i\\) \\(\\varepsilon_i\\)’s also . Consistent estimators \\(\\rho\\) \\(\\sigma_v\\) obtained regressing \\(e_i\\)’s \\(e_{-1}\\)’s. Using estimates Eq. (4.28) provides consistent estimate \\(\\boldsymbol\\Sigma\\). Applying steps recursively gives efficient estimator \\(\\boldsymbol\\beta\\) (Cochrane Orcutt (1949)).","code":""},{"path":"ChapterLS.html","id":"asymptotic-properties-of-the-ols-estimator-in-the-grm-framework","chapter":"4 Linear Regressions","heading":"4.5.3 Asymptotic properties of the OLS estimator in the GRM framework","text":"Since \\(\\mathbf{b} = \\boldsymbol\\beta + \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1} \\mathbf{X}'\\boldsymbol\\varepsilon\\) \\(\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})=\\boldsymbol\\Sigma\\), :\n\\[\\begin{equation}\n\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\frac{1}{n}\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\right)\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\\tag{4.30}\n\\end{equation}\\]Therefore, conditional covariance matrix OLS estimator \\(\\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) longer, using \\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) inference may misleading. , see construct appropriate estimates covariance matrix \\(\\mathbf{b}\\). , let us prove OLS estimator remains consistent GRM framework.Proposition 4.13  (Consistency OLS estimator GRM framework) \\(\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, \\(\\mbox{plim }(\\mathbf{b})=\\boldsymbol\\beta\\).Proof. \\(\\mathbb{V}ar(\\mathbf{b})=\\mathbb{E}[\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})]+\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]\\). Since \\(\\mathbb{E}(\\mathbf{b}|\\mathbf{X})=\\boldsymbol\\beta\\), \\(\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]=0\\). Eq. (4.30) implies \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\rightarrow 0\\). Hence \\(\\mathbf{b}\\) converges mean square, therefore probability (see Prop. 9.13).Prop. 4.14 gives asymptotic distribution OLS estimator GRM framework.Proposition 4.14  (Asymptotic distribution OLS estimator GRM framework) \\(Q_{xx}=\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, :\n\\[\n\\sqrt{n}(\\mathbf{b}-\\boldsymbol\\beta) \\overset{d}{\\rightarrow} \\mathcal{N}(0,Q_{xx}^{-1}Q_{x\\boldsymbol\\Sigma x}Q_{xx}^{-1}).\n\\]IV estimator also features normal asymptotic distribution:Proposition 4.15  (Asymptotic distribution IV estimator GRM framework) regressors IV variables “well-behaved”, :\n\\[\n\\mathbf{b}_{iv} \\overset{}{\\sim} \\mathcal{N}(\\boldsymbol\\beta,\\mathbf{V}_{iv}),\n\\]\n\n\\[\n\\mathbf{V}_{iv} = \\frac{1}{n}(\\mathbf{Q}^*)\\mbox{ plim }\\left(\\frac{1}{n} \\mathbf{Z}'\\boldsymbol\\Sigma \\mathbf{Z}\\right)(\\mathbf{Q}^*)',\n\\]\n\n\\[\n\\mathbf{Q}^* = [\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}\\mathbf{Q}_{zx}]^{-1}\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}.\n\\]practical purposes, one needs estimates \\(\\boldsymbol\\Sigma\\) Props. 4.14 4.15. complication comes fact \\(\\boldsymbol\\Sigma\\) dimension \\(n \\times n\\), estimation —based sample length \\(n\\)— therefore infeasible general case. Notwithstanding, looking Eq. (4.30), appears one can focus estimation \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) (\\(\\mbox{plim }\\left(\\frac{1}{n} \\mathbf{Z}'\\boldsymbol\\Sigma \\mathbf{Z}\\right)\\) IV case). matrix dimension \\(K \\times K\\), estimation easier.:\n\\[\\begin{equation}\n\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X} = \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j. \\tag{4.31}\n\\end{equation}\\]-called robust covariance matrices estimates previous matrix. computation based fact \\(\\mathbf{b}\\) consistent, \\(e_i\\)’s consistent estimators \\(\\varepsilon_i\\)’s.following sections (4.5.4 4.5.5), present two types robust covariance matrices.","code":""},{"path":"ChapterLS.html","id":"HAC","chapter":"4 Linear Regressions","heading":"4.5.4 HAC-robust covariance matrices","text":"heteroskedasticity prevails, .e., matrix \\(\\boldsymbol\\Sigma\\) Eq. (4.24), one can use formula proposed White (1980) estimate \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\) (see Example 4.8). residuals feature heteroskedasticity auto-correlation, one can use Newey West (1987) approach (see Example 4.9).Example 4.8  (Heteroskedasticity) case Eq. (4.24). \\(\\sigma_{,j}=0\\) \\(\\ne j\\). Hence, case, need estimate \\(\\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i\\). White (1980) shown , general conditions:\n\\[\\begin{equation}\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right) =\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}e_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right). \\tag{4.32}\n\\end{equation}\\]\nestimator \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\) therefore :\n\\[\\begin{equation}\nM_{HC0} = \\frac{1}{n}\\mathbf{X}'\n\\left[\n\\begin{array}{cccc}\ne_1^2 & 0 & \\dots & 0 \\\\\n0 & e_2^2 &  \\\\\n\\vdots & & \\ddots&0 \\\\\n0 & \\dots & 0 & e_n^2\n\\end{array}\n\\right]\n\\mathbf{X}.\\tag{4.33}\n\\end{equation}\\]\n\\(e_i\\) OLS residuals regression. previous estimator often called HC0. HC1 estimator, due J. MacKinnon White (1985), obtained applying adjustment factor \\(n/(n-K)\\) number degrees freedom (Prop. 4.6). :\n\\[\\begin{equation}\nM_{HC1} = \\frac{n}{n-K}M_{HC0}.\\tag{4.34}\n\\end{equation}\\]can illustrate influence heteroskedasticity using simulations. Consider following model:\n\\[\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2),\n\\]\n\\(x_i\\)’s ..d. \\(t(6)\\).simulated sample (\\(n=200\\)) model:\nFigure 4.6: Situation heteroskedasticity. model \\(y_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2)\\), \\(x_i\\)’s ..d. \\(t(6)\\).\nsimulate 1000 samples model \\(n=200\\). sample, compute OLS estimate \\(\\beta\\) (\\(=1\\)). 1000 OLS estimations, employ () standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)) (b) White formula estimate variance \\(b\\). formula, compute average 1000 resulting standard deviations compare standard deviation 1000 OLS estimate \\(\\beta\\).results show White formula yields, average, estimated standard deviation much closer “true” value standard OLS formula. latter underestimate standard deviation \\(b\\).following example, regress GDP growth rates Jordà, Schularick, Taylor (2017) database systemic financial crisis dummy. compute HC0- HC1-based standard deviations parameter estimate, compare one based standard OLS formula. adjusted standard deviations close one provided non-adjusted OLS formula.Example 4.9  (Heteroskedasticity Autocorrelation (HAC)) Newey West (1987) proposed formula address heteroskedasticity auto-correlation residuals (Eqs. (4.24) (4.25)). show , correlation terms \\(\\) \\(j\\) gets sufficiently small \\(|-j|\\) increases:\n\\[\\begin{eqnarray}\n&&\\mbox{plim} \\left( \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j \\right) \\approx  \\\\\n&&\\mbox{plim} \\left( \\frac{1}{n}\\sum_{t=1}^{n}e_{t}^2\\mathbf{x}_t\\mathbf{x}'_t +\n\\frac{1}{n}\\sum_{\\ell=1}^{L}\\sum_{t=\\ell+1}^{n}w_\\ell e_{t}e_{t-\\ell}(\\mathbf{x}_t\\mathbf{x}'_{t-\\ell} + \\mathbf{x}_{t-\\ell}\\mathbf{x}'_{t})\n\\right), \\nonumber \\tag{4.35}\n\\end{eqnarray}\\]\n\\(w_\\ell = 1 - \\ell/(L+1)\\) (\\(L\\) large).Let us illustrate influence autocorrelation using simulations. consider following model:\n\\[\\begin{equation}\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2),\\tag{4.36}\n\\end{equation}\\]\n\\(x_i\\)’s \\(\\varepsilon_i\\)’s :\n\\[\\begin{equation}\nx_i = 0.8 x_{-1} + u_i \\quad \\quad \\varepsilon_i = 0.8 \\varepsilon_{-1} + v_i, \\tag{4.37}\n\\end{equation}\\]\n\\(u_i\\)’s \\(v_i\\)’s ..d. \\(\\mathcal{N}(0,1)\\).simulate 500 samples model \\(n=200\\). sample, compute OLS estimate \\(\\beta\\) (=1). 1000 OLS estimations, employ () standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)), (b) White formula, (c) Newey-West formula estimate variance \\(b\\). formula, compute average 500 resulting standard deviations compare standard deviation 500 OLS estimate \\(\\beta\\).results show Newey-West formula yields, average, estimated standard deviation closer “true” value standard OLS formula. latter underestimate standard deviation \\(b\\).precedes suggest , residuals feature autocorrelation, important use appropriately adjusted covariance matrices make statistical inference. detect autocorrelation residuals? popular test proposed Durbin Watson (1950) Durbin Watson (1951). Durbin-Watson test statistic :\n\\[\nDW = \\frac{\\sum_{=2}^{n}(e_i - e_{-1})^2}{\\sum_{=1}^{n}e_i^2}= 2(1 - r) - \\underbrace{\\frac{e_1^2 + e_n^2}{\\sum_{=1}^{n}e_i^2}}_{\\overset{p}{\\rightarrow} 0},\n\\]\n\\(r\\) slope regression \\(e_i\\)’s \\(e_{-1}\\)’s, .e.:\n\\[\nr = \\frac{\\sum_{=2}^{n}e_i e_{-1}}{\\sum_{=1}^{n-1}e_i^2}.\n\\]\n(\\(r\\) consistent estimator \\(\\mathbb{C}(\\varepsilon_i,\\varepsilon_{-1})\\), .e. \\(\\rho\\) Eq. (4.27).)one-sided test \\(H_0\\): \\(\\rho=0\\) \\(H_1\\): \\(\\rho>0\\) carried comparing \\(DW\\) values \\(d_L(T, K)\\) \\(d_U(T, K)\\):\n\\[\n\\left\\{\n\\begin{array}{ll}\n\\mbox{$DW < d_L$,}&\\mbox{ null hypothesis rejected;}\\\\\n\\mbox{$DW > d_U$,}&\\mbox{ hypothesis rejected;}\\\\\n\\mbox{$d_L \\le DW \\le d_U$,} &\\mbox{ conclusion drawn.}\n\\end{array}\n\\right.\n\\]Example 4.10  (Durbin-Watson test) regress short-term nominal US interest rate inflation. employ Durbin-Watson test see whether residuals auto-correlated (quite obviously case).","code":"\nn <- 200\nx <- rt(n,df=6)\ny <- x + x*rnorm(n)\npar(plt=c(.1,.95,.1,.95))\nplot(x,y,pch=19)\nn <- 200 # sample size\nN <- 1000 # number of simulated samples\nXX <- matrix(rt(n*N,df=6),n,N)\nYY <- matrix(XX + XX*rnorm(n),n,N)\nall_b       <- NULL;all_V_OLS   <- NULL;all_V_White <- NULL\nfor(j in 1:N){\n  Y <- matrix(YY[,j],ncol=1)\n  X <- matrix(XX[,j],ncol=1)\n  b <- solve(t(X)%*%X) %*% t(X)%*%Y\n  e <- Y - X %*% b\n  S <- 1/n * t(X) %*% diag(c(e^2)) %*% X\n  V_OLS   <- solve(t(X)%*%X) * var(e)\n  V_White <- 1/n * (solve(1/n*t(X)%*%X)) %*% S %*% (solve(1/n*t(X)%*%X))\n  all_b       <- c(all_b,b)\n  all_V_OLS   <- c(all_V_OLS,V_OLS)\n  all_V_White <- c(all_V_White,V_White)\n}\nc(sd(all_b),mean(sqrt(all_V_OLS)),mean(sqrt(all_V_White)))## [1] 0.14024423 0.06748804 0.13973431\nlibrary(lmtest)\nlibrary(sandwich)\nnT <- dim(JST)[1]\nJST$growth <- NaN\nJST$growth[2:nT] <- log(JST$rgdpbarro[2:nT]/JST$rgdpbarro[1:(nT-1)])\nJST.red <- subset(JST,year>1950)\nJST.red$iso <- as.factor(JST.red$iso)\nJST.red$year <- as.factor(JST.red$year)\neq <- lm(growth~crisisJST+iso+year,data=JST.red)\nvcovHC0 <- vcovHC(eq, type = \"HC0\")\nvcovHC1 <- vcovHC(eq, type = \"HC1\")\nstargazer::stargazer(eq, eq, eq,type = \"text\",\n                     column.labels = c(\"No HAC\", \"HC0\",\"HC1\"),\n                     omit = c(\"iso\",\"year\"),no.space = TRUE,keep.stat = \"n\",\n                     se = list(NULL,sqrt(diag(vcovHC0)),sqrt(diag(vcovHC1))))## \n## ==========================================\n##                   Dependent variable:     \n##              -----------------------------\n##                         growth            \n##               No HAC      HC0       HC1   \n##                 (1)       (2)       (3)   \n## ------------------------------------------\n## crisisJST    -0.015*** -0.015*** -0.015***\n##               (0.005)   (0.005)   (0.006) \n## Constant     0.042***  0.042***  0.042*** \n##               (0.005)   (0.007)   (0.007) \n## ------------------------------------------\n## Observations   1,258     1,258     1,258  \n## ==========================================\n## Note:          *p<0.1; **p<0.05; ***p<0.01\nlibrary(AEC)\nn <- 100 # sample length\nnb.sim <- 500 # number of simulated samples\nall.b <- NULL;all.OLS.stdv.b <- NULL\nall.Whi.stdv.b <- NULL;all.NW.stdv.b <- NULL\nfor(i in 1:nb.sim){\n  eps <- rnorm(n);x <- rnorm(n)\n  for(i in 2:n){\n    eps[i] <- eps[i] + .8*eps[i-1]\n    x[i]   <- x[i]   + .8*x[i-1]\n  }\n  y <- x + eps\n  eq <- lm(y~x)\n  all.b <- c(all.b,eq$coefficients[2])\n  all.OLS.stdv.b <- c(all.OLS.stdv.b,summary(eq)$coefficients[2,2])\n  X <- cbind(rep(1,n),x)\n  XX <- t(X) %*% X;XX_1 <- solve(XX)\n  E2 <- diag(eq$residuals^2)\n  White.V <- XX_1 %*% (t(X) %*% E2 %*% X) %*% XX_1\n  all.Whi.stdv.b <- c(all.Whi.stdv.b,sqrt(White.V[2,2]))\n  # HAC:\n  U <- X * cbind(eq$residuals,eq$residuals)\n  XSigmaX <- NW.LongRunVariance(U,5)\n  NW.V <- 1/n * (n*XX_1) %*% XSigmaX %*% (n*XX_1)\n  all.NW.stdv.b <- c(all.NW.stdv.b,sqrt(NW.V[2,2]))\n}\ncbind(sd(all.b),mean(all.OLS.stdv.b),\n      mean(all.Whi.stdv.b),mean(all.NW.stdv.b))##          [,1]      [,2]      [,3]     [,4]\n## [1,] 0.201172 0.1013048 0.0962689 0.146974\nlibrary(car)\ndata <- subset(JST,iso==\"USA\");T <- dim(data)[1]\ndata$infl <- c(NaN,100*log(data$cpi[2:T]/data$cpi[1:(T-1)]))\ndata$infl[(data$infl< -5)|(data$infl>10)] <- NaN\npar(mfrow=c(1,2))\nplot(data$year,data$stir,ylim=c(-10,20),type=\"l\",lwd=2,xlab=\"\",\n     ylab=\"\",main=\"Nominal rate and inflation\")\nlines(data$year,data$infl,col=\"red\",lwd=2)\neq <- lm(stir~infl,data=data)\nplot(eq$residuals,type=\"l\",col=\"blue\",main=\"Residuals\",xlab=\"\",ylab=\"\")\ndurbinWatsonTest(eq)##  lag Autocorrelation D-W Statistic p-value\n##    1       0.7321902     0.4984178       0\n##  Alternative hypothesis: rho != 0"},{"path":"ChapterLS.html","id":"Clusters","chapter":"4 Linear Regressions","heading":"4.5.5 Cluster-robust covariance matrices","text":"present section based J. G. MacKinnon, Nielsen, Webb (2022); another useful reference Cameron Miller (2014). see one can approximate \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) (see Prop. 4.14) dataset can decomposed clusters. clusters constitute partition sample, error terms may correlated within given cluster, across different clusters. cluster may, e.g., gathers entities given geographical area, industry, age cohort.OLS estimator satisfies:\n\\[\\begin{equation}\n\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon.\\tag{4.38}\n\\end{equation}\\]\nConsider set \\(\\{n_1,n_2,\\dots,n_G\\}\\) s.t. \\(n=\\sum_g n_g\\), based following decomposition \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{c}\n\\mathbf{X}_1 \\\\\n\\mathbf{X}_2 \\\\\n\\vdots\\\\\n\\mathbf{X}_G\n\\end{array}\n\\right].\n\\]\nnotations, Eq. (4.38) rewrites:\n\\[\\begin{equation}\n\\mathbf{b} - \\boldsymbol\\beta = \\left(\\sum_{g=1}^G \\mathbf{X}_g'\\mathbf{X}_g\\right)^{-1}\\sum_{g=1}^G \\mathbf{s}_g,\\tag{4.39}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\mathbf{s}_g = \\mathbf{X}_g'\\boldsymbol\\varepsilon_g \\tag{4.40}\n\\end{equation}\\]\ndenotes score vector (dimension \\(K \\times 1\\)) associated \\(g^{th}\\) cluster.model correctly specified \\(\\mathbb{E}(\\mathbf{s}_g)=0\\) clusters \\(g\\). Note Eq. (4.39) valid partition \\(\\{1,\\dots,n\\}\\). Dividing sample clusters becomes meaningful assume following hypothesis holds:Hypothesis 4.6  (Clusters) :\n\\[\n()\\; \\mathbb{E}(\\mathbf{s}_g\\mathbf{s}_g')=\\Sigma_g,\\quad (ii)\\; \\mathbb{E}(\\mathbf{s}_g\\mathbf{s}_q')=0,\\;g \\ne q,\n\\]\n\\(s_g\\) defined Eq. (4.40).real assumption \\((ii)\\); first one simply gives notation covariance matrix score associated \\(g^{th}\\) cluster. Remark covariance matrices can differ across clusters. , cluster-based inference robust heteroskedasticity intra-cluster dependence without imposing restrictions (unknown) form either .Naturally, matrix \\(\\Sigma_g\\) depends covariance structure \\(\\varepsilon\\)’s. particular, \\(\\Omega_g = \\mathbb{E}(\\boldsymbol\\varepsilon_g\\boldsymbol\\varepsilon_g'|\\mathbf{X}_g)\\), \\(\\Sigma_g = \\mathbb{E}(\\mathbf{X}_g'\\Omega_g\\mathbf{X}_g)\\).Hypothesis 4.6, comes conditional covariance matrix \\(\\mathbf{b}\\) :\n\\[\\begin{equation}\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\sum_{g=1}^G \\Sigma_g\\right)\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\tag{4.41}\n\\end{equation}\\]Let us denote \\(\\varepsilon_{g,}\\) error associated \\(^{th}\\) component vector \\(\\boldsymbol\\varepsilon_g\\). Consider special case \\(\\mathbb{E}(\\varepsilon_{g,} \\varepsilon_{g,j}|\\mathbf{X}_g)=\\sigma^2\\mathbb{}_{\\{=j\\}}\\), Eq. (4.41) gives standard expression \\(\\sigma^2\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\) (see Eq. (4.8)).\\(\\mathbb{E}(\\varepsilon_{gi} \\varepsilon_{gj}|\\mathbf{X}_g)=\\sigma_{gi}^2\\mathbb{}_{\\{=j\\}}\\), fall case addressed White formula (see Example 4.8). , case, conditional covariance matrix \\(\\mathbf{b}\\) :\n\\[\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}'\\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]\\mathbf{X}\\right)\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\n\\]\nWhite (1980), natural way approach conditional covariance given Eq. (4.41) consists replacing \\(\\Sigma_g\\) matrices sample equivalent, .e. \\(\\widehat{\\Sigma}_g=\\mathbf{X}_g'\\mathbf{e}_g\\mathbf{e}_g'\\mathbf{X}_g\\). Adding corrections number degrees freedom, leads following estimate covariance matrix \\(\\mathbf{b}\\):\n\\[\\begin{equation}\n\\frac{G(n-1)}{(G-1)(n-K)}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\sum_{g=1}^G\\widehat{\\Sigma}_g\\right) \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}. \\tag{4.42}\n\\end{equation}\\]\nprevious estimate CRCV1 J. G. MacKinnon, Nielsen, Webb (2022). Note indeed find White-MacKinnon estimator (Eq. (4.34)) \\(G=n\\).Remark one cluster (\\(G=1\\)), neglecting degree--freedom correction, :\n\\[\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}'\\mathbf{e}\\mathbf{e}'\\mathbf{X}\\right) \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1} = 0\n\\]\n\\(\\mathbf{X}'\\mathbf{e}=0\\). Hence, large clusters necessarily increase variance.Often, working panel data (see Chapter 5), want cluster different dimensions. typical case data indexed individuals (\\(\\)) time (\\(t\\)). case, may indeed suspect : () residuals correlated across clusters dates (e.g., monthly data, cluster may one year) (b) residuals correlated across clusters individuals (e.g., data county level cluster may state). case, one can employ two-way clustering.Formally, consider two distinct partitions data: one index \\(g\\), \\(g \\\\{1,\\dots,G\\}\\), index \\(h\\), \\(h \\\\{1,\\dots,H\\}\\). Accordingly, denote \\(\\mathbf{X}_{g,h}\\) submatrix \\(\\mathbf{X}\\) contains explanatory variables corresponding clusters \\(g\\) \\(h\\) (e.g., firms given country \\(g\\) given date \\(h\\)). also denote \\(\\mathbf{X}_{g,\\bullet}\\) (respectively \\(\\mathbf{X}_{\\bullet,h}\\)) submatrix \\(\\mathbf{X}\\) containing explanatory variables pertaining cluster \\(g\\), possible values \\(h\\) (resp. cluster \\(h\\), possible values \\(g\\)).make following assumption:Hypothesis 4.7  (Two-way clusters) :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}(\\mathbf{s}_{g,\\bullet}\\mathbf{s}_{g,\\bullet}')=\\Sigma_g,\\quad \\mathbb{E}(\\mathbf{s}_{\\bullet,h}\\mathbf{s}_{\\bullet,h}')=\\Sigma^*_h,\\quad \\mathbb{E}(\\mathbf{s}_{g,h}\\mathbf{s}_{g,h}')=\\Sigma_{g,h},\\\\ &&\\mathbb{E}(\\mathbf{s}_{g,h}\\mathbf{s}_{q,k}')=0\\;\\mbox{}g\\neq q\\mbox{ }h \\ne k.\n\\end{eqnarray*}\\]Proposition 4.16  (Covariance scores two-way-cluster setup) assumption, matrix covariance scores given :\n\\[\n\\Sigma = \\sum_{g=1}^G \\Sigma_{g} + \\sum_{h=1}^H \\Sigma^*_{h} - \\sum_{g=1}^G\\sum_{h=1}^H \\Sigma_{g,h}.\n\\]\n(last term right-hand side must subtracted order avoid double counting.)Proof. :\n\\[\\begin{eqnarray*}\n\\Sigma &=& \\sum_{g=1}^G\\sum_{q=1}^G\\sum_{h=1}^H\\sum_{k=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{q,k}'\\\\\n&=& \\sum_{g=1}^G\\underbrace{\\left(\\sum_{h=1}^H\\sum_{k=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{g,k}'\\right)}_{=\\Sigma_g}+\\sum_{h=1}^H\\underbrace{\\left(\\sum_{g=1}^G\\sum_{q=1}^G \\mathbf{s}_{g,h}\\mathbf{s}_{q,h}'\\right)}_{=\\Sigma^*_h}-\\sum_{g=1}^G\\sum_{h=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{g,h}',\n\\end{eqnarray*}\\]\ngives result.asymptotic theory can based two different approaches: () large number clusters (common case), (ii) fixed number clusters large number observations cluster (see Subsections 4.1 4.2 J. G. MacKinnon, Nielsen, Webb (2022)). variable \\(N_g\\)’s (clusters’ sizes heterogeneous terms size), less reliable asymptotic inference based Eq. (4.42), especially clusters unusually large, distribution data heavy-tailed. issues somehow mitigated clusters approximate factor structure.practice, \\(\\Sigma\\) estimated :\n\\[\n\\widehat{\\Sigma} = \\sum_{g=1}^G \\widehat{\\mathbf{s}}_{g,\\bullet}\\widehat{\\mathbf{s}}_{g,\\bullet}' + \\sum_{h=1}^H \\widehat{\\mathbf{s}}_{\\bullet,h}\\widehat{\\mathbf{s}}_{\\bullet,h} - \\sum_{g=1}^G\\sum_{h=1}^H \\widehat{\\mathbf{s}}_{g,h}\\widehat{\\mathbf{s}}_{g,h}',\n\\]\nuse:\n\\[\n\\widehat{\\mathbb{V}ar}(\\mathbf{b}) = \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\widehat{\\Sigma}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\n\\]alternative asymptotic approximation distribution statistic interest, one can resort bootstrap approximation (see Section 5 J. G. MacKinnon, Nielsen, Webb (2022)). R, package fwildclusterboot allows implement approaches.8Let us come back analysis effect systemic financial crises GDP growth. Clustering data country level , , country time levels gives following:","code":"\neq <- lm(growth~crisisJST+iso+year,data=JST.red)\nvcov1 <- vcovHC(eq, type = \"HC1\")\nvcov2 <- vcovCL(eq, cluster = JST.red[, c(\"iso\")])\nvcov3 <- vcovCL(eq, cluster = JST.red[, c(\"iso\",\"year\")])\nstargazer::stargazer(eq, eq, eq, eq,type = \"text\",\n                     column.labels = c(\"No HAC\", \"Heterosk.\",\n                                       \"1-way Clust.\",\"1-way Clust.\"),\n                     omit = c(\"iso\",\"year\"),no.space = TRUE,keep.stat = \"n\",\n                     se = list(NULL,sqrt(diag(vcov1)),\n                               sqrt(diag(vcov2)),sqrt(diag(vcov3))))## \n## ==========================================================\n##                           Dependent variable:             \n##              ---------------------------------------------\n##                                 growth                    \n##               No HAC   Heterosk. 1-way Clust. 1-way Clust.\n##                 (1)       (2)        (3)          (4)     \n## ----------------------------------------------------------\n## crisisJST    -0.015*** -0.015***  -0.015***     -0.015**  \n##               (0.005)   (0.006)    (0.006)      (0.007)   \n## Constant     0.042***  0.042***    0.042***     0.042***  \n##               (0.005)   (0.007)    (0.007)      (0.002)   \n## ----------------------------------------------------------\n## Observations   1,258     1,258      1,258        1,258    \n## ==========================================================\n## Note:                          *p<0.1; **p<0.05; ***p<0.01"},{"path":"ChapterLS.html","id":"shrinkage-methods","chapter":"4 Linear Regressions","heading":"4.6 Shrinkage methods","text":"Chosing appropriate explanatory variables often complicated, especially presence many potentially relevant covariates. Keeping large number covariates results large standard deviations estimated parameters (see Section 4.3.3). order address issue, shrinkage methods designed. objective methods help select limited number variables shrinking regression coefficients less useful variables towards zero. two best-known shrinkage techniques ridge regression lasso approach.9 cases (ridge lasso), OLS minimization problem (see Section 4.2), .e.,\n\\[\\begin{equation}\n\\mathbf{b} = \\underset{\\boldsymbol\\beta}{\\mbox{argmin}}\\; \\sum_{=1}^n(y_i - \\mathbf{x}_i'\\boldsymbol\\beta)^2\n\\end{equation}\\]\nreplaced following:\n\\[\\begin{equation}\n\\mathbf{b}_\\lambda = \\underset{\\boldsymbol\\beta}{\\mbox{argmin}}\\; \\sum_{=1}^n(y_i - \\mathbf{x}_i'\\boldsymbol\\beta)^2 + \\lambda f(\\boldsymbol\\beta),\\tag{4.43}\n\\end{equation}\\]\n\\(\\lambda f(\\boldsymbol\\beta)\\) penalty term positively depends “size” components \\(\\boldsymbol\\beta\\). term called shrinkage penalty term.Specifically, assuming vector \\(\\mathbf{x}_i\\) potential covariates dimension \\(K \\times 1\\), :\n\\[\\begin{eqnarray*}\nf(\\boldsymbol\\beta) & = & \\sum_{j=1}^K \\beta_j^2 \\quad \\mbox{ridge case ($\\ell_2$ norm)},\\\\\nf(\\boldsymbol\\beta) & = & \\sum_{j=1}^K |\\beta_j| \\quad \\mbox{lasso case ($\\ell_1$ norm)}.\n\\end{eqnarray*}\\]cases, want involve intercept set parameters shrink, preceding equations respectively replaced :\n\\[\\begin{eqnarray*}\nf(\\boldsymbol\\beta) & = & \\sum_{j=2}^K \\beta_j^2 \\quad \\mbox{(ridge)},\\\\\nf(\\boldsymbol\\beta) & = & \\sum_{j=2}^K |\\beta_j| \\quad \\mbox{(lasso)}.\n\\end{eqnarray*}\\]nature penalty —based either \\(\\ell_1\\) \\(\\ell_2\\) norm— implies different behaviour parameter estimates \\(\\lambda\\) —tuning parameter— grows. ridge regression, coefficient estimates go zero (shrinkage); lasso case, coefficients reach zero \\(\\lambda\\) reach values. words, ridge regression achieve shrinkage, lasso regressions achieve shrinkage variable selection.Parameter \\(\\lambda\\) determined separately minimization problem Eq. (4.43). One can combine standard criteria (e.g., BIC Akaike) purpose.R, one can use glmnet package run ridge lasso regressions. following example, employ package model interest rates proposed debtors, using data extracted Lending Club platform.begin , let us define variables want consider:Let us standardize data:Next, define set \\(\\lambda\\) use, run ridge lasso regressions:following figure shows estimated parameters depend \\(\\lambda\\):Let us take two values \\(\\lambda\\) see associated estimated parameters context lasso regressions:glmnet package (see Hastie et al. (2021)) also offers tools implement cross-validation:","code":"\nlibrary(AEC)\nlibrary(glmnet)\ncredit$owner <- 1*(credit$home_ownership==\"OWN\")\ncredit$renter <- 1*(credit$home_ownership==\"MORTGAGE\")\ncredit$verification_status <- 1*(credit$verification_status==\"Not Verified\")\ncredit$emp_length_10 <- 1*(credit$emp_length_10)\ncredit$log_annual_inc <- log(credit$annual_inc)\ncredit$log_funded_amnt <- log(credit$funded_amnt)\ncredit$annual_inc2 <- (credit$annual_inc)^2\ncredit$funded_amnt2 <- (credit$funded_amnt)^2\nx <- subset(credit,\n            select = c(delinq_2yrs,annual_inc,annual_inc2,log_annual_inc,\n                       dti,installment,verification_status,funded_amnt,\n                       funded_amnt2,log_funded_amnt,pub_rec,emp_length_10,\n                       owner,renter,pub_rec_bankruptcies,revol_util,revol_bal))\ny <- scale(credit$int_rate)\nx <- scale(x)\ngrid.lambda <- seq(0,.2,by=.005)\nresult.ridge <- glmnet(x, y, alpha = 0, lambda = grid.lambda)\nresult.lasso <- glmnet(x, y, alpha = 1, lambda = grid.lambda)\nvariab <- 6\nplot(result.ridge$lambda,coef(result.ridge)[variab,],type=\"l\",\n     ylim=c(min(coef(result.ridge)[variab,],coef(result.lasso)[variab,]),\n            max(coef(result.ridge)[variab,],coef(result.lasso)[variab,])),\n     xlab=expression(lambda),ylab=\"Estimated parameter\",lwd=2)\nlines(result.lasso$lambda,coef(result.lasso)[variab,],col=\"red\",lwd=2)\ni <- 20; j <- 40\ncbind(result.lasso$lambda[i],result.lasso$lambda[j])##       [,1]  [,2]\n## [1,] 0.105 0.005\ncbind(coef(result.lasso)[,i],coef(result.lasso)[,j])##                               [,1]          [,2]\n## (Intercept)          -1.044971e-15  1.088731e-14\n## delinq_2yrs           6.308870e-02  6.893527e-02\n## annual_inc            0.000000e+00  4.595653e-03\n## annual_inc2           0.000000e+00  0.000000e+00\n## log_annual_inc        0.000000e+00 -3.612382e-02\n## dti                   0.000000e+00  2.242246e-02\n## installment           1.476796e-01  8.228729e+00\n## verification_status   0.000000e+00 -9.750047e-04\n## funded_amnt           0.000000e+00 -7.309169e+00\n## funded_amnt2          0.000000e+00 -4.711846e-01\n## log_funded_amnt       0.000000e+00 -2.460932e-01\n## pub_rec               3.390816e-02  5.997252e-02\n## emp_length_10         0.000000e+00 -1.924941e-02\n## owner                 0.000000e+00 -2.444599e-02\n## renter               -3.882640e-02 -6.243087e-02\n## pub_rec_bankruptcies  0.000000e+00  0.000000e+00\n## revol_util            0.000000e+00  0.000000e+00\n## revol_bal             0.000000e+00  2.402268e-03\n# Compute values of y predicted by the model, for all lambdas:\npred1 <- predict(result.lasso,as.matrix(x))\n# Compute values of y predicted by the model, for a specific value:\npred2 <- predict(result.lasso,as.matrix(x),s=0.085)\n# cross validation (cv):\ncvglmnet <- cv.glmnet(as.matrix(x),y)\nplot(cvglmnet)\n# lambda.min: lambda that gives minimum mean cross-validated error\ncvglmnet$lambda.min ## [1] 0.004091039\n# lambda.1se: largest lambda s.t. cost within the one-std-dev cv-based band\ncvglmnet$lambda.1se## [1] 0.00448991\ncoef(cvglmnet, s = \"lambda.min\") # associated parameters## 18 x 1 sparse Matrix of class \"dgCMatrix\"\n##                                 s1\n## (Intercept)           1.128990e-14\n## delinq_2yrs           6.625860e-02\n## annual_inc            6.054923e-03\n## annual_inc2           .           \n## log_annual_inc       -3.795151e-02\n## dti                   2.065272e-02\n## installment           8.518961e+00\n## verification_status  -2.894435e-03\n## funded_amnt          -7.560853e+00\n## funded_amnt2         -5.041923e-01\n## log_funded_amnt      -2.537567e-01\n## pub_rec               5.804333e-02\n## emp_length_10        -1.894666e-02\n## owner                -2.484192e-02\n## renter               -6.039831e-02\n## pub_rec_bankruptcies  .           \n## revol_util            .           \n## revol_bal             3.093752e-03\n# predicted values of y for specific values of x:\npredict(cvglmnet, newx = as.matrix(x)[1:5,], s = \"lambda.min\") ##        lambda.min\n## 21529  0.34496384\n## 21547 -0.04553753\n## 21579  0.56455499\n## 21583 -0.20696954\n## 21608 -0.12165356"},{"path":"Panel.html","id":"Panel","chapter":"5 Panel regressions","heading":"5 Panel regressions","text":"","code":""},{"path":"Panel.html","id":"specification-and-notations","chapter":"5 Panel regressions","heading":"5.1 Specification and notations","text":"standard panel situation follows: sample covers lot “entities”, indexed \\(\\\\{1,\\dots,n\\}\\), \\(n\\) large, , entity, observe different variables small number periods \\(t \\\\{1,\\dots,T\\}\\). longitudinal dataset.linear panel regression model :\n\\[\\begin{equation}\ny_{,t} = \\mathbf{x}'_{,t}\\underbrace{\\boldsymbol\\beta}_{K \\times 1} + \\underbrace{\\mathbf{z}'_{}\\boldsymbol\\alpha}_{\\mbox{Individual effects}} + \\varepsilon_{,t}.\\tag{5.1}\n\\end{equation}\\]running panel regressions, usual objective estimate \\(\\boldsymbol\\beta\\).Figure 5.1 illustrates panel-data situation. model \\(y_i = \\alpha_i + \\beta x_{,t} + \\varepsilon_{,t}\\), \\(t \\\\{1,2\\}\\). Panel (b), blue dots \\(t=1\\), red dots \\(t=2\\). lines relate dots associated entity \\(\\). remarkable simulated model , unconditional correlation \\(y\\) \\(x\\) negative, conditional correlation (conditional \\(\\alpha_i\\)) positive. Indeed, sign conditional correlation sign \\(\\beta\\), positive th simulated example (\\(\\beta=5\\)). words, one know panel nature data, tempting say \\(\\beta<0\\), case, due fixed effects (\\(\\alpha_i\\)’s) negatively correlated \\(x_i\\)’s.\nFigure 5.1: data panels. Panel (b), blue dots \\(t=1\\), red dots \\(t=2\\). lines relate dots associated entity \\(\\).\nFigure 5.2 presents type plot based Cigarette Consumption Panel dataset (CigarettesSW dataset, used J. Stock Watson (2003)). dataset documents average consumption cigarettes 48 continental US states two dates (1985 1995).\nFigure 5.2: Cigarette consumption versus real price CigarettesSW panel dataset.\nmake use following notations:\n\\[\n\\mathbf{y}_i =\n\\underbrace{\\left[\n\\begin{array}{c}\ny_{,1}\\\\\n\\vdots\\\\\ny_{,T}\n\\end{array}\\right]}_{T \\times 1}, \\quad\n\\boldsymbol\\varepsilon_i =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\varepsilon_{,1}\\\\\n\\vdots\\\\\n\\varepsilon_{,T}\n\\end{array}\\right]}_{T \\times 1}, \\quad\n\\mathbf{x}_i =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\mathbf{x}_{,1}'\\\\\n\\vdots\\\\\n\\mathbf{x}_{,T}'\n\\end{array}\\right]}_{T \\times K}, \\quad\n\\mathbf{X} =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\mathbf{x}_{1}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}\n\\end{array}\\right]}_{(nT) \\times K}.\n\\]\n\\[\n\\tilde{\\mathbf{y}}_i =\n\\left[\n\\begin{array}{c}\ny_{,1} - \\bar{y}_i\\\\\n\\vdots\\\\\ny_{,T} - \\bar{y}_i\n\\end{array}\\right], \\quad\n\\tilde{\\boldsymbol\\varepsilon}_i =\n\\left[\n\\begin{array}{c}\n\\varepsilon_{,1} - \\bar{\\varepsilon}_i\\\\\n\\vdots\\\\\n\\varepsilon_{,T} - \\bar{\\varepsilon}_i\n\\end{array}\\right],\n\\]\n\\[\n\\tilde{\\mathbf{x}}_i =\n\\left[\n\\begin{array}{c}\n\\mathbf{x}_{,1}' - \\bar{\\mathbf{x}}_i'\\\\\n\\vdots\\\\\n\\mathbf{x}_{,T}' - \\bar{\\mathbf{x}}_i'\n\\end{array}\\right], \\quad\n\\tilde{\\mathbf{X}} =\n\\left[\n\\begin{array}{c}\n\\tilde{\\mathbf{x}}_{1}\\\\\n\\vdots\\\\\n\\tilde{\\mathbf{x}}_{n}\n\\end{array}\\right], \\quad\n\\tilde{\\mathbf{Y}} =\n\\left[\n\\begin{array}{c}\n\\tilde{\\mathbf{y}}_{1}\\\\\n\\vdots\\\\\n\\tilde{\\mathbf{y}}_{n}\n\\end{array}\\right],\n\\]\n\n\\[\n\\bar{y}_i = \\frac{1}{T} \\sum_{t=1}^T y_{,t}, \\quad \\bar{\\varepsilon}_i = \\frac{1}{T}\\sum_{t=1}^T \\varepsilon_{,t} \\quad \\mbox{} \\quad \\bar{\\mathbf{x}}_i = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{x}_{,t}.\n\\]","code":"\nT <- 2; n <- 12 # 2 periods and 12 entities\nalpha <- 5*rnorm(n) # draw fixed effects\nx.1 <- rnorm(n) - .5*alpha # note: x_i's correlate to alpha_i's\nx.2 <- rnorm(n) - .5*alpha\nbeta <- 5; sigma <- .3\ny.1 <- alpha + x.1 + sigma*rnorm(n);y.2 <- alpha + x.2 + sigma*rnorm(n)\nx <- c(x.1,x.2) # pooled x\ny <- c(y.1,y.2) # pooled y\npar(mfrow=c(1,2))\nplot(x,y,col=\"black\",pch=19,xlab=\"x\",ylab=\"y\",main=\"(a)\")\nplot(x,y,col=\"black\",pch=19,xlab=\"x\",ylab=\"y\",main=\"(b)\")\npoints(x.1,y.1,col=\"blue\",pch=19);points(x.2,y.2,col=\"red\",pch=19)\nfor(i in 1:n){lines(c(x.1[i],x.2[i]),c(y.1[i],y.2[i]))}"},{"path":"Panel.html","id":"three-standard-cases","chapter":"5 Panel regressions","heading":"5.2 Three standard cases","text":"three typical situations:Pooled regression: \\(\\mathbf{z}_i \\equiv 1\\). case amounts case studied Chapter 4.Fixed Effects (Section 5.3): \\(\\mathbf{z}_i\\) unobserved, correlates \\(\\mathbf{x}_i\\) \\(\\Rightarrow\\) \\(\\mathbf{b}\\) biased inconsistent OLS regression \\(\\mathbf{y}\\) \\(\\mathbf{X}\\) (omitted variable, see Section 4.3.2).Random Effects (Section 5.4): \\(\\mathbf{z}_i\\) unobserved, uncorrelated \\(\\mathbf{x}_i\\). model writes:\n\\[\ny_{,t} = \\mathbf{x}'_{,t}\\boldsymbol\\beta + \\alpha +  \\underbrace{{\\color{blue}u_i + \\varepsilon_{,t}}}_{\\mbox{compound error}},\n\\]\n\\(\\alpha = \\mathbb{E}(\\mathbf{z}'_{}\\boldsymbol\\alpha)\\) \\(u_i = \\mathbf{z}'_{}\\boldsymbol\\alpha - \\mathbb{E}(\\mathbf{z}'_{}\\boldsymbol\\alpha) \\perp \\mathbf{x}_i\\). case, OLS consistent, efficient. GLS can used gain efficiencies OLS (see Section 4.5.2 presentation GLS approach).","code":""},{"path":"Panel.html","id":"FixedEffect","chapter":"5 Panel regressions","heading":"5.3 Estimation of Fixed-Effects Models","text":"Hypothesis 5.1  (Fixed-effect model) assume :perfect multicollinearity among regressors.\\(\\mathbb{E}(\\varepsilon_{,t}|\\mathbf{X})=0\\), \\(,t\\).:\n\\[\n\\mathbb{E}(\\varepsilon_{,t}\\varepsilon_{j,s}|\\mathbf{X}) =\n\\left\\{\n\\begin{array}{cl}\n\\sigma^2 & \\mbox{$=j$ $s=t$},\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\\right.\n\\]assumptions analogous introduced standard linear regression:\\(\\leftrightarrow\\) Hyp. 4.1, (ii) \\(\\leftrightarrow\\) Hyp. 4.2, (iii) \\(\\leftrightarrow\\) Hyp. 4.3 + 4.4.matrix form, given \\(\\), model writes:\n\\[\n\\mathbf{y}_i = \\mathbf{x}_i \\boldsymbol\\beta + \\mathbf{1}\\alpha_i + \\boldsymbol\\varepsilon_i,\n\\]\n\\(\\mathbf{1}\\) \\(T\\)-dimensional vector ones.Least Square Dummy Variable (LSDV) model:\n\\[\\begin{equation}\n\\mathbf{y} = [\\mathbf{X} \\quad \\mathbf{D}]\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\n\\end{array}\n\\right]\n+ \\boldsymbol\\varepsilon, \\tag{5.2}\n\\end{equation}\\]\n:\n\\[\n\\mathbf{D} = \\underbrace{ \\left[\\begin{array}{cccc}\n\\mathbf{1}&\\mathbf{0}&\\dots&\\mathbf{0}\\\\\n\\mathbf{0}&\\mathbf{1}&\\dots&\\mathbf{0}\\\\\n&&\\vdots&\\\\\n\\mathbf{0}&\\mathbf{0}&\\dots&\\mathbf{1}\\\\\n\\end{array}\\right]}_{(nT \\times n)}.\n\\]linear regression (Eq. (5.2)) —dummy variables— satisfies Gauss-Markov conditions (Theorem 4.1). Hence, context, OLS estimator best linear unbiased estimator (BLUE).Denoting \\(\\mathbf{Z}\\) matrix \\([\\mathbf{X} \\quad \\mathbf{D}]\\), \\(\\mathbf{b}\\) \\(\\mathbf{}\\) respective OLS estimates \\(\\boldsymbol\\beta\\) \\(\\boldsymbol\\alpha\\), :\n\\[\\begin{equation}\n\\boxed{\n\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\n\\right]\n= [\\mathbf{Z}'\\mathbf{Z}]^{-1}\\mathbf{Z}'\\mathbf{y}.} \\tag{5.3}\n\\end{equation}\\]asymptotical distribution \\([\\mathbf{b}',\\mathbf{}']'\\) derives standard OLS context: Prop. 4.11 can used replaced \\(\\mathbf{X}\\) \\(\\mathbf{Z}=[\\mathbf{X} \\quad \\mathbf{D}]\\).:\n\\[\\begin{equation}\n\\boxed{\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\n\\right] \\overset{d}{\\rightarrow}\n\\mathcal{N}\\left(\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\n\\end{array}\n\\right],\n\\sigma^2 \\frac{Q^{-1}}{nT}\n\\right),}\n\\end{equation}\\]\n\n\\[\nQ = \\mbox{plim}_{nT \\rightarrow \\infty} \\frac{1}{nT} \\mathbf{Z}'\\mathbf{Z},\n\\]\nassuming previous limit exists.practice, estimator covariance matrix \\([\\mathbf{b}',\\mathbf{}']'\\) :\n\\[\ns^2 \\left( \\mathbf{Z}'\\mathbf{Z}\\right)^{-1} \\quad \\quad s^2 = \\frac{\\mathbf{e}'\\mathbf{e}}{nT - K - n},\n\\]\n\\(\\mathbf{e}\\) \\((nT) \\times 1\\) vector OLS residuals.alternative way expressing LSDV estimators. involves residual-maker matrix matrix \\(\\mathbf{M_D}=\\mathbf{} - \\mathbf{D}(\\mathbf{D}'\\mathbf{D})^{-1}\\mathbf{D}'\\) (see Eq. (4.3)), acts operator removes entity-specific means, .e.:\n\\[\n\\tilde{\\mathbf{Y}} = \\mathbf{M_D}\\mathbf{Y}, \\quad \\tilde{\\mathbf{X}} = \\mathbf{M_D}\\mathbf{X} \\quad \\quad \\tilde{\\boldsymbol\\varepsilon} = \\mathbf{M_D}\\boldsymbol\\varepsilon.\n\\]notations, using Frisch-Waugh theorem (Theorem 4.2), get another expression estimator \\(\\mathbf{b}\\) appearing Eq. (5.3):\n\\[\\begin{equation}\n\\boxed{\\mathbf{b} = [\\mathbf{X}'\\mathbf{M_D}\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{M_D}\\mathbf{y}.}\\tag{5.4}\n\\end{equation}\\]amounts regressing \\(\\tilde{y}_{,t}\\)’s (\\(= y_{,t} - \\bar{y}_i\\)) \\(\\tilde{\\mathbf{x}}_{,t}\\)’s (\\(=\\mathbf{x}_{,t} - \\bar{\\mathbf{x}}_i\\)).estimate \\(\\boldsymbol\\alpha\\) given :\n\\[\\begin{equation}\n\\boxed{\\mathbf{} = (\\mathbf{D}'\\mathbf{D})^{-1}\\mathbf{D}'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}),} \\tag{5.5}\n\\end{equation}\\]\nobtained developing second row \n\\[\n\\left[\n\\begin{array}{cc}\n\\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{D}\\\\\n\\mathbf{D}'\\mathbf{X} & \\mathbf{D}'\\mathbf{D}\n\\end{array}\\right]\n\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\\right] =\n\\left[\n\\begin{array}{c}\n\\mathbf{X}'\\mathbf{Y}\\\\\n\\mathbf{D}'\\mathbf{Y}\n\\end{array}\\right],\n\\]\nfirst-order conditions resulting least squares problem (see Eq. (4.2)).One can use different types fixed effects regression. Typically, one can time entity fixed effects. case, model writes:\n\\[\ny_{,t} = \\mathbf{x}_i'\\boldsymbol\\beta + \\alpha_i + \\gamma_t + \\varepsilon_{,t}.\n\\]LSDV approach (Eq. (5.2)) can still resorted . suffices extend \\(\\mathbf{Z}\\) matrix additional columns (called time dummies):\n\\[\\begin{equation}\n\\mathbf{y} = [\\mathbf{X} \\quad \\mathbf{D} \\quad \\mathbf{C}]\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\\\\\n\\boldsymbol\\gamma\n\\end{array}\n\\right]\n+ \\boldsymbol\\varepsilon, \\tag{5.6}\n\\end{equation}\\]\n:\n\\[\n\\mathbf{C} = \\left[\\begin{array}{cccc}\n\\boldsymbol{\\delta}_1&\\boldsymbol{\\delta}_2&\\dots&\\boldsymbol{\\delta}_{T-1}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\boldsymbol{\\delta}_1&\\boldsymbol{\\delta}_2&\\dots&\\boldsymbol{\\delta}_{T-1}\\\\\n\\end{array}\\right],\n\\]\n\\(T\\)-dimensional vector \\(\\boldsymbol\\delta_t\\) (time dummy) \n\\[\n[0,\\dots,0,\\underbrace{1}_{\\mbox{t$^{th}$ entry}},0,\\dots,0]'.\n\\]Using state year fixed effects CigarettesSW panel dataset yields following results:Example 5.1  (Housing prices interest rates) example, want estimate effect short long-term interest rate housing prices. data come Jordà, Schularick, Taylor (2017) dataset (see website).","code":"\ndata(\"CigarettesSW\", package = \"AER\")\nCigarettesSW$rprice  <- with(CigarettesSW, price/cpi) # compute real price\nCigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)\neq.pooled <- lm(log(packs)~log(rprice)+log(rincome),data=CigarettesSW)\neq.LSDV <- lm(log(packs)~log(rprice)+log(rincome)+state,\n              data=CigarettesSW)\nCigarettesSW$year <- as.factor(CigarettesSW$year)\neq.LSDV2 <- lm(log(packs)~log(rprice)+log(rincome)+state+year,\n               data=CigarettesSW)\nstargazer::stargazer(eq.pooled,eq.LSDV,eq.LSDV2,type=\"text\",no.space = TRUE,\n                     omit=c(\"state\",\"year\"),\n                     add.lines=list(c('State FE','No','Yes','Yes'),\n                                    c('Year FE','No','No','Yes')),\n                     omit.stat=c(\"f\",\"ser\"))## \n## ==========================================\n##                   Dependent variable:     \n##              -----------------------------\n##                       log(packs)          \n##                 (1)       (2)       (3)   \n## ------------------------------------------\n## log(rprice)  -1.334*** -1.210*** -1.056***\n##               (0.135)   (0.114)   (0.149) \n## log(rincome)  0.318**    0.121     0.497  \n##               (0.136)   (0.190)   (0.304) \n## Constant     10.067*** 9.954***  8.360*** \n##               (0.516)   (0.264)   (1.049) \n## ------------------------------------------\n## State FE        No        Yes       Yes   \n## Year FE         No        No        Yes   \n## Observations    96        96        96    \n## R2             0.552     0.966     0.967  \n## Adjusted R2    0.542     0.929     0.931  \n## ==========================================\n## Note:          *p<0.1; **p<0.05; ***p<0.01\nlibrary(AEC);library(sandwich)\ndata(JST); JST <- subset(JST,year>1950);N <- dim(JST)[1]\nJST$hpreal <- JST$hpnom/JST$cpi # real house price index\nJST$dhpreal <- 100*log(JST$hpreal/c(NaN,JST$hpreal[1:(N-1)]))\n# Put NA's when change in country:\nJST$dhpreal[c(0,JST$iso[2:N]!=JST$iso[1:(N-1)])] <- NaN\nJST$dhpreal[abs(JST$dhpreal)>30] <- NaN # remove extreme price change\nJST$YEAR <- as.factor(JST$year) # to have time fixed effects\neq1_noFE <- lm(dhpreal ~ stir + ltrate,data=JST)\neq1_FE   <- lm(dhpreal ~ stir + ltrate + iso + YEAR,data=JST)\neq2_noFE <- lm(dhpreal ~ I(ltrate-stir),data=JST)\neq2_FE <- lm(dhpreal ~ I(ltrate-stir) + iso + YEAR,data=JST)\nvcov_cluster1_noFE <- vcovHC(eq1_noFE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster1_FE   <- vcovHC(eq1_FE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster2_noFE <- vcovHC(eq2_noFE, cluster = JST[, c(\"iso\",\"YEAR\")])\nvcov_cluster2_FE   <- vcovHC(eq2_FE, cluster = JST[, c(\"iso\",\"YEAR\")])\nrobust_se_FE1_noFE <- sqrt(diag(vcov_cluster1_noFE))\nrobust_se_FE1_FE   <- sqrt(diag(vcov_cluster1_FE))\nrobust_se_FE2_noFE <- sqrt(diag(vcov_cluster2_noFE))\nrobust_se_FE2_FE   <- sqrt(diag(vcov_cluster2_FE))\nstargazer::stargazer(eq1_noFE, eq1_FE, eq2_noFE, eq2_FE, type = \"text\",\n                     column.labels = c(\"no FE\", \"with FE\", \"no FE\",\"with FE\"),\n                     omit = c(\"iso\",\"YEAR\",\"Constant\"),keep.stat = \"n\",\n                     add.lines=list(c('Country FE','No','Yes','No','Yes'),\n                                    c('Year FE','No','Yes','No','Yes')),\n                     se = list(robust_se_FE1_noFE,robust_se_FE1_FE,\n                               robust_se_FE2_noFE,robust_se_FE2_FE))## \n## =======================================================\n##                           Dependent variable:          \n##                  --------------------------------------\n##                                 dhpreal                \n##                    no FE   with FE    no FE    with FE \n##                     (1)      (2)       (3)       (4)   \n## -------------------------------------------------------\n## stir             0.485***  0.532***                    \n##                   (0.149)  (0.170)                     \n##                                                        \n## ltrate           -0.690*** -0.384**                    \n##                   (0.164)  (0.182)                     \n##                                                        \n## I(ltrate - stir)                    -0.476*** -0.475***\n##                                      (0.145)   (0.159) \n##                                                        \n## -------------------------------------------------------\n## Country FE          No       Yes       No        Yes   \n## Year FE             No       Yes       No        Yes   \n## Observations       1,141    1,141     1,141     1,141  \n## =======================================================\n## Note:                       *p<0.1; **p<0.05; ***p<0.01"},{"path":"Panel.html","id":"RandomEffect","chapter":"5 Panel regressions","heading":"5.4 Estimation of random effects models","text":", individual effects assumed correlated variables (\\(\\mathbf{x}_i\\)’s). context, OLS estimator consistent. However, efficient. GLS approach can employed gain efficiency.Random-effect models write:\n\\[\ny_{,t}=\\mathbf{x}'_{}\\boldsymbol\\beta + (\\alpha + \\underbrace{u_i}_{\\substack{\\text{Random}\\\\\\text{heterogeneity}}}) + \\varepsilon_{,t},\n\\]\n\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\varepsilon_{,t}|\\mathbf{X})&=&\\mathbb{E}(u_{}|\\mathbf{X}) =0,\\\\\n\\mathbb{E}(\\varepsilon_{,t}\\varepsilon_{j,s}|\\mathbf{X}) &=&\n\\left\\{\n\\begin{array}{cl}\n\\sigma_\\varepsilon^2 & \\mbox{ $=j$ $s=t$},\\\\\n0 & \\mbox{ otherwise.}\n\\end{array}\n\\right.\\\\\n\\mathbb{E}(u_{}u_{j}|\\mathbf{X}) &=&\n\\left\\{\n\\begin{array}{cl}\n\\sigma_u^2 & \\mbox{ $=j$},\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\n\\right.\\\\\n\\mathbb{E}(\\varepsilon_{,t}u_{j}|\\mathbf{X})&=&0 \\quad \\text{$$, $j$ $t$}.\n\\end{eqnarray*}\\]Introducing notations \\(\\eta_{,t} = u_i + \\varepsilon_{,t}\\) \\(\\boldsymbol\\eta_i = [\\eta_{,1},\\dots,\\eta_{,T}]'\\), \\(\\mathbb{E}(\\boldsymbol\\eta_i |\\mathbf{X}) = \\mathbf{0}\\) \\(\\mathbb{V}ar(\\boldsymbol\\eta_i | \\mathbf{X}) = \\boldsymbol\\Gamma\\), \n\\[\n\\boldsymbol\\Gamma = \\left[  \\begin{array}{ccccc}\n\\sigma_\\varepsilon^2+\\sigma_u^2 & \\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_u^2\\\\\n\\sigma_u^2 & \\sigma_\\varepsilon^2+\\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_u^2\\\\\n\\vdots && \\ddots && \\vdots \\\\\n\\sigma_u^2 & \\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_\\varepsilon^2+\\sigma_u^2\\\\\n\\end{array}\n\\right] = \\sigma_\\varepsilon^2\\mathbf{} + \\sigma_u^2\\mathbf{1}\\mathbf{1}'.\n\\]Denoting \\(\\boldsymbol\\Sigma\\) covariance matrix \\(\\boldsymbol\\eta = [\\boldsymbol\\eta_1',\\dots,\\boldsymbol\\eta_n']'\\), :\n\\[\n\\boldsymbol\\Sigma = \\mathbf{} \\otimes \\boldsymbol\\Gamma.\n\\]knew \\(\\boldsymbol\\Sigma\\), apply (feasible) GLS (Eq. (4.29), Section 4.5.2):\n\\[\n\\boldsymbol\\beta = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}.\n\\]\n(explained Section 4.5.2, amounts regressing \\({\\boldsymbol\\Sigma^{-1/2}}'\\mathbf{y}\\) \\({\\boldsymbol\\Sigma^{-1/2}}'\\mathbf{X}\\).)can checked \\(\\boldsymbol\\Sigma^{-1/2} = \\mathbf{} \\otimes (\\boldsymbol\\Gamma^{-1/2})\\) \n\\[\n\\boldsymbol\\Gamma^{-1/2} = \\frac{1}{\\sigma_\\varepsilon}\\left( \\mathbf{} - \\frac{\\theta}{T}\\mathbf{1}\\mathbf{1}'\\right),\\quad \\mbox{}\\quad\\theta = 1 - \\frac{\\sigma_\\varepsilon}{\\sqrt{\\sigma_\\varepsilon^2+T\\sigma_u^2}}.\n\\]Hence, knew \\(\\boldsymbol\\Sigma\\), transform data follows:\n\\[\n\\boldsymbol\\Gamma^{-1/2}\\mathbf{y}_i = \\frac{1}{\\sigma_\\varepsilon}\\left[\\begin{array}{c}y_{,1} - \\theta\\bar{y}_i\\\\y_{,2} - \\theta\\bar{y}_i\\\\\\vdots\\\\y_{,T} - \\theta\\bar{y}_i\\\\\\end{array}\\right].\n\\]\\(\\boldsymbol\\Sigma\\) unknown? One can take deviations group means remove heterogeneity:\n\\[\\begin{equation}\ny_{,t} - \\bar{y}_i = [\\mathbf{x}_{,t} - \\bar{\\mathbf{x}}_i]'\\boldsymbol\\beta + (\\varepsilon_{,t} - \\bar{\\varepsilon}_i).\\tag{5.7}\n\\end{equation}\\]\nprevious equation can consistently estimated OLS. (Although residuals correlated across \\(t\\)’s observations pertaining given entity, OLS remain consistent; see Prop. 4.13.)\\(\\mathbb{E}\\left[\\sum_{=1}^{T}(\\varepsilon_{,t}-\\bar{\\varepsilon}_i)^2\\right] = (T-1)\\sigma_{\\varepsilon}^2\\).\\(\\varepsilon_{,t}\\)’s observed \\(\\mathbf{b}\\), OLS estimator \\(\\boldsymbol\\beta\\) Eq. (5.7), consistent estimator \\(\\boldsymbol\\beta\\). Using adjustment degrees freedom, can approximate variance :\n\\[\n\\hat{\\sigma}_e^2 = \\frac{1}{nT-n-K}\\sum_{=1}^{n}\\sum_{t=1}^{T}(e_{,t} - \\bar{e}_i)^2.\n\\]\\(\\sigma_u^2\\)? can exploit fact OLS consistent pooled regression:\n\\[\n\\mbox{plim }s^2_{pooled} = \\mbox{plim }\\frac{\\mathbf{e}'\\mathbf{e}}{nT-K-1} = \\sigma_u^2 + \\sigma_\\varepsilon^2,\n\\]\ntherefore use \\(s^2_{pooled} - \\hat{\\sigma}_e^2\\) approximation \\(\\sigma_u^2\\).Let us come back Example 5.1 (relationship changes housing prices interest rates). following, use random effect specification; compare results obtained pooled regression fixed-effect model. , use function plm package name. (Note eq.FE similar eq1 Example 5.1.)One can run Hausman (1978) test order check whether fixed-effect model needed. Indeed, case (.e., covariates correlated disturbances), preferable use random-effect estimation latter efficient.p-value high, reject null hypothesis according covariates errors uncorrelated. therefore prefer random-effect model.Example 5.2  (Spatial data) example makes use Airbnb prices (Zürich, 22 June 2017), collected Tom Slee’s website. covariates number bedrooms number people can accommodated. consider use district fixed effects. Figure 5.3 shows price explain (size circles proportional prices). white lines delineate 12 districts city.\nFigure 5.3: Airbnb prices Zurich area, 22 June 2017. size circles proportional prices. White lines delineate 12 districts city.\nLet us regress prices covariates well district dummies:Figure 5.4 compares residuals without fixed effects. sizes circles proportional absolute values residuals, color indicates sign (blue positive).\nFigure 5.4: Regression residuals. sizes circles proportional absolute values residuals, color indicates sign (blue negative).\nfixed effects, colors better balanced within district.","code":"\nlibrary(plm);library(stargazer)\neq.RE <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"random\",effect=\"twoways\")\neq.FE <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"within\",effect=\"twoways\")\neq0   <- plm(dhpreal ~ stir + ltrate,data=JST,index=c(\"iso\",\"YEAR\"),\n             model=\"pooling\") \nstargazer(eq0, eq.RE, eq.FE, type = \"text\",no.space = TRUE,\n                     column.labels=c(\"Pooled\",\"Random Effect\",\"Fixed Effects\"),\n                     add.lines=list(c('State FE','No','Yes','Yes'),\n                                    c('Year FE','No','Yes','Yes')),\n                     omit.stat=c(\"f\",\"ser\"))## \n## ==================================================\n##                       Dependent variable:         \n##              -------------------------------------\n##                             dhpreal               \n##               Pooled   Random Effect Fixed Effects\n##                 (1)         (2)           (3)     \n## --------------------------------------------------\n## stir         0.485***    0.456***      0.532***   \n##               (0.114)     (0.019)       (0.134)   \n## ltrate       -0.690***   -0.541***     -0.384***  \n##               (0.127)     (0.020)       (0.145)   \n## Constant     4.103***    3.341***                 \n##               (0.421)     (0.096)                 \n## --------------------------------------------------\n## State FE        No          Yes           Yes     \n## Year FE         No          Yes           Yes     \n## Observations   1,141       1,141         1,141    \n## R2             0.027       0.024         0.015    \n## Adjusted R2    0.025       0.022        -0.067    \n## ==================================================\n## Note:                  *p<0.1; **p<0.05; ***p<0.01\nphtest(eq.FE,eq.RE)## \n##  Hausman Test\n## \n## data:  dhpreal ~ stir + ltrate\n## chisq = 3.8386, df = 2, p-value = 0.1467\n## alternative hypothesis: one model is inconsistent\neq_noFE <- lm(price~bedrooms+accommodates,data=airbnb)\neq_FE   <- lm(price~bedrooms+accommodates+neighborhood,data=airbnb)\n# Adjust standard errors:\ncov_FE          <- vcovHC(eq_FE, cluster = airbnb[, c(\"neighborhood\")])\nrobust_se_FE    <- sqrt(diag(cov_FE))\ncov_noFE        <- vcovHC(eq_noFE, cluster = airbnb[, c(\"neighborhood\")])\nrobust_se_noFE  <- sqrt(diag(cov_noFE))\nstargazer::stargazer(eq_FE, eq_noFE, eq_FE, eq_noFE, type = \"text\",\n                     column.labels = c(\"FE (no HAC)\", \"No FE (no HAC)\",\n                                       \"FE (with HAC)\", \"No FE (with HAC)\"),\n                     omit = c(\"neighborhood\"),no.space = TRUE,\n                     omit.labels = c(\"District FE\"),keep.stat = \"n\",\n                     se = list(NULL, NULL, robust_se_FE, robust_se_noFE))## \n## ======================================================================\n##                                 Dependent variable:                   \n##              ---------------------------------------------------------\n##                                        price                          \n##              FE (no HAC) No FE (no HAC) FE (with HAC) No FE (with HAC)\n##                  (1)          (2)            (3)            (4)       \n## ----------------------------------------------------------------------\n## bedrooms      7.229***      5.629**       7.229***        5.629***    \n##                (2.135)      (2.194)        (2.052)        (2.073)     \n## accommodates  16.426***    17.449***      16.426***      17.449***    \n##                (1.284)      (1.323)        (1.431)        (1.428)     \n## Constant      95.118***    68.417***      95.118***      68.417***    \n##                (5.323)      (3.223)        (5.664)        (3.527)     \n## ----------------------------------------------------------------------\n## District FE      Yes           No            Yes             No       \n## ----------------------------------------------------------------------\n## Observations    1,321        1,321          1,321          1,321      \n## ======================================================================\n## Note:                                      *p<0.1; **p<0.05; ***p<0.01"},{"path":"Panel.html","id":"DynPanel","chapter":"5 Panel regressions","heading":"5.5 Dynamic Panel Regressions","text":"precedes, assumed correlation observations indexed \\((,t)\\) indexed \\((j,s)\\) long \\(j \\ne \\) \\(t \\ne s\\). one suspects errors \\(\\varepsilon_{,t}\\) correlated (across entities \\(\\) given date \\(t\\), across dates given entity, ), one employ robust covariance matrix (see Section 4.5.5).several cases, auto-correlation variable interest may stem auto-regressive specification. , Eq. (5.1) replaced :\n\\[\\begin{equation}\ny_{,t} = \\rho y_{,t-1} + \\mathbf{x}'_{,t}\\underbrace{\\boldsymbol\\beta}_{K \\times 1} + \\underbrace{\\alpha_i}_{\\mbox{Individual effects}} + \\varepsilon_{,t}.\\tag{5.8}\n\\end{equation}\\]case, even explanatory variables \\(\\mathbf{x}_{,t}\\) uncorrelated errors \\(\\varepsilon_{,t}\\), additional explanatory variable \\(y_{,t-1}\\) correlates errors \\(\\varepsilon_{,t-1},\\varepsilon_{,t-2},\\dots,\\varepsilon_{,1}\\). result, LSDV estimate model parameters \\(\\{\\rho,\\boldsymbol\\beta\\}\\) may biased, even \\(n\\) large. see , notice LSDV regression amounts regressing \\(\\widetilde{\\mathbf{y}}\\) \\(\\widetilde{\\mathbf{X}}\\) (see Eq. (5.4)), elements \\(\\widetilde{\\mathbf{X}}\\) explanatory variables subtract within-sample means. particular, :\n\\[\n\\tilde{y}_{,t-1} = y_{,t-1} - \\frac{1}{T} \\sum_{s=1}^{T} y_{,s-1},\n\\]\ncorrelates corresponding error, :\n\\[\n\\tilde{\\varepsilon}_{,t} = \\varepsilon_{,t} - \\frac{1}{T} \\sum_{s=1}^{T} \\varepsilon_{,s}.\n\\]previous equation shows within-group estimator (LSDV) introduces realizations \\(\\varepsilon_{,t}\\) errors transformed error term (\\(\\tilde{\\varepsilon}_{,t}\\)). result, large-\\(n\\) fixed-\\(T\\) panels, consistent right-hand-side variables regression strictly exogenous (.e., correlate past, present, future errors \\(\\varepsilon_{,t}\\)).10 case lags \\(y_{,t}\\) right-hand side regression formula.following simulation illustrate bias. \\(x\\)-coordinates dots fixed effects \\(\\alpha_i\\)’s, \\(y\\)-coordinates LSDV estimates. blue line 45-degree line.\nFigure 5.5: illustration bias pertianing LSDV estimation approach presence auto-correlation depend variable.\nprevious example, estimate \\(\\rho\\) (whose true value 0.8) 0.531.address , one can resort instrumental-variable regressions. Anderson Hsiao (1982) , particular, proposed first-differenced Two Stage Least Squares (2SLS) estimator (see Eq. (4.22) Section 4.4). estimation based following transformation model:\n\\[\\begin{equation}\n\\Delta y_{,t} = \\rho \\Delta y_{,t-1} + (\\Delta \\mathbf{x}_{,t})'\\boldsymbol\\beta + \\Delta\\varepsilon_{,t}.\\tag{5.9}\n\\end{equation}\\]\nOLS estimates parameters biased \\(\\varepsilon_{,t-1}\\) —part error \\(\\Delta\\varepsilon_{,t}\\)— correlated \\(y_{,t-1}\\) —part “explanatory variable”, namely \\(\\Delta y_{,t-1}\\). consistent estimates can obtained using 2SLS instrumental variables correlated \\(\\Delta y_{,t}\\) orthogonal \\(\\Delta\\varepsilon_{,t}\\). One can instance use \\(\\{y_{,t-2},\\mathbf{x}_{,t-2}\\}\\) instruments. Note approach can implemented 3 time observations per entity \\(\\).explanatory variables \\(\\mathbf{x}_{,t}\\) assumed predetermined (.e., contemporaneous correlate errors \\(\\varepsilon_{,t}\\)), \\(\\mathbf{x}_{,t-1}\\) can added instruments associated \\(\\Delta y_{,t}\\). , variables (\\(\\mathbf{x}_{,t}\\)’s) exogenous (.e., contemporaneous correlate errors \\(\\varepsilon_{,s}\\), \\(\\forall s\\)), \\(\\mathbf{x}_{,t}\\) also constitute valid instrument.Using previous (simulated) example, approach consists following steps:OLS estimate \\(\\rho\\) (whose true value 0.8) 0.531, obtain rho_2SLS \\(=\\) 0.89.Let us come back general case (covariates \\(\\mathbf{x}_{,k}\\)’s). \\(t=3\\), \\(y_{,1}\\) (\\(\\mathbf{x}_{,1}\\)) possible instrument. However, \\(t=4\\), one use \\(y_{,2}\\) \\(y_{,1}\\) (well \\(\\mathbf{x}_{,2}\\) \\(\\mathbf{x}_{,1}\\)). generally, defining matrix \\(Z_i\\) follows:\n\\[\nZ_i = \\left[\n\\begin{array}{ccccccccccccccccc}\n\\mathbf{z}_{,1}' & 0 & \\dots \\\\\n0 & \\mathbf{z}_{,1}' & \\mathbf{z}_{,2}' & 0 & \\dots \\\\\n0 &0 &0 & \\mathbf{z}_{,1} & \\mathbf{z}_{,2}' & \\mathbf{z}_{,3}' & 0 & \\dots \\\\\n\\vdots \\\\\n0 & \\dots &&&&&& 0 & \\mathbf{z}_{,1}' &  \\dots &   \\mathbf{z}_{,T-2}'\n\\end{array}\n\\right],\n\\]\n\\(\\mathbf{z}_{,t} = [y_{,t},\\mathbf{x}_{,t}']'\\), moment conditions:11\n\\[\n\\mathbb{E}(Z_i'\\Delta  {\\boldsymbol\\varepsilon}_i)=0,\n\\]\\(\\Delta{\\boldsymbol\\varepsilon}_i = [ \\Delta \\varepsilon_{,3},\\dots,\\Delta \\varepsilon_{,T}]'\\).restrictions used GMM approach employed Arellano Bond (1991). Specifically, GMM estimator model parameters given :\n\\[\n\\mbox{argmin}\\;\\left(\\frac{1}{n} \\sum_{=1}^n Z_i' \\Delta \\boldsymbol\\varepsilon_i\\right)'W_n\\left(\\frac{1}{n} \\sum_{=1}^n Z_i' \\Delta \\boldsymbol\\varepsilon_i\\right),\n\\]\nusing weighting matrix\n\\[\nW_n = \\left(\\frac{1}{n}\\sum_{=1}^n Z_i'\\widehat{\\Delta\\boldsymbol\\varepsilon_i}\\widehat{\\Delta\\boldsymbol\\varepsilon_i}'Z_i\\right)^{-1},\n\\]\n\\(\\widehat{\\Delta\\boldsymbol\\varepsilon_i}\\)’s consistent estimates \\(\\Delta\\boldsymbol\\varepsilon_i\\)’s result preliminary estimation. sense, estimator two-step GMM one.disturbances homoskedastic, can shown asymptotically equivalent (efficient) GMM estimator can obtained using:\n\\[\nW_{1,n} = \\left(\\frac{1}{n}Z_i'HZ_i\\right)^{-1},\n\\]\n\\(H\\) \\((T-2) \\times (T-2)\\) matrix form:\n\\[\nH = \\left[\\begin{array}{ccccccc}\n2 & -1 & 0 & \\dots &0 \\\\\n-1 & 2 & -1 &  & \\vdots \\\\\n0 & \\ddots& \\ddots & \\ddots & 0 \\\\\n\\vdots &  & -1 & 2&-1\\\\\n0&\\dots & 0 & -1 & 2\n\\end{array}\\right].\n\\]straightforward extend GMM methods cases one lag dependent variable right-hand side equation cases disturbances feature limited moving-average serial correlation.pdynmc package allows run GMM approaches (see Fritsch, Pua, Schnurbus (2019)). following lines code allow replicate results Arellano Bond (1991):generate novel results (m2) replacing “onestep” “twostep” (estimation field). resulting estimated coefficients :Arellano Bond (1991) proposed specification test. model correctly specified, errors Eq. (5.9) —first-difference equation— feature non-zero first-order auto-correlations, zero higher-order autocorrelations.Function mtest.fct package pdynmc implements test. result present case:One can also implement Hansen J-test -identifying restrictions (see Section 6.1.3):","code":"\nn <- 400;T <- 10\nrho <- 0.8;sigma <- .5\nalpha <- rnorm(n)\ny <- alpha /(1-rho) + sigma^2/(1 - rho^2) * rnorm(n)\nall_y <- y\nfor(t in 2:T){\n  y <- rho * y + alpha + sigma * rnorm(n)\n  all_y <- rbind(all_y,y)\n}\ny   <- c(all_y[2:T,]);y_1 <- c(all_y[1:(T-1),])\nD <- diag(n) %x% rep(1,T-1)\nZ <- cbind(c(y_1),D)\nb <- solve(t(Z) %*% Z) %*% t(Z) %*% y\na <- b[2:(n+1)]\nplot(alpha,a)\nlines(c(-10,10),c(-10,10),col=\"blue\")\nDy   <- c(all_y[3:T,]) - c(all_y[2:(T-1),])\nDy_1 <- c(all_y[2:(T-1),]) - c(all_y[1:(T-2),])\ny_2  <- c(all_y[1:(T-2),])\nZ <- matrix(y_2,ncol=1)\nPz <- Z %*% solve(t(Z) %*% Z) %*% t(Z)\nDy_1hat <- Pz %*% Dy_1\nrho_2SLS <- solve(t(Dy_1hat) %*% Dy_1hat) %*% t(Dy_1hat) %*% Dy\nlibrary(pdynmc)\ndata(EmplUK, package = \"plm\")\ndat <- EmplUK\ndat[,c(4:7)]         <- log(dat[,c(4:7)])\nm1 <- pdynmc(dat = dat, # name of the dataset\n             varname.i = \"firm\", # name of the cross-section identifier\n             varname.t = \"year\", # name of the time-series identifiers\n             use.mc.diff = TRUE, # use moment conditions from equations in differences? (i.e. instruments in levels) \n             use.mc.lev = FALSE, # use moment conditions from equations in levels? (i.e. instruments in differences)\n             use.mc.nonlin = FALSE, # use nonlinear (quadratic) moment conditions?\n             include.y = TRUE, # instruments should be derived from the lags of the dependent variable?\n             varname.y = \"emp\", # name of the dependent variable in the dataset\n             lagTerms.y = 2, # number of lags of the dependent variable\n             fur.con = TRUE, # further control variables (covariates) are included?\n             fur.con.diff = TRUE, # include further control variables in equations from differences ?\n             fur.con.lev = FALSE, # include further control variables in equations from level?\n             varname.reg.fur = c(\"wage\", \"capital\", \"output\"), # covariate(s) -in the dataset- to treat as further controls\n             lagTerms.reg.fur = c(1,2,2), # number of lags of the further controls\n             include.dum = TRUE, # A logical variable indicating whether dummy variables for the time periods are included (defaults to 'FALSE').\n             dum.diff = TRUE, # A logical variable indicating whether dummy variables are included in the equations in first differences (defaults to 'NULL').\n             dum.lev = FALSE, # A logical variable indicating whether dummy variables are included in the equations in levels (defaults to 'NULL').\n             varname.dum = \"year\",\n             w.mat = \"iid.err\", # One of the character strings c('\"iid.err\"', '\"identity\"', '\"zero.cov\"') indicating the type of weighting matrix to use (defaults to '\"iid.err\"')\n             std.err = \"corrected\",\n             estimation = \"onestep\", # One of the character strings c('\"onestep\"', '\"twostep\"', '\"iterative\"'). Denotes the number of iterations of the parameter procedure (defaults to '\"twostep\"').\n             opt.meth = \"none\" # numerical optimization procedure. When no nonlinear moment conditions are employed in estimation, closed form estimates can be computed by setting the argument to '\"none\"\n)\nsummary(m1,digits=3)## \n## Dynamic linear panel estimation (onestep)\n## Estimation steps: 1\n## \n## Coefficients:\n##             Estimate Std.Err.rob z-value.rob Pr(>|z.rob|)    \n## L1.emp      0.686226    0.144594       4.746      < 2e-16 ***\n## L2.emp     -0.085358    0.056016      -1.524      0.12751    \n## L0.wage    -0.607821    0.178205      -3.411      0.00065 ***\n## L1.wage     0.392623    0.167993       2.337      0.01944 *  \n## L0.capital  0.356846    0.059020       6.046      < 2e-16 ***\n## L1.capital -0.058001    0.073180      -0.793      0.42778    \n## L2.capital -0.019948    0.032713      -0.610      0.54186    \n## L0.output   0.608506    0.172531       3.527      0.00042 ***\n## L1.output  -0.711164    0.231716      -3.069      0.00215 ** \n## L2.output   0.105798    0.141202       0.749      0.45386    \n## 1979        0.009554    0.010290       0.929      0.35289    \n## 1980        0.022015    0.017710       1.243      0.21387    \n## 1981       -0.011775    0.029508      -0.399      0.68989    \n## 1982       -0.027059    0.029275      -0.924      0.35549    \n## 1983       -0.021321    0.030460      -0.700      0.48393    \n## 1976       -0.007703    0.031411      -0.245      0.80646    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  41 total instruments are employed to estimate 16 parameters\n##  27 linear (DIF) \n##  8 further controls (DIF) \n##  6 time dummies (DIF) \n##  \n## J-Test (overid restrictions):  70.82 with 25 DF, pvalue: <0.001\n## F-Statistic (slope coeff):  528.06 with 10 DF, pvalue: <0.001\n## F-Statistic (time dummies):  14.98 with 6 DF, pvalue: 0.0204##      L1.emp      L2.emp     L0.wage     L1.wage  L0.capital  L1.capital \n##  0.62870890 -0.06518800 -0.52575951  0.31128961  0.27836190  0.01409950 \n##  L2.capital   L0.output   L1.output   L2.output        1979        1980 \n## -0.04024847  0.59192286 -0.56598515  0.10054264  0.01121551  0.02306871 \n##        1981        1982        1983        1976 \n## -0.02135806 -0.03111604 -0.01799335 -0.02336762\nmtest.fct(m1,order=3)## \n##  Arellano and Bond (1991) serial correlation test of degree 3\n## \n## data:  1step GMM Estimation\n## normal = 0.045945, p-value = 0.9634\n## alternative hypothesis: serial correlation of order 3 in the error terms\njtest.fct(m1)## \n##  J-Test of Hansen\n## \n## data:  1step GMM Estimation\n## chisq = 70.82, df = 25, p-value = 2.905e-06\n## alternative hypothesis: overidentifying restrictions invalid\njtest.fct(m2)## \n##  J-Test of Hansen\n## \n## data:  2step GMM Estimation\n## chisq = 31.381, df = 25, p-value = 0.1767\n## alternative hypothesis: overidentifying restrictions invalid"},{"path":"Panel.html","id":"introduction-to-program-evaluation","chapter":"5 Panel regressions","heading":"5.6 Introduction to program evaluation","text":"section brielfy introduces econometrics program evaluation. Program evaluation refer analysis causal effects “treatments” broad sense. treatment can, e.g., correspond implementation (announcement) policy measures. comprehensive review proposed Abadie Cattaneo (2018). seminal book subject Angrist Pischke (2008).","code":""},{"path":"Panel.html","id":"presentation-of-the-problem","chapter":"5 Panel regressions","heading":"5.6.1 Presentation of the problem","text":"begin , let us consider single entity. simplify notations, drop entity index (\\(\\)). Let us denote \\(Y\\) outcome variable (variable interest), \\(W\\) binary variable indicating whether considered entity received treatment (\\(W=1\\)) (\\(W=0\\)), \\(X\\) vector covariates, assumed predetermined relative treatment. , \\(W\\) \\(X\\) correlated, values \\(X\\) determined \\(W\\) (way realization \\(W\\) affect \\(X\\)). Typcally, \\(X\\) contains characteristics considered entity.interested effect treatment, :\n\\[\nY_1 - Y_0,\n\\]\n\\(Y_1\\) correspond outcome obtained treatment, \\(Y_0\\) outcome obtained without . Notice :\n\\[\nY = (1-W) Y_0 + W Y_1.\n\\]problem observing \\((Y,W,X)\\) sufficient observe treatment effect \\(Y_1 - Y_0\\). Additional assumptions needed estimate , , precisely, expectations (average treatment effect):\n\\[\nATE = \\mathbb{E}(Y_1 - Y_0).\n\\]Importantly, \\(ATE\\) different following quantity:\n\\[\n\\alpha = \\underbrace{\\mathbb{E}(Y|W=1)}_{=\\mathbb{E}(Y_1|W=1)} - \\underbrace{\\mathbb{E}(Y|W=0)}_{=\\mathbb{E}(Y_0|W=0)},\n\\]\neasier estimate. Indeed, consistent estimate \\(\\alpha\\) difference means outcome variables two sub-samples: one containing treated entities (gives estimate \\(\\mathbb{E}(Y_1|W=1)\\)) containing non-treated entities (gives estimate \\(\\mathbb{E}(Y_0|W=0)\\)). Coming back \\(ATE\\), problem won’t direct information regarding \\(\\mathbb{E}(Y_0|W=1)\\) \\(\\mathbb{E}(Y_1|W=0)\\). However, two conditional expectations part \\(ATE\\). Indeed, \\(ATE = \\mathbb{E}(Y_1) - \\mathbb{E}(Y_0)\\), :\n\\[\\begin{eqnarray}\n\\mathbb{E}(Y_1) &=& \\mathbb{E}(Y_1|W=0)\\mathbb{P}(W=0)+\\mathbb{E}(Y_1|W=1)\\mathbb{P}(W=1) \\tag{5.10} \\\\\n\\mathbb{E}(Y_0) &=& \\mathbb{E}(Y_0|W=0)\\mathbb{P}(W=0)+\\mathbb{E}(Y_0|W=1)\\mathbb{P}(W=1). \\tag{5.11}\n\\end{eqnarray}\\]","code":""},{"path":"Panel.html","id":"randomized-controlled-trials-rcts","chapter":"5 Panel regressions","heading":"5.6.2 Randomized controlled trials (RCTs)","text":"context Randomized controlled trials (RCTs), entities randomly assigned receive treatment. result, \\(\\mathbb{E}(Y_1) = \\mathbb{E}(Y_1|W=0) = \\mathbb{E}(Y_1|W=1)\\) \\(\\mathbb{E}(Y_0) = \\mathbb{E}(Y_0|W=0) = \\mathbb{E}(Y_0|W=1)\\). Using Eqs. (5.10) (5.11) yields \\(ATE = \\alpha\\).Therefore, context, estimating \\(\\mathbb{E}(Y_1-Y_0)\\) amounts computing difference two sample means, namely () sample mean subset \\(Y_i\\)’s corresponding entities \\(W_i=1\\), (b) one \\(W_i=0\\).accurate estimates can obtained regressions. Assume model reads:\n\\[\nY_{} = W_{} \\beta_{1} + X_i'\\boldsymbol\\beta_z + \\varepsilon_i,\n\\]\n\\(\\mathbb{E}(\\varepsilon_i|X_i) = 0\\) (\\(W_i\\) independent \\(X_i\\) \\(\\varepsilon_i\\)). case, obtain consistent estimate \\(\\beta_1\\) regressing \\(\\mathbf{y}\\) \\(\\mathbf{Z} = [\\mathbf{w},\\mathbf{X}]\\).","code":""},{"path":"Panel.html","id":"difference-in-difference-did-approach","chapter":"5 Panel regressions","heading":"5.6.3 Difference-in-Difference (DiD) approach","text":"approach popular methodology implemented cases \\(W\\) considered independent variable. exploits two dimensions: entities (\\(\\)), time (\\(t\\)). simplify exposition, consider two periods (\\(t=0\\) \\(t=1\\)).Consider following model:\n\\[\\begin{equation}\nY_{,t} = W_{,t} \\beta_1 + \\mu_i + \\delta_t + \\varepsilon_{,t}\\tag{5.12}\n\\end{equation}\\]parameter interest \\(\\beta_{1}\\), treatment effect (recall \\(W_{,t} \\\\{0,1\\}\\)). Usually, entities \\(\\), \\(W_{,t=0}=0\\). treated date 1, .e., \\(W_{,1} \\\\{0,1\\}\\).disturbance \\(\\varepsilon_{,t}\\) affects outcome, assume relate selection treatment; therefore, \\(\\mathbb{E}(\\varepsilon_{,t}|W_{,t})=0\\). contrast, exclude correlation \\(W_{,t}\\) (\\(t=1\\)) \\(\\mu_i\\); hence, \\(\\mu_i\\) may constitute confounder. Finally, suppose micro-variables \\(W_i\\) affect time fixed effects \\(\\delta_t\\), \\(\\mathbb{E}(\\delta_t|W_{,t})=\\mathbb{E}(\\delta_t)\\).:\\[\n\\begin{array}{cccccccccc}\n\\mathbb{E}(Y_{,1}|W_{,1}=1) &=& \\beta_1 &+& \\mathbb{E}(\\mu_i|W_{,1}=1) &+&\\mathbb{E}(\\delta_1|W_{,1}=1) &+& \\mathbb{E}(\\varepsilon_{,1}) \\\\\n\\mathbb{E}(Y_{,0}|W_{,1}=1) &=&  && \\mathbb{E}(\\mu_i|W_{,1}=1) &+&\\mathbb{E}(\\delta_0|W_{,1}=1) &+& \\mathbb{E}(\\varepsilon_{,0}) \\\\\n\\mathbb{E}(Y_{,1}|W_{,1}=0) &=& && \\mathbb{E}(\\mu_i|W_{,1}=0) &+&\\mathbb{E}(\\delta_1|W_{,1}=0) &+& \\mathbb{E}(\\varepsilon_{,1}) \\\\\n\\mathbb{E}(Y_{,0}|W_{,1}=0) &=&  && \\mathbb{E}(\\mu_i|W_{,1}=0) &+&\\mathbb{E}(\\delta_0|W_{,1}=0) &+& \\mathbb{E}(\\varepsilon_{,0}).\n\\end{array}\n\\]\n, assumptions, can checked :\\[\n\\beta_1 = \\mathbb{E}(\\Delta Y_{,1}|W_{,1}=1) - \\mathbb{E}(\\Delta Y_{,1}|W_{,1}=0),\n\\]\n\\(\\Delta Y_{,1}=Y_{,1}-Y_{,0}\\). Therefore, context, treatment effect appears difference (two conditionnal expectations) difference (outcome variable, time).illustrated Figure 5.6, represents generic framework.\nFigure 5.6: Source: Abadie et al., (1998).\npractice, implementing approach consists running linear regression type Eq. (5.12). regressions also usually involve controls top fixed effects \\(\\mu_i\\). illustrated next subsection, parameter interest (\\(\\beta_1\\)) often associated interaction term.","code":""},{"path":"Panel.html","id":"application-of-the-did-approach","chapter":"5 Panel regressions","heading":"5.6.4 Application of the DiD approach","text":"example based data used Meyer, Viscusi, Durbin (1995). dataset part wooldridge package. paper examines effect workers’ compensation injury time work. exploits natural experiment approach comparing individuals injured increases maximum weekly benefit amount. Specifically, 1980, cap weekly earnings covered worker’s compensation increased Kentucky Michigan. Let us check whether new policy followed increase amount time workers spent unemployed (example, higher compensation may reduce workers’ incentives avoid injury).shown Figure 5.7, measure affected high-earning workers. idea exploited Meyer, Viscusi, Durbin (1995) compare increase time work -1980 higher-earnings workers one hand (entities received treatment) low-earnings workers hand (control group).\nFigure 5.7: Source: Meyer et al., (1995).\nnext lines codes replicate results. dependent variable logarithm duration benefits. information use ?injury, loaded wooldridge library.table results , parameter interest one associated interaction term afchnge:highearn. Columns 2 3 correspond first two column Table 6 Meyer, Viscusi, Durbin (1995).","code":"\nlibrary(wooldridge)\ndata(injury)\ninjury <- subset(injury,ky==1)\ninjury$indust <- as.factor(injury$indust)\ninjury$injtype <- as.factor(injury$injtype)\n#names(injury)\neq1 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn,data=injury)\neq2 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn +\n            lprewage*highearn + male + married + lage + ltotmed + hosp +\n            indust + injtype,data=injury)\neq3 <- lm(log(durat) ~ afchnge + highearn + afchnge*highearn +\n            lprewage*highearn + male + married + lage + indust +\n            injtype,data=injury)\nstargazer::stargazer(eq1,eq2,eq3,type=\"text\",\n                     omit=c(\"indust\",\"injtype\",\"Constant\"),no.space = TRUE,\n                     add.lines = list(c(\"industry dummy\",\"no\",\"yes\",\"yes\"),\n                                      c(\"injury dummy\",\"no\",\"yes\",\"yes\")),\n                     order = c(1,2,18,3:17,19,20),omit.stat = c(\"f\",\"ser\"))## \n## ===============================================\n##                        Dependent variable:     \n##                   -----------------------------\n##                            log(durat)          \n##                      (1)       (2)       (3)   \n## -----------------------------------------------\n## afchnge             0.008    -0.004     0.016  \n##                    (0.045)   (0.038)   (0.045) \n## highearn          0.256***   -0.595    -1.522  \n##                    (0.047)   (0.930)   (1.099) \n## afchnge:highearn  0.191***  0.162***  0.215*** \n##                    (0.069)   (0.059)   (0.069) \n## lprewage                     0.207**   0.258** \n##                              (0.088)   (0.104) \n## male                         -0.070*   -0.072  \n##                              (0.039)   (0.046) \n## married                       0.055     0.051  \n##                              (0.035)   (0.041) \n## lage                        0.244***  0.252*** \n##                              (0.044)   (0.052) \n## ltotmed                     0.361***           \n##                              (0.011)           \n## hosp                        0.252***           \n##                              (0.044)           \n## highearn:lprewage             0.065     0.232  \n##                              (0.158)   (0.187) \n## -----------------------------------------------\n## industry dummy       no        yes       yes   \n## injury dummy         no        yes       yes   \n## Observations        5,626     5,347     5,347  \n## R2                  0.021     0.319     0.049  \n## Adjusted R2         0.020     0.316     0.046  \n## ===============================================\n## Note:               *p<0.1; **p<0.05; ***p<0.01"},{"path":"estimation-methods.html","id":"estimation-methods","chapter":"6 Estimation Methods","heading":"6 Estimation Methods","text":"chapter presents three approaches estimate parametric models: General Method Moments (GMM), Maximum Likelihood approach (ML), Bayesian approach. general context following: observe sample \\(\\mathbf{y}=\\{y_1,\\dots,y_n\\}\\), assume data generated model parameterized \\({\\boldsymbol\\theta} \\\\mathbb{R}^K\\), want estimate vector \\({\\boldsymbol\\theta}_0\\).","code":""},{"path":"estimation-methods.html","id":"secGMM","chapter":"6 Estimation Methods","heading":"6.1 Generalized Method of Moments (GMM)","text":"","code":""},{"path":"estimation-methods.html","id":"definition-of-the-gmm-estimator","chapter":"6 Estimation Methods","heading":"6.1.1 Definition of the GMM estimator","text":"denote \\(y_i\\) \\(p \\times 1\\) vector variables; \\(\\boldsymbol\\theta\\) \\(K \\times 1\\) vector parameters, \\(h(y_i;\\boldsymbol\\theta)\\) continuous \\(r \\times 1\\) vector-valued function.denote \\(\\boldsymbol\\theta_0\\) true value \\(\\boldsymbol\\theta\\) assume \\(\\boldsymbol\\theta_0\\) satisfies:\n\\[\n\\mathbb{E}[h(y_i;\\boldsymbol\\theta_0)] = \\mathbf{0}.\n\\]denote \\(\\underline{y_i}\\) information contained current past observations \\(y_i\\), : \\(\\underline{y_i} = \\{y_i,y_{-1},\\dots,y_1\\}\\). denote \\(g(\\underline{y_n};\\boldsymbol\\theta)\\) sample average \\(h(y_i;\\boldsymbol\\theta)\\) vectors, .e.:\n\\[\ng(\\underline{y_n};\\boldsymbol\\theta) = \\frac{1}{n} \\sum_{=1}^{n} h(y_i;\\boldsymbol\\theta).\n\\]intuition behind GMM estimator following: choose \\(\\boldsymbol\\theta\\) make sample moment close possible population values, 0.Definition 6.1  GMM estimator \\(\\boldsymbol\\theta_0\\) given :\n\\[\n\\hat{\\boldsymbol\\theta}_n = \\mbox{argmin}_{\\boldsymbol\\theta} \\quad g(\\underline{y_n};\\boldsymbol\\theta)'\\, W_n \\, g(\\underline{y_n};\\boldsymbol\\theta),\n\\]\n\\(W_n\\) positive definite matrix (may depend \\(\\underline{y_n}\\)).specific case \\(K = r\\) (dimension \\(\\boldsymbol\\theta\\) \\(h(y_i;\\boldsymbol\\theta)\\) —\\(g(\\underline{y_n};\\boldsymbol\\theta)\\)— \\(\\hat{\\boldsymbol\\theta}_n\\) satisfies:\n\\[\ng(\\underline{y_n};\\hat{\\boldsymbol\\theta}_n) = \\mathbf{0}.\n\\]\nregularity identification conditions, estimator consistent, \\(\\hat{\\boldsymbol\\theta}_{n}\\) converges towards \\(\\boldsymbol\\theta_0\\) probability, denote :\n\\[\\begin{equation}\n\\mbox{plim}_n\\;\\hat{\\boldsymbol\\theta}_{n}= \\boldsymbol\\theta_0,\\quad \\mbox{} \\quad\\hat{\\boldsymbol\\theta}_{n} \\overset{p}{\\rightarrow} \\boldsymbol\\theta_0,\\tag{6.1}\n\\end{equation}\\]\n.e. \\(\\forall \\varepsilon>0\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_0|>\\varepsilon) = 0\\) (Definition 9.14).Definition 6.1 involves positive definite matrix \\(W_n\\). one can take positive definite matrix consistency (Eq. (6.1)), can shown GMM estimator achieves minimum asymptotic variance \\(W_n\\) inverse matrix \\(S\\), latter defined :\n\\[\nS = Asy.\\mathbb{V}ar\\left(\\sqrt{n}g(\\underline{y_n};\\hat{\\boldsymbol\\theta}_n)\\right).\n\\]\ncase, \\(W_n\\) said optimal weighting matrix.intuition behind result underlies Generalized Least Squares (see Section 4.5.2), : beneficial use criterion weights inversely proportional variances moments.\\(h(x_i;\\boldsymbol\\theta_0)\\) correlated \\(h(x_j;\\boldsymbol\\theta_0)\\), \\(\\ne j\\), :\n\\[\nS = \\mathbb{V}ar(h(x_i;\\boldsymbol\\theta_0)),\n\\]\ncan approximated \n\\[\n\\hat{\\Gamma}_{0,n}=\\frac{1}{n}\\sum_{=1}^{n} h(x_i;\\hat{\\boldsymbol\\theta}_n)h(x_{};\\hat{\\boldsymbol\\theta}_n)'.\n\\]time series context, often correlation \\(x_i\\) \\(x_{+k}\\), especially small \\(k\\)’s. case, time series \\(\\{y_i\\}\\) covariance stationary (see Def. 8.4), :\n\\[\nS := \\sum_{\\nu = -\\infty}^{\\infty} \\Gamma_\\nu,\n\\]\n\\(\\Gamma_\\nu := \\mathbb{E}[h(x_i;\\boldsymbol\\theta_0) h(x_{-\\nu};\\boldsymbol\\theta_0)']\\). Matrix \\(S\\) called long-run variance process \\(\\{y_i\\}\\) (see Def. 8.9).\\(\\nu \\ge 0\\), let us define \\(\\hat{\\Gamma}_{\\nu,n}\\) :\n\\[\n\\hat{\\Gamma}_{\\nu,n} = \\frac{1}{n} \\sum_{=\\nu + 1}^{n} h(x_i;\\hat{\\boldsymbol\\theta}_n)h(x_{-\\nu};\\hat{\\boldsymbol\\theta}_n)',\n\\]\n\\(S\\) can approximated Newey West (1987) formula (similar Eq. (8.6)):\n\\[\\begin{equation}\n\\hat{\\Gamma}_{0,n} + \\sum_{\\nu=1}^{q}\\left[1-\\frac{\\nu}{q+1}\\right](\\hat{\\Gamma}_{\\nu,n}+\\hat{\\Gamma}_{\\nu,n}').    \\tag{6.2}\n\\end{equation}\\]","code":""},{"path":"estimation-methods.html","id":"asymptotic-distribution-of-the-gmm-estimator","chapter":"6 Estimation Methods","heading":"6.1.2 Asymptotic distribution of the GMM estimator","text":":\n\\[\\begin{equation}\n\\boxed{\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0) \\overset{d}{\\rightarrow} \\mathcal{N}(0,V),}\\tag{6.3}\n\\end{equation}\\]\n\\(V = (DS^{-1}D')^{-1}\\), \n\\[\nD := \\left.\\mathbb{E}\\left(\\frac{\\partial h(x_i;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right)\\right|_{\\boldsymbol\\theta = \\boldsymbol\\theta_0}.\n\\]Matrix \\(V\\) can approximated \n\\[\\begin{equation}\n\\hat{V}_n = (\\hat{D}_n\\hat{S}_n^{-1}\\hat{D}_n')^{-1},\\tag{6.4}\n\\end{equation}\\]\n\\(\\hat{S}_n\\) given Eq. (6.2) \n\\[\n\\hat{D}'_n := \\left.\\frac{\\partial g(\\underline{y_n};\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}_n}.\n\\]\npractice, previous matrix computed numerically.","code":""},{"path":"estimation-methods.html","id":"overidentif","chapter":"6 Estimation Methods","heading":"6.1.3 Testing hypotheses in the GMM framework","text":"first important test one concerning validity moment restrictions (Sargan-Hansen test; Sargan (1958) Hansen (1982)). Assume number restrictions imposed larger number parameters estimate (\\(r>K\\)). case, restrictions said -identifiying.correct specification, asymptotically :\n\\[\n\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)  \\sim \\mathcal{N}(0,S).\n\\]\nresult, comes :\n\\[\\begin{equation}\nJ_n = \\left(\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)\\right)'S^{-1}\\left(\\sqrt{n}g(\\underline{y_n};{\\boldsymbol\\theta}_0)\\right) \\tag{6.5}\n\\end{equation}\\]\nasymptotically follows \\(\\chi^2\\) distribution. number degrees freedom equal \\(r-K\\). (Note , \\(r=K\\), , expected, \\(J=0\\).) , asymptotically:\n\\[\nJ_n \\sim \\chi^2(r-K).\n\\]\nGMM framework also allows easily test linear restrictions parameters. First, given Eq. (6.3), Wald tests (see Eq. (4.13) Section 4.2.5) readily available. Second, one can also resort test equivalent likelihood ratio tests (see Definition 6.8). precisely, consider unconstrained model constrained version model, number restrictions equal \\(k\\). two models estimated considering moment constraints, weighting matrix —using Eq. (6.4), based unrestricted model—, :\n\\[\nn \\left[(g(\\underline{y_n};\\hat{{\\boldsymbol\\theta}}^*_n)-g(\\underline{y_n};\\hat{{\\boldsymbol\\theta}}_n)\\right] \\sim \\chi^2(k),\n\\]\n\\(\\hat{{\\boldsymbol\\theta}}^*_n\\) constrained estimate \\({\\boldsymbol\\theta}_0\\).","code":""},{"path":"estimation-methods.html","id":"example-estimation-of-the-stochastic-discount-factor-s.d.f.","chapter":"6 Estimation Methods","heading":"6.1.4 Example: Estimation of the Stochastic Discount Factor (s.d.f.)","text":"-arbitrage assumption, exists random variable \\(\\mathcal{M}_{t,t+1}\\) (s.d.f.) \n\\[\n\\mathbb{E}_t(\\mathcal{M}_{t,t+1}R_{t+1})=1\n\\]\n(gross) asset return \\(R_t\\). following, \\(R_t\\) denotes \\(n_r\\)-dimensional vector gross returns.consider following specification s.d.f.:\n\\[\\begin{equation}\n\\mathcal{M}_{t,t+1} = 1 - \\textbf{b}_M'(F_{t+1} - \\mathbb{E}_t(F_{t+1})), \\tag{6.6}\n\\end{equation}\\]\n\\(F_t\\) vector factors. Eq. (6.6) reads:\n\\[\n\\mathbb{E}_t([1 - \\textbf{b}_M'(F_{t+1} - \\mathbb{E}_t(F_{t+1}))]R_{t+1})=1.\n\\]Assume date-\\(t\\) information set \\(\\mathcal{}_t=\\{\\textbf{z}_t,\\mathcal{}_{t-1}\\}\\), \\(\\textbf{z}_t\\) vector variables observed date \\(t\\). (\\(\\mathbb{E}_t(\\bullet) \\equiv \\mathbb{E}(\\bullet|\\mathcal{}_t)\\).)can use \\(\\textbf{z}_t\\) instrument. Indeed, :\n\\[\\begin{eqnarray}\n&&\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1]) \\nonumber \\\\\n&=&\\mathbb{E}(\\mathbb{E}_t\\{z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1]\\})\\nonumber\\\\\n&=&\\mathbb{E}(z_{,t} \\underbrace{\\mathbb{E}_t\\{\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1\\}}_{1 - \\mathbb{E}_t(\\mathcal{M}_{t,t+1}R_{t+1})=0})=0.\\tag{6.7}\n\\end{eqnarray}\\]\nconverted conditional moment condition unconditional one (need implement GMM approach described ). However, stage, can still directly use GMM formulas conditional expectation \\(\\mathbb{E}_t(F_{t+1})\\) appears \\(\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1])=0\\).go , let us assume :\n\\[\n\\mathbb{E}_t(F_{t+1}) = \\textbf{b}_F \\textbf{z}_t.\n\\]\ncan easily estimate matrix \\(\\textbf{b}_F\\) (dimension \\(n_F \\times n_z\\)) OLS. Note OLS can seen special GMM case. Indeed, done Eq. (6.7), can show , \\(j^{th}\\) component \\(F_t\\), :\n\\[\n\\mathbb{E}( [F_{j,t+1} - \\textbf{b}_{F,j} \\textbf{z}_t]\\textbf{z}_{t})=0,\n\\]\n\\(\\textbf{b}_{F,j}\\) denotes \\(j^{th}\\) row \\(\\textbf{b}_{F}\\). yields OLS formula.Equipped \\(\\textbf{b}_F\\), rely following moment restrictions estimate \\(\\textbf{b}_M\\):\n\\[\n\\mathbb{E}(z_{,t} [\\textbf{b}_M'\\{F_{t+1} - \\textbf{b}_F \\textbf{z}_t\\}R_{t+1}-R_{t+1}+1])=0.\n\\]\nSpecifically, number restrictions \\(n_R \\times n_z\\). Let us implement approach U.S. context, using data extracted FRED database. factor \\(F_t\\), use changes VIX personal consumption expenditures. returns (\\(R_t\\)) based Wilshire 5000 Price Index (stock price index) ICE BofA BBB US Corporate Index Total Return Index (bond return index).Define matrices containing \\(F_{t+1}\\), \\(\\textbf{z}_t\\), \\(R_{t+1}\\) vectors:Function f_aux compute \\(h(x_t;{\\boldsymbol\\theta})\\) \\(g(\\underline{y_T};{\\boldsymbol\\theta})\\); function f2beMin function minimized.Now, let’s minimize function, using use BFGS numerical algorithm (part optim wrapper). run 5 iterations (\\(W\\) updated).Finally, let’s compute standard deviation parameter estimates, using Eq. (6.4):Hansen statistic can used test model (see Eq. (6.5)). model correct, :\n\\[\nT g(\\underline{y_T};{\\boldsymbol\\theta})'\\, S^{-1} \\, g(\\underline{y_T};{\\boldsymbol\\theta}) \\sim \\,..d.\\,\\chi^2(J - K),\n\\]\n\\(J\\) number moment constraints (\\(n_z \\times n_r\\) ) \\(K\\) number estimated parameters (\\(=n_F\\) ).","code":"\nlibrary(fredr)\nfredr_set_key(\"df65e14c054697a52b4511e77fcfa1f3\")\nstart_date <- as.Date(\"1990-01-01\"); end_date <- as.Date(\"2022-01-01\")\nf <- function(ticker){\n  fredr(series_id = ticker,\n        observation_start = start_date,observation_end = end_date,\n        frequency = \"m\",aggregation_method = \"avg\")\n}\nvix <- f(\"VIXCLS\") # VIX\npce <- f(\"PCE\") # Personal consumption expenditures\nsto <- f(\"WILL5000PRFC\") # Wilshire 5000 Full Cap Price Index\nbdr <- f(\"BAMLCC0A4BBBTRIV\") # ICE BofA BBB US Corp. Index Tot. Return\nT <- dim(vix)[1]\ndvix <- c(vix$value[3:T]/vix$value[2:(T-1)]) # change in VIX t+1\ndpce <- c(pce$value[3:T]/pce$value[2:(T-1)]) # change in PCE t+1\ndsto <- c(sto$value[3:T]/sto$value[2:(T-1)]) # return t+1\ndbdr <- c(bdr$value[3:T]/bdr$value[2:(T-1)]) # return t+1\ndvix_1 <- c(vix$value[2:(T-1)]/vix$value[1:(T-2)]) # change in VIX t\ndpce_1 <- c(pce$value[2:(T-1)]/pce$value[1:(T-2)]) # change in PCE t\ndsto_1 <- c(sto$value[2:(T-1)]/sto$value[1:(T-2)]) # return t\ndbdr_1 <- c(bdr$value[2:(T-1)]/bdr$value[1:(T-2)]) # return t\nF_tp1 <- cbind(dvix,dpce)\nZ     <- cbind(1,dvix_1,dpce_1,dsto_1,dbdr_1)\nb_F <- t(solve(t(Z) %*% Z) %*% t(Z) %*% F_tp1)\nF_innov <- F_tp1 - Z %*% t(b_F)\nR_tp1 <- cbind(dsto,dbdr)\nn_F <- dim(F_tp1)[2]; n_R <- dim(R_tp1)[2]; n_z <- dim(Z)[2]\nf_aux <- function(theta){\n  b_M <- matrix(theta[1:n_F],ncol=1)\n  R_aux <- matrix(F_innov %*% b_M,T-2,n_R) * R_tp1 - R_tp1 + 1\n  H <- (R_aux %x% matrix(1,1,n_z)) * (matrix(1,1,n_R) %x% Z)\n  g <- matrix(apply(H,2,mean),ncol=1)\n  return(list(g=g,H=H))\n}\nf2beMin <- function(theta,W){# function to be minimized\n  res <- f_aux(theta)\n  return(t(res$g) %*% W %*% res$g)\n}\nlibrary(AEC)\ntheta <- c(rep(0,n_F)) # inital value\nfor(i in 1:10){# recursion on W\n  res <- f_aux(theta)\n  W <-  solve(NW.LongRunVariance(res$H,q=6))\n  res.optim <- optim(theta,f2beMin,W=W,\n                     method=\"BFGS\", # could be \"Nelder-Mead\"\n                     control=list(trace=FALSE,maxit=200),hessian=TRUE)\n  theta <- res.optim$par\n}\neps <- .0001\ng0 <- f_aux(theta)$g\nD <- NULL\nfor(i in 1:length(theta)){\n  theta.i <- theta\n  theta.i[i] <- theta.i[i] + eps\n  gi <- f_aux(theta.i)$g\n  D <- cbind(D,(gi-g0)/eps)\n}\nV <- 1/T * solve(t(D) %*% W %*% D)\nstd.dev <- sqrt(diag(V));t.stud <- theta/std.dev\ncbind(theta,std.dev,t.stud)##            theta    std.dev     t.stud\n## [1,]  -0.7180716  0.4646617 -1.5453642\n## [2,] -11.2042452 17.1039449 -0.6550679\ng <- f_aux(theta)$g\nHansen_stat <- T * t(g) %*% W %*% g\npvalue <- pchisq(q = Hansen_stat,df = n_R*n_z - n_F)\npvalue##           [,1]\n## [1,] 0.8789782"},{"path":"estimation-methods.html","id":"secMLE","chapter":"6 Estimation Methods","heading":"6.2 Maximum Likelihood Estimation","text":"","code":""},{"path":"estimation-methods.html","id":"intuition","chapter":"6 Estimation Methods","heading":"6.2.1 Intuition","text":"Intuitively, Maximum Likelihood Estimation (MLE) consists looking value \\({\\boldsymbol\\theta}\\) probability observed \\(\\mathbf{y}\\) (sample hand) highest possible.set example, assume time periods arrivals two customers shop, denoted \\(y_i\\), ..d. follow exponential distribution, .e. \\(y_i \\sim \\,..d.\\, \\mathcal{E}(\\lambda)\\). observed arrivals time, thereby constituting sample \\(\\mathbf{y}=\\{y_1,\\dots,y_n\\}\\). want estimate \\(\\lambda\\) (.e. case, vector parameters simply \\({\\boldsymbol\\theta} = \\lambda\\)).density \\(Y\\) (one observation) \\(f(y;\\lambda) = \\dfrac{1}{\\lambda}\\exp(-y/\\lambda)\\). Fig. 6.1 represents density functions different values \\(\\lambda\\).200 observations reported bottom Fig. 6.1 (red bars). build histogram display chart.\nFigure 6.1: red ticks, bottom, indicate observations (200 ). historgram based 200 observations\nestimate \\(\\lambda\\)? Intuitively, one led take \\(\\lambda\\) (theoretical) distribution closest histogram (can seen “empirical distribution”). approach consistent idea picking \\(\\lambda\\) probability observing values included \\(\\mathbf{y}\\) highest.Let us formal. Assume four observations: \\(y_1=1.1\\), \\(y_2=2.2\\), \\(y_3=0.7\\) \\(y_4=5.0\\). probability jointly observing:\\(1.1-\\varepsilon \\le Y_1 < 1.1+\\varepsilon\\),\\(2.2-\\varepsilon \\le Y_2 < 2.2+\\varepsilon\\),\\(0.7-\\varepsilon \\le Y_3 < 0.7+\\varepsilon\\), \\(5.0-\\varepsilon \\le Y_4 < 5.0+\\varepsilon\\)?\\(y_i\\)’s ..d., probability \\(\\prod_{=1}^4(2\\varepsilon f(y_i,\\lambda))\\).\nnext plot shows probability (divided \\(16\\varepsilon^4\\), depend \\(\\lambda\\)) function \\(\\lambda\\).\nFigure 6.2: Proba. \\(y_i-\\varepsilon \\le Y_i < y_i+\\varepsilon\\), \\(\\\\{1,2,3,4\\}\\). vertical red line indicates maximum function.\nvalue \\(\\lambda\\) maximizes probability 2.26.Let us come back example 200 observations:\nFigure 6.3: Log-likelihood function associated 200 ..d. observations. vertical red line indicates maximum function.\ncase, value \\(\\lambda\\) maxmimizes probability 3.42.","code":""},{"path":"estimation-methods.html","id":"definition-and-properties","chapter":"6 Estimation Methods","heading":"6.2.2 Definition and properties","text":"\\(f(y;\\boldsymbol\\theta)\\) denotes probability density function (p.d.f.) random variable \\(Y\\) depends set parameters \\(\\boldsymbol\\theta\\). density \\(n\\) independent identically distributed (..d.) observations \\(Y\\) given :\n\\[\nf(\\mathbf{y};\\boldsymbol\\theta) = \\prod_{=1}^n f(y_i;\\boldsymbol\\theta),\n\\]\n\\(\\mathbf{y}\\) denotes vector observations; \\(\\mathbf{y} = \\{y_1,\\dots,y_n\\}\\).Definition 6.2  (Likelihood function) likelihood function :\n\\[\n\\mathcal{L}: \\boldsymbol\\theta \\rightarrow  \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})=f(\\mathbf{y};\\boldsymbol\\theta)=f(y_1,\\dots,y_n;\\boldsymbol\\theta).\n\\]often work \\(\\log \\mathcal{L}\\), log-likelihood function.Example 6.1  (Gaussian distribution) \\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) = - \\frac{1}{2}\\sum_{=1}^n\\left( \\log \\sigma^2 + \\log 2\\pi + \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right).\n\\]Definition 6.3  (Score) score \\(S(y;\\boldsymbol\\theta)\\) given \\(\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\).\\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) (Example 6.1), \n\\[\n\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} =\n\\left[\\begin{array}{c}\n\\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\mu}\\\\\n\\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\sigma^2}\n\\end{array}\\right] =\n\\left[\\begin{array}{c}\n\\dfrac{y-\\mu}{\\sigma^2}\\\\\n\\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right)\n\\end{array}\\right].\n\\]Proposition 6.1  (Score expectation) expectation score zero.Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left(\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\right) &=&\n\\int \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta) dy \\\\\n&=& \\int \\frac{\\partial f(y;\\boldsymbol\\theta)/\\partial \\boldsymbol\\theta}{f(y;\\boldsymbol\\theta)} f(y;\\boldsymbol\\theta) dy =\n\\frac{\\partial}{\\partial \\boldsymbol\\theta} \\int f(y;\\boldsymbol\\theta) dy\\\\\n&=&\\partial 1 /\\partial \\boldsymbol\\theta = 0,\n\\end{eqnarray*}\\]\ngives result.Definition 6.4  (Fisher information matrix) information matrix (minus) expectation second derivatives log-likelihood function:\n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = - \\mathbb{E} \\left( \\frac{\\partial^2 \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right).\n\\]Proposition 6.2  \n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = \\mathbb{E} \\left[ \\left( \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} \\right)\n\\left( \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} \\right)' \\right] = \\mathbb{V}ar[S(Y;\\boldsymbol\\theta)].\n\\]Proof. \\(\\frac{\\partial^2 \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = \\frac{\\partial^2 f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\frac{1}{f(Y;\\boldsymbol\\theta)} - \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\). expectation first right-hand side term \\(\\partial^2 1 /(\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta') = \\mathbf{0}\\), gives result.Example 6.2  \\(y_i \\sim\\,..d.\\, \\mathcal{N}(\\mu,\\sigma^2)\\), let \\(\\boldsymbol\\theta = [\\mu,\\sigma^2]'\\) \n\\[\n\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} = \\left[\\frac{y-\\mu}{\\sigma^2} \\quad \\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right) \\right]',\n\\]\n\n\\[\n\\mathcal{}_Y(\\boldsymbol\\theta) = \\mathbb{E}\\left( \\frac{1}{\\sigma^4}\n\\left[\n\\begin{array}{cc}\n\\sigma^2&y-\\mu\\\\\ny-\\mu & \\frac{(y-\\mu)^2}{\\sigma^2}-\\frac{1}{2}\n\\end{array}\\right]\n\\right)=\n\\left[\n\\begin{array}{cc}\n1/\\sigma^2&0\\\\\n0 & 1/(2\\sigma^4)\n\\end{array}\\right].\n\\]Proposition 6.3  (Additive property Information matrix) information matrix resulting two independent experiments sum information matrices:\n\\[\n\\mathcal{}_{X,Y}(\\boldsymbol\\theta) = \\mathcal{}_X(\\boldsymbol\\theta) + \\mathcal{}_Y(\\boldsymbol\\theta).\n\\]Proof. Directly deduced definition information matrix (Def. 6.4), using epxectation product independent variables product expectations.Theorem 6.1  (Frechet-Darmois-Cramer-Rao bound) Consider unbiased estimator \\(\\boldsymbol\\theta\\) denoted \\(\\hat{\\boldsymbol\\theta}(Y)\\). variance random variable \\(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}\\) (linear combination components \\(\\hat{\\boldsymbol\\theta}\\)) larger :\n\\[\n(\\boldsymbol\\omega'\\boldsymbol\\omega)^2/(\\boldsymbol\\omega' \\mathcal{}_Y(\\boldsymbol\\theta) \\boldsymbol\\omega).\n\\]Proof. Cauchy-Schwarz inequality implies \\(\\sqrt{\\mathbb{V}ar(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}(Y))\\mathbb{V}ar(\\boldsymbol\\omega'S(Y;\\boldsymbol\\theta))} \\ge |\\boldsymbol\\omega'\\mathbb{C}ov[\\hat{\\boldsymbol\\theta}(Y),S(Y;\\boldsymbol\\theta)]\\boldsymbol\\omega |\\). Now, \\(\\mathbb{C}ov[\\hat{\\boldsymbol\\theta}(Y),S(Y;\\boldsymbol\\theta)] = \\int_y \\hat{\\boldsymbol\\theta}(y) \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta)dy = \\frac{\\partial}{\\partial \\boldsymbol\\theta}\\int_y \\hat{\\boldsymbol\\theta}(y) f(y;\\boldsymbol\\theta)dy = \\mathbf{}\\) \\(\\hat{\\boldsymbol\\theta}\\) unbiased. Therefore \\(\\mathbb{V}ar(\\boldsymbol\\omega'\\hat{\\boldsymbol\\theta}(Y)) \\ge \\mathbb{V}ar(\\boldsymbol\\omega'S(Y;\\boldsymbol\\theta))^{-1} (\\boldsymbol\\omega'\\boldsymbol\\omega)^2\\). Prop. 6.2 leads result.Definition 6.5  (Identifiability) vector parameters \\(\\boldsymbol\\theta\\) identifiable , vector \\(\\boldsymbol\\theta^*\\):\n\\[\n\\boldsymbol\\theta^* \\ne \\boldsymbol\\theta \\Rightarrow \\mathcal{L}(\\boldsymbol\\theta^*;\\mathbf{y}) \\ne \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\n\\]Definition 6.6  (Maximum Likelihood Estimator (MLE)) maximum likelihood estimator (MLE) vector \\(\\boldsymbol\\theta\\) maximizes likelihood function. Formally:\n\\[\\begin{equation}\n\\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\\tag{6.8}\n\\end{equation}\\]Definition 6.7  (Likelihood equation) necessary condition maximizing likelihood function (regularity assumption, see Hypotheses 6.1) :\n\\[\\begin{equation}\n\\dfrac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}.\n\\end{equation}\\]Hypothesis 6.1  (Regularity assumptions) :\\(\\boldsymbol\\theta \\\\Theta\\) \\(\\Theta\\) compact.\\(\\boldsymbol\\theta_0\\) identified.log-likelihood function continuous \\(\\boldsymbol\\theta\\).\\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) exists.log-likelihood function \\((1/n)\\log\\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges almost surely \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\), uniformly \\(\\boldsymbol\\theta \\\\Theta\\).log-likelihood function twice continuously differentiable open neighborood \\(\\boldsymbol\\theta_0\\).matrix \\(\\mathbf{}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right)\\) —Fisher Information matrix— exists nonsingular.Proposition 6.4  (Properties MLE) regularity conditions (Assumptions 6.1), MLE :Consistent: \\(\\mbox{plim}\\; \\boldsymbol\\theta_{MLE} = {\\boldsymbol\\theta}_0\\) (\\({\\boldsymbol\\theta}_0\\) true vector parameters).Asymptotically normal:\n\\[\\begin{equation}\n\\boxed{\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0)^{-1}).} \\tag{6.9}\n\\end{equation}\\]Asymptotically efficient: \\(\\boldsymbol\\theta_{MLE}\\) asymptotically efficient achieves Freechet-Darmois-Cramer-Rao lower bound consistent estimators.Invariant: MLE \\(g(\\boldsymbol\\theta_0)\\) \\(g(\\boldsymbol\\theta_{MLE})\\) \\(g\\) continuous continuously differentiable function.Proof. See Appendix 9.5.Since \\(\\mathcal{}_Y(\\boldsymbol\\theta_0)=\\frac{1}{n}\\mathbf{}(\\boldsymbol\\theta_0)\\), asymptotic covariance matrix MLE \\([\\mathbf{}(\\boldsymbol\\theta_0)]^{-1}\\), :\n\\[\n[\\mathbf{}(\\boldsymbol\\theta_0)]^{-1} = \\left[- \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right) \\right]^{-1}.\n\\]\ndirect (analytical) evaluation expectation often reach. can however estimated , either:\n\\[\\begin{eqnarray}\n\\hat{\\mathbf{}}_1^{-1} &=&  \\left( - \\frac{\\partial^2 \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};\\mathbf{y})}{\\partial {\\boldsymbol\\theta} \\partial {\\boldsymbol\\theta}'}\\right)^{-1}, \\tag{6.10}\\\\\n\\hat{\\mathbf{}}_2^{-1} &=&  \\left( \\sum_{=1}^n \\frac{\\partial \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};y_i)}{\\partial {\\boldsymbol\\theta}} \\frac{\\partial \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};y_i)}{\\partial {\\boldsymbol\\theta'}} \\right)^{-1}.  \\tag{6.11}\n\\end{eqnarray}\\]Asymptotically, \\((\\hat{\\mathbf{}}_1^{-1})\\hat{\\mathbf{}}_2=Id\\), , two formulas provide result.case (suspected) misspecification, one can use -called sandwich estimator covariance matrix.12 covariance matrix given :\n\\[\\begin{equation}\n\\hat{\\mathbf{}}_3^{-1} = \\hat{\\mathbf{}}_2^{-1} \\hat{\\mathbf{}}_1 \\hat{\\mathbf{}}_2^{-1}.\\tag{6.12}\n\\end{equation}\\]","code":""},{"path":"estimation-methods.html","id":"to-sum-up-mle-in-practice","chapter":"6 Estimation Methods","heading":"6.2.3 To sum up – MLE in practice","text":"implement MLE, need:parametric model (depending vector parameters \\(\\boldsymbol\\theta\\) whose “true” value \\(\\boldsymbol\\theta_0\\)) specified...d. sources randomness identified.density associated one observation \\(y_i\\) computed analytically (function \\(\\boldsymbol\\theta\\)): \\(f(y;\\boldsymbol\\theta)\\).log-likelihood \\(\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) = \\sum_i \\log f(y_i;\\boldsymbol\\theta)\\).MLE estimator results optimization problem (Eq. (6.8)):\n\\[\\begin{equation}\n\\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).\n\\end{equation}\\]: \\(\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}({\\boldsymbol\\theta}_0,\\mathbf{}(\\boldsymbol\\theta_0)^{-1})\\), \\(\\mathbf{}(\\boldsymbol\\theta_0)^{-1}\\) estimated means Eq. (6.10), Eq. (6.11), Eq. (6.12). time, computation numerical.","code":""},{"path":"estimation-methods.html","id":"example-mle-estimation-of-a-mixture-of-gaussian-distribution","chapter":"6 Estimation Methods","heading":"6.2.4 Example: MLE estimation of a mixture of Gaussian distribution","text":"Consider returns Swiss Market Index (SMI). Assume returns independently drawn mixture Gaussian distributions. p.d.f. \\(f(x;\\boldsymbol\\theta)\\), \\(\\boldsymbol\\theta = [\\mu_1,\\sigma_1,\\mu_2,\\sigma_2,p]'\\), given :\n\\[\np \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}\\exp\\left(-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2}\\right) + (1-p)\\frac{1}{\\sqrt{2\\pi\\sigma_2^2}}\\exp\\left(-\\frac{(x - \\mu_2)^2}{2\\sigma_2^2}\\right).\n\\]\n(See p.d.f. mixtures Gaussian distributions.)\nFigure 6.4: Time series SMI weekly returns (source: Yahoo Finance).\nBuild log-likelihood function (fucntion log.f), use numerical BFGS algorithm maximize (using optim wrapper):Next, compute estimates covariance matrix MLE (using Eqs. (6.10), (6.11), (6.12)), compare three sets resulting standard deviations five estimated paramters:According first (respectively third) type estimate covariance matrix, 95% confidence interval \\(\\mu_1\\) [0.182, 0.42] (resp. [0.151, 0.451]).Note directly estimated parameter \\(p\\) \\(\\nu = \\log(p/(1-p))\\) (way \\(p = \\exp(\\nu)/(1+\\exp(\\nu))\\)). order get estimate standard deviation esitmate \\(p\\), can implement Delta method. method based fact , function \\(g\\) continuous neighborhood \\(\\boldsymbol\\theta_0\\) large \\(n\\), :\n\\[\\begin{equation}\n\\mathbb{V}ar(g(\\hat{\\boldsymbol\\theta}_n)) \\approx \\frac{\\partial g(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'}\\mathbb{V}ar(\\hat{\\boldsymbol\\theta}_n)\\frac{\\partial g(\\hat{\\boldsymbol\\theta}_n)'}{\\partial \\boldsymbol\\theta}.\\tag{6.13}\n\\end{equation}\\]previous results show MLE estimate \\(p\\) 0.8749539, standard deviation approximately equal 0.0612573.finish , let us draw estimated parametric p.d.f. (mixture Gaussian distribution), compare non-parametric (kernel-based) estimate p.d.f. (using function density):\nFigure 6.5: Comparison different estimates distribution returns.\n","code":"\nlibrary(AEC);data(smi)\nT <- dim(smi)[1]\nh <- 5 # holding period (one week)\nsmi$r <- c(rep(NaN,h),\n           100*c(log(smi$Close[(1+h):T]/smi$Close[1:(T-h)])))\nindic.dates <- seq(1,T,by=5)  # weekly returns\nsmi <- smi[indic.dates,]\nsmi <- smi[complete.cases(smi),]\npar(mfrow=c(1,1));par(plt=c(.15,.95,.1,.95))\nplot(smi$Date,smi$r,type=\"l\",xlab=\"\",ylab=\"in percent\")\nabline(h=0,col=\"blue\")\nabline(h=mean(smi$r,na.rm = TRUE)+2*sd(smi$r,na.rm = TRUE),lty=3,col=\"blue\")\nabline(h=mean(smi$r,na.rm = TRUE)-2*sd(smi$r,na.rm = TRUE),lty=3,col=\"blue\")\nf <- function(theta,y){ # Likelihood function\n  mu.1 <- theta[1]; mu.2 <- theta[2]\n  sigma.1 <- theta[3]; sigma.2 <- theta[4]\n  p <- exp(theta[5])/(1+exp(theta[5]))\n  res <- p*1/sqrt(2*pi*sigma.1^2)*exp(-(y-mu.1)^2/(2*sigma.1^2)) +\n    (1-p)*1/sqrt(2*pi*sigma.2^2)*exp(-(y-mu.2)^2/(2*sigma.2^2))\n  return(res)\n}\nlog.f <- function(theta,y){ #log-Likelihood function\n  return(-sum(log(f(theta,y))))\n}\nres.optim <- optim(c(0,0,0.5,1.5,.5),\n                   log.f,\n                   y=smi$r,\n                   method=\"BFGS\", # could be \"Nelder-Mead\"\n                   control=list(trace=FALSE,maxit=100),hessian=TRUE)\ntheta <- res.optim$par\ntheta## [1]  0.3012379 -1.3167476  1.7715072  4.8197596  1.9454889\n# Hessian approach:\nI.1 <- solve(res.optim$hessian)\n# Outer-product of gradient approach:\nlog.f.0 <- log(f(theta,smi$r))\nepsilon <- .00000001\nd.log.f <- NULL\nfor(i in 1:length(theta)){\n  theta.i <- theta\n  theta.i[i] <- theta.i[i] + epsilon\n  log.f.i <- log(f(theta.i,smi$r))\n  d.log.f <- cbind(d.log.f,\n                   (log.f.i - log.f.0)/epsilon)\n}\nI.2 <- solve(t(d.log.f) %*% d.log.f)\n# Misspecification-robust approach (sandwich formula):\nI.3 <- I.1 %*% solve(I.2) %*% I.1\ncbind(diag(I.1),diag(I.2),diag(I.3))##             [,1]        [,2]       [,3]\n## [1,] 0.003683422 0.003199481 0.00586160\n## [2,] 0.226892824 0.194283391 0.38653389\n## [3,] 0.005764271 0.002769579 0.01712255\n## [4,] 0.194081311 0.047466419 0.83130838\n## [5,] 0.092114437 0.040366005 0.31347858\ng <- function(theta){\n  mu.1 <- theta[1]; mu.2 <- theta[2]\n  sigma.1 <- theta[3]; sigma.2 <- theta[4]\n  p <- exp(theta[5])/(1+exp(theta[5]))\n  return(c(mu.1,mu.2,sigma.1,sigma.2,p))\n}\n# Computation of g's gradient around estimated theta:\neps <- .00001\ng.theta <- g(theta)\ng.gradient <- NULL\nfor(i in 1:5){\n  theta.perturb <- theta\n  theta.perturb[i] <- theta[i] + eps\n  g.gradient <- cbind(g.gradient,(g(theta.perturb)-g.theta)/eps)\n}\nVar <- g.gradient %*% I.3 %*% t(g.gradient)\nstdv.g.theta <- sqrt(diag(Var))\nstdv.theta <- sqrt(diag(I.3))\ncbind(theta,stdv.theta,g.theta,stdv.g.theta)##           theta stdv.theta    g.theta stdv.g.theta\n## [1,]  0.3012379 0.07656108  0.3012379   0.07656108\n## [2,] -1.3167476 0.62171850 -1.3167476   0.62171850\n## [3,]  1.7715072 0.13085316  1.7715072   0.13085316\n## [4,]  4.8197596 0.91176114  4.8197596   0.91176114\n## [5,]  1.9454889 0.55989158  0.8749539   0.06125726\nx <- seq(-5,5,by=.01)\npar(plt=c(.1,.95,.1,.95))\nplot(x,f(theta,x),type=\"l\",lwd=2,xlab=\"returns, in percent\",ylab=\"\",\n     ylim=c(0,1.4*max(f(theta,x))))\nlines(density(smi$r),type=\"l\",lwd=2,lty=3)\nlines(x,dnorm(x,mean=mean(smi$r),sd = sd(smi$r)),col=\"red\",lty=2,lwd=2)\nrug(smi$r,col=\"blue\")\nlegend(\"topleft\",\n       c(\"Kernel estimate (non-parametric)\",\n         \"Estimated mixture of Gaussian distr. (MLE, parametric)\",\n         \"Normal distribution\"),\n       lty=c(3,1,2),lwd=c(2), # line width\n       col=c(\"black\",\"black\",\"red\"),pt.bg=c(1),pt.cex = c(1),\n       bg=\"white\",seg.len = 4)"},{"path":"estimation-methods.html","id":"TestMLE","chapter":"6 Estimation Methods","heading":"6.2.5 Test procedures","text":"Suppose want test following parameter restrictions:\n\\[\\begin{equation}\n\\boxed{H_0: \\underbrace{h(\\boldsymbol\\theta)}_{r \\times 1}=0.}\n\\end{equation}\\]context MLE, three tests largely used:Likelihood Ratio (LR) test,Wald (W) test,Lagrange Multiplier (LM) test.rationale behind three tests:13LR: \\(h(\\boldsymbol\\theta)=0\\), imposing restriction estimation (restricted estimator) result large decrease likelihood function (w.r.t unrestricted estimation).Wald: \\(h(\\boldsymbol\\theta)=0\\), \\(h(\\hat{\\boldsymbol\\theta})\\) far \\(0\\) (even restrictions imposed MLE).LM: \\(h(\\boldsymbol\\theta)=0\\), gradient likelihood function small evaluated restricted estimator.terms implementation, LR necessitates estimate restricted unrestricted models, Wald test requires estimation unrestricted model , LM tests requires estimation restricted model .shown , three test statistics associated three tests coincide asymptotically. (Therefore, naturally asymptotic distribution, \\(\\chi^2\\).)Proposition 6.5  (Asymptotic distribution Wald statistic) regularity conditions (Assumptions 6.1) \\(H_0: h(\\boldsymbol\\theta)=0\\), Wald statistic, defined :\n\\[\n\\boxed{\\xi^W = h(\\hat{\\boldsymbol\\theta})' \\mathbb{V}ar[h(\\hat{\\boldsymbol\\theta})]^{-1} h(\\hat{\\boldsymbol\\theta}),}\n\\]\n\n\\[\\begin{equation}\n\\mathbb{V}ar[h(\\hat{\\boldsymbol\\theta})] = \\left(\\frac{\\partial h(\\hat{\\boldsymbol\\theta})}{\\partial \\boldsymbol\\theta'} \\right) \\mathbb{V}ar[\\hat{\\boldsymbol\\theta}]\n\\left(\\frac{\\partial h(\\hat{\\boldsymbol\\theta})'}{\\partial \\boldsymbol\\theta} \\right),\\tag{6.14}\n\\end{equation}\\]\nasymptotically \\(\\chi^2(r)\\), number degrees freedom \\(r\\) corresponds dimension \\(h(\\boldsymbol\\theta)\\). (Note Eq. (6.14) one used Delta method, see Eq. (6.13).)Wald test, defined critical region\n\\[\n\\{\\xi^W \\ge \\chi^2_{1-\\alpha}(r)\\},\n\\]\n\\(\\chi^2_{1-\\alpha}(r)\\) denotes quantile level \\(1-\\alpha\\) \\(\\chi^2(r)\\) distribution, asymptotic level \\(\\alpha\\) consistent.14Proof. See Appendix 9.5.practice, Eq. (6.14), \\(\\mathbb{V}ar[\\hat{\\boldsymbol\\theta}]\\) replaced estimate given, e.g., Eq. (6.10), Eq. (6.11), Eq. (6.12).Proposition 6.6  (Asymptotic distribution LM test statistic) regularity conditions (Assumptions 6.1) \\(H_0: h(\\boldsymbol\\theta)=0\\), LM statistic\n\\[\\begin{equation}\n\\boxed{\\xi^{LM} =\n\\left(\\left.\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta'}\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}^0}  \\right)\n[\\mathbf{}(\\hat{\\boldsymbol\\theta}^0)]^{-1}\n\\left(\\left.\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta }\\right|_{\\boldsymbol\\theta = \\hat{\\boldsymbol\\theta}^0}  \\right),} \\tag{6.15}\n\\end{equation}\\]\n(\\(\\hat{\\boldsymbol\\theta}^0\\) restricted MLE estimator) \\(\\chi^2(r)\\).test defined critical region:\n\\[\n\\{\\xi^{LM} \\ge \\chi^2_{1-\\alpha}(r)\\}\n\\]\nasymptotic level \\(\\alpha\\) consistent (see Defs. 3.2 3.3). test called Score Lagrange Multiplier (LM) test.Proof. See Appendix 9.5.Definition 6.8  (Likelihood Ratio test statistics) likelihood ratio associated restriction form \\(H_0: h({\\boldsymbol\\theta})=0\\) given :\n\\[\nLR = \\frac{\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})}{\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})} \\quad (\\[0,1]),\n\\]\n\\(\\mathcal{L}_R\\) (respectively \\(\\mathcal{L}_U\\)) likelihood function imposes (resp. impose) restriction. likelihood ratio test statistic given \\(-2\\log(LR)\\), :\n\\[\n\\boxed{\\xi^{LR}= 2 (\\log\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})-\\log\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})).}\n\\]Proposition 6.7  (Asymptotic equivalence LR, LM, Wald tests) null hypothesis \\(H_0\\), , asymptotically:\n\\[\n\\xi^{LM} = \\xi^{LR} = \\xi^{W}.\n\\]Proof. See Appendix 9.5.","code":""},{"path":"estimation-methods.html","id":"bayesian-approach","chapter":"6 Estimation Methods","heading":"6.3 Bayesian approach","text":"","code":""},{"path":"estimation-methods.html","id":"introduction","chapter":"6 Estimation Methods","heading":"6.3.1 Introduction","text":"excellent introduction Bayesian methods proposed Martin Haugh, 2017.suggested name approach, starting point Bayes formula:\n\\[\n\\mathbb{P}(|B) = \\frac{\\mathbb{P}(\\& B)}{\\mathbb{P}(B)},\n\\]\n\\(\\) \\(B\\) two “events”. instance, \\(\\) may : parameter \\(\\alpha\\) (conceived something stochastic) lies interval \\([,b]\\). Assume interested probability occurrence \\(\\). Without specific information (“unconditionally”), probability \\(\\mathbb{P}()\\). evaluation probability can better provided additional form information. Typically, event \\(B\\) tends occur simultaneously \\(\\), knowledge \\(B\\) can useful. Bayes formula says additional information (\\(B\\)) can used “update” probability event \\(\\).case, intuition work follows: assume know form data-generating process (DGP). , know structure model used draw stochastic data; also know type distributions used generate data. However, know numerical values parameters characterizing DGP. Let us denote \\({\\boldsymbol\\theta}\\) vector unknown parameters. parameters known exactly, assume –even without observed data– priors distribution. , case example (\\(\\) \\(B\\)), observation data generated model can reduce uncertainty associated \\({\\boldsymbol\\theta}\\). Loosely speaking, combining priors observations data generated model result “thinner” distributions components \\({\\boldsymbol\\theta}\\). latter distributions called posterior distributions.15Let us formalize intuition. Define prior \\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\) model realizations (“data”) vector \\(\\mathbf{y}\\). joint distribution \\((\\mathbf{y},{\\boldsymbol\\theta})\\) given :\n\\[\nf_{Y,{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta}) = f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}),\n\\]\n, symmetrically, \n\\[\nf_{Y,{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta}) = f_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y})f_Y(\\mathbf{y}),\n\\]\n\\(f_{{\\boldsymbol\\theta}|Y}(\\cdot,\\mathbf{y})\\), distribution parameters conditional observations, posterior distribution.last two equations imply :\n\\[\\begin{equation}\nf_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y}) = \\frac{f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{{\\boldsymbol\\theta}}({\\boldsymbol\\theta})}{f_Y(\\mathbf{y})}.\\tag{6.16}\n\\end{equation}\\]\nNote \\(f_Y\\) marginal (unconditional) distribution \\(\\mathbf{y}\\), can written:\n\\[\\begin{equation}\nf_Y(\\mathbf{y}) = \\int f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}) d {\\boldsymbol\\theta}.\n\\end{equation}\\]Eq. (6.16) sometimes rewritten follows:\n\\[\\begin{equation}\nf_{{\\boldsymbol\\theta}|Y}({\\boldsymbol\\theta},\\mathbf{y}) \\propto f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta},\\mathbf{y}) := f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta}), \\tag{6.17}\n\\end{equation}\\]\n\\(\\propto\\) means, loosely speaking, “proportional ”. rare instances, starting given priors, one can analytically compute posterior distribution \\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta},\\mathbf{y})\\). However, cases, reach. One resort numerical approaches compute posterior distribution. Monte Carlo Markov Chains (MCMC) one .According Bernstein-von Mises Theorem, Bayesian MLE estimators large sample properties. (particular, Bayesian approach also achieve FDCR bound, see Theorem 6.1.) intuition behind result influence prior diminishes increasing sample sizes.","code":""},{"path":"estimation-methods.html","id":"monte-carlo-markov-chains","chapter":"6 Estimation Methods","heading":"6.3.2 Monte-Carlo Markov Chains","text":"MCMC techniques aim using simulations approach distribution whose distribution difficult obtain analytically. Indeed, circumstances, one can draw distribution even know analytical expression.Definition 6.9  (Markov Chain) sequence \\(\\{z_i\\}\\) said (first-order) Markovian process satisfies:\n\\[\nf(z_i|z_{-1},z_{-2},\\dots) = f(z_i|z_{-1}).\n\\]Metropolis-Hastings (MH) algorithm specific MCMC approach allows generate samples \\({\\boldsymbol\\theta}\\)’s whose distribution approximately corresponds posterior distribution Eq. (6.16).MH algorithm recursive algorithm. , one can draw \\(^{th}\\) value \\({\\boldsymbol\\theta}\\), denoted \\({\\boldsymbol\\theta}_i\\), one already drawn \\({\\boldsymbol\\theta}_{-1}\\). Assume \\({\\boldsymbol\\theta}_{-1}\\). obtain value \\({\\boldsymbol\\theta}_i\\) implementing following steps:Draw \\(\\tilde{{\\boldsymbol\\theta}}_i\\) conditional distribution \\(Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\cdot,{\\boldsymbol\\theta}_{-1})\\), called proposal distribution.Draw \\(u\\) uniform distribution \\([0,1]\\).Compute\n\\[\\begin{equation}\n\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1}):= \\min\\left(\\frac{f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}}_i,\\mathbf{y})}{f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})}\\times\\frac{Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}({\\boldsymbol\\theta}_{-1},\\tilde{{\\boldsymbol\\theta}}_i)}{Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})},1\\right),\\tag{6.18}\n\\end{equation}\\]\n\\(f_{{\\boldsymbol\\theta},Y}\\) given Eq. (6.17).\\(u<\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})\\), take \\({\\boldsymbol\\theta}_i = \\tilde{{\\boldsymbol\\theta}}_i\\), otherwise leave \\({\\boldsymbol\\theta}_i\\) equal \\({\\boldsymbol\\theta}_{-1}\\).can shown , distribution draws converges posterior distribution. , sufficiently large number iterations, draws can considered drawn posterior distribution.16To get insights algorithm, consider case symmetric proposal distribution, :\n\\[\\begin{equation}\nQ_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})=Q_{\\tilde{{\\boldsymbol\\theta}}|{\\boldsymbol\\theta}}({\\boldsymbol\\theta}_{-1},\\tilde{{\\boldsymbol\\theta}}_i).\\tag{6.19}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})= \\min\\left(\\frac{q(\\tilde{{\\boldsymbol\\theta}},y)}{q({\\boldsymbol\\theta}_{-1},y)},1\\right). \\tag{6.20}\n\\end{equation}\\]\nRemember , marginal distribution data (\\(f_Y(\\mathbf{y})\\)), \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})\\) probability observing \\(\\mathbf{y}\\) conditional model parameterized \\(\\tilde{\\boldsymbol\\theta}\\). , Eq. (6.20), appears probability larger \\(\\tilde{\\boldsymbol\\theta}\\) \\({\\boldsymbol\\theta}_{-1}\\) (case \\(\\tilde{\\boldsymbol\\theta}\\) seems “consistent observations \\(\\mathbf{y}\\)” \\({\\boldsymbol\\theta}_{-1}\\)), accept \\({\\boldsymbol\\theta}_i\\). contrast, \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})<f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})\\), necessarily accept proposed value \\(\\tilde{{\\boldsymbol\\theta}}\\), especially \\(f_{{\\boldsymbol\\theta},Y}(\\tilde{{\\boldsymbol\\theta}},\\mathbf{y})\\ll f_{{\\boldsymbol\\theta},Y}({\\boldsymbol\\theta}_{-1},\\mathbf{y})\\) (case \\(\\tilde{\\boldsymbol\\theta}\\) seems far less consistent observations \\(\\mathbf{y}\\) \\({\\boldsymbol\\theta}_{-1}\\), , accordingly, acceptance probability, namely \\(\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})\\), small).choice proposal distribution \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}\\) crucial get rapid convergence algorithm. Looking Eq. (6.18), easily seen optimal choice \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}(\\cdot,{\\boldsymbol\\theta}_i)=f_{{\\boldsymbol\\theta}|Y}(\\cdot,\\mathbf{y})\\). case, \\(\\alpha(\\tilde{{\\boldsymbol\\theta}}_i,{\\boldsymbol\\theta}_{-1})\\equiv 1\\) (see Eq. (6.18)). accept draws proposal distribution, distribution directly posterior distribution. course, situation realistoc objective algorithm precisely approximate posterior distribution.common choice \\(Q\\) multivariate normal distribution. \\({\\boldsymbol\\theta}\\) dimension \\(K\\), can instance use:\n\\[\nQ(\\tilde{\\boldsymbol\\theta},{\\boldsymbol\\theta})= \\frac{1}{\\left(\\sqrt{2\\pi\\sigma^2}\\right)^K}\\exp\\left(-\\frac{1}{2}\\sum_{j=1}^K\\frac{(\\tilde{\\boldsymbol\\theta}_j-{\\boldsymbol\\theta}_j)^2}{\\sigma^2}\\right),\n\\]\nexample symmetric proposal distribution (see Eq. (6.19)). Equivalently, :\n\\[\n\\tilde{\\boldsymbol\\theta} = {\\boldsymbol\\theta} + \\varepsilon,\n\\]\n\\(\\varepsilon\\) \\(K\\)-dimensional vector independent zero-mean normal disturbances variance \\(\\sigma^2\\).17 One determine appropriate value \\(\\sigma\\). low, \\(\\alpha\\) close 1 (\\(\\tilde{{\\boldsymbol\\theta}}_i\\) close \\({\\boldsymbol\\theta}_{-1}\\)), accept often proposed value (\\(\\tilde{{\\boldsymbol\\theta}}_i\\)). seems favourable situation. may . Indeed, means take large number iterations explore whole distribution \\({\\boldsymbol\\theta}\\). \\(\\sigma\\) large? case, likely porposed values (\\(\\tilde{{\\boldsymbol\\theta}}_i\\)) often result poor likelihoods; probability acceptance low Markov chain may blocked initial value. Therefore, intermediate values \\(\\sigma^2\\) determined. acceptance rate (.e., average value \\(\\alpha(\\tilde{{\\boldsymbol\\theta}},{\\boldsymbol\\theta}_{-1})\\)) can used guide . Indeed, literature explores optimal values acceptance rate (order obtain best possible fit posterior minimum number algorithm iterations). particular, following Roberts, Gelman, Gilks (1997), people often target acceptance rate order magnitude 20%.important note , implement approach, one able compute joint p.d.f. \\(q({\\boldsymbol\\theta},\\mathbf{y})=f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\) (Eq. (6.17)). , soon one can evaluate likelihood (\\(f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})\\)) prior (\\(f_{\\boldsymbol\\theta}({\\boldsymbol\\theta})\\)), can employ methodology.","code":""},{"path":"estimation-methods.html","id":"example-ar1-specification","chapter":"6 Estimation Methods","heading":"6.3.3 Example: AR(1) specification","text":"following example, employ MCMC order estimate posterior distributions three parameters defining AR(1) model (see Section 8.2.2). specification follows:\n\\[\ny_t = \\mu + \\rho y_{t-1} + \\sigma \\varepsilon_{t}, \\quad \\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,1).\n\\]\nHence, \\({\\boldsymbol\\theta} = [\\mu,\\rho,\\sigma]\\). Let us first simulate process \\(T\\) periods:Next, let us write likelihood function, .e. \\(f_{Y|{\\boldsymbol\\theta}}(\\mathbf{y},{\\boldsymbol\\theta})\\). \\(\\rho\\), expected 0 1, use logistic transformation. \\(\\sigma\\), expected positive, use exponential transformation.Next define function rQ draws (Gaussian) proposal distribution, well function Q, computes \\(Q_{\\tilde{\\boldsymbol\\theta}|{\\boldsymbol\\theta}}(\\tilde{\\boldsymbol\\theta},{\\boldsymbol\\theta})\\):consider Gaussian priors:Function p_tilde corresponds \\(f_{{\\boldsymbol\\theta},Y}\\):can now define function \\(\\alpha\\) (Eq. (6.18)):Now, set us write MCMC function:Specify Gaussian priors:\nFigure 6.6: upper line plot compares prior (black) posterior (red) distributions. vertical dashed blue lines indicate true values parameters. second row plots show sequence \\(\\boldsymbol\\theta_i\\)’s generated MCMC algorithm. sequences ones used produce posterior distributions (red lines) upper plots.\n","code":"\nmu <- .6; rho <- .8; sigma <- .5 # true model specification\nT <- 20 # number of observations\ny0 <- mu/(1-rho)\nY <- NULL\nfor(t in 1:T){\n  if(t==1){y <- y0}\n  y <- mu + rho*y + sigma * rnorm(1)\n  Y <- c(Y,y)}\nplot(Y,type=\"l\",xlab=\"time t\",ylab=expression(y[t]))\nlikelihood <- function(param,Y){\n  mu  <- param[1]\n  rho <- exp(param[2])/(1+exp(param[2]))\n  sigma <- exp(param[3])\n  MU <- mu/(1-rho)\n  SIGMA2 <- sigma^2/(1-rho^2)\n  L <- 1/sqrt(2*pi*SIGMA2)*exp(-(Y[1]-MU)^2/(2*SIGMA2))\n  Y1 <- Y[2:length(Y)]\n  Y0 <- Y[1:(length(Y)-1)]\n  aux <- 1/sqrt(2*pi*sigma^2)*exp(-(Y1-mu-rho*Y0)^2/(2*sigma^2))\n  L <- L * prod(aux)\n  return(L)\n}\nrQ <- function(x,a){\n  n <- length(x)\n  y <- x + a * rnorm(n)\n  return(y)}\nQ <- function(y,x,a){\n  q <- 1/sqrt(2*pi*a^2)*exp(-(y - x)^2/(2*a^2))\n  return(prod(q))}\nprior <- function(param,means_prior,stdv_prior){\n  f <- 1/sqrt(2*pi*stdv_prior^2)*exp(-(param - \n                                         means_prior)^2/(2*stdv_prior^2))\n  return(prod(f))}\np_tilde <- function(param,Y,means_prior,stdv_prior){\n  p <- likelihood(param,Y) * prior(param,means_prior,stdv_prior)\n  return(p)}\nalpha <- function(y,x,means_prior,stdv_prior,a){\n  aux <- p_tilde(y,Y,means_prior,stdv_prior)/\n    p_tilde(x,Y,means_prior,stdv_prior) * Q(y,x,a)/Q(x,y,a)\n  alpha_proba <- min(aux,1)\n  return(alpha_proba)}\nMCMC <- function(Y,means_prior,stdv_prior,a,N){\n  x <- means_prior\n  all_theta <- NULL\n  count_accept <- 0\n  for(i in 1:N){\n    y <- rQ(x,a)\n    alph <- alpha(y,x,means_prior,stdv_prior,a)\n    #print(alph)\n    u <- runif(1)\n    if(u < alph){\n      count_accept <- count_accept + 1\n      x <- y}\n    all_theta <- rbind(all_theta,x)}\n  print(paste(\"Acceptance rate:\",toString(round(count_accept/N,3))))\n  return(all_theta)}\ntrue_values <- c(mu,log(rho/(1-rho)),log(sigma))\nmeans_prior <- c(1,0,0) # as if we did not know the true values\nstdv_prior <- rep(2,3)\nresultMCMC <- MCMC(Y,means_prior,stdv_prior,a=.45,N=20000)## [1] \"Acceptance rate: 0.098\"\npar(mfrow=c(2,3))\nfor(i in 1:length(means_prior)){\n  m <- means_prior[i]\n  s <- stdv_prior[i]\n  x <- seq(m-3*s,m+3*s,length.out = 100)\n  par(mfg=c(1,i))\n  aux <- density(resultMCMC[,i])\n  par(plt=c(.15,.95,.15,.85))\n  plot(x,dnorm(x,m,s),type=\"l\",xlab=\"\",ylab=\"\",main=paste(\"Parameter\",i),\n       ylim=c(0,max(aux$y)))\n  lines(aux$x,aux$y,col=\"red\",lwd=2)\n  abline(v=true_values[i],lty=2,col=\"blue\")\n  par(mfg=c(2,i))\n  plot(resultMCMC[,i],1:length(resultMCMC[,i]),xlim=c(min(x),max(x)),\n       type=\"l\",xlab=\"\",ylab=\"\")}"},{"path":"binary-choice-models.html","id":"binary-choice-models","chapter":"7 Binary-choice models","heading":"7 Binary-choice models","text":"microeconometric models, variables interest often feature restricted distributions —instance discontinuous support—, necessitates specific models. Typically, many instances, variables explained (\\(y_i\\)’s) two possible values (\\(0\\) \\(1\\), say). , binary variables. probability equal either 0 1 may depend independent variables, gathered vectors \\(\\mathbf{x}_i\\) (\\(K \\times 1\\)).spectrum applications wide:Binary decisions (e.g. referendums, owner renter, living city countryside, /labour force,…),Contamination (disease default),Success/failure (exams).Without loss generality, model reads:\n\\[\\begin{equation}\\label{eq:binaryBenroulli}\ny_i | \\mathbf{X} \\sim \\mathcal{B}(g(\\mathbf{x}_i;\\boldsymbol\\theta)),\n\\end{equation}\\]\n\\(g(\\mathbf{x}_i;\\boldsymbol\\theta)\\) parameter Bernoulli distribution. words, conditionally \\(\\mathbf{X}\\):\n\\[\\begin{equation}\ny_i = \\left\\{\n\\begin{array}{cl}\n1 & \\mbox{ probability } g(\\mathbf{x}_i;\\boldsymbol\\theta)\\\\\n0 & \\mbox{ probability } 1-g(\\mathbf{x}_i;\\boldsymbol\\theta),\n\\end{array}\n\\right.\\tag{7.1}\n\\end{equation}\\]\n\\(\\boldsymbol\\theta\\) vector parameters estimated.estimation strategy assume \\(g(\\mathbf{x}_i;\\boldsymbol\\theta)\\) can proxied \\(\\tilde{\\boldsymbol\\theta}'\\mathbf{x}_i\\) run linear regression estimate \\(\\tilde{\\boldsymbol\\theta}\\) (situation called Linear Probability Model, LPM):\n\\[\ny_i = \\tilde{\\boldsymbol\\theta}'\\mathbf{x}_i + \\varepsilon_i.\n\\]\nNotwithstanding fact specification exclude negative probabilities probabilities greater one, compatible assumption zero conditional mean (Hypothesis 4.2) assumption non-correlated residuals (Hypothesis 4.4), difficultly homoskedasticity assumption (Hypothesis 4.3). Moreover, \\(\\varepsilon_i\\)’s Gaussian (\\(y_i \\\\{0,1\\}\\)). Hence, using linear regression study relationship \\(\\mathbf{x}_i\\) \\(y_i\\) can consistent inefficient.Figure 7.1 illustrates fit resulting application LPM model binary (dependent) variables.\nFigure 7.1: Fitting binary variable linear model (Linear Probability Model, LPM). model \\(\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)\\), \\(\\Phi\\) c.d.f. normal distribution \\(x_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\).\nExcept last row (LPM case), Table 7.1 provides examples functions \\(g\\) valued \\([0,1]\\), can therefore used models type: \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = g(\\boldsymbol\\theta'\\mathbf{x}_i)\\) (see Eq. (7.1)). “linear” case given comparison, note satisfy \\(g(\\boldsymbol\\theta'\\mathbf{x}_i) \\[0,1]\\) value \\(\\boldsymbol\\theta'\\mathbf{x}_i\\).Table 7.1:  table provides examples function \\(g\\), s.t. \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol heta) = g(\\boldsymbol\\theta'\\mathbf{x}_i)\\). LPM case (last row) given comparison , , satisfy \\(g(\\boldsymbol\\theta'\\mathbf{x}_i) \\[0,1]\\) value \\(\\boldsymbol\\theta'\\mathbf{x}_i\\).Figure 7.2 displays first three \\(g\\) functions appearing Table 7.1.\nFigure 7.2: Probit, Logit, Log-log functions.\nprobit logit models popular binary-choice models. probit model, :\n\\[\\begin{equation}\ng(z) = \\Phi(z),\\tag{7.2}\n\\end{equation}\\]\n\\(\\Phi\\) c.d.f. normal distribution. logit model:\n\\[\\begin{equation}\ng(z) = \\frac{1}{1+\\exp(-z)}.\\tag{7.3}\n\\end{equation}\\]Figure 7.3 shows conditional probabilities associated (probit) model used generate data Figure 7.1.\nFigure 7.3: model \\(\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)\\), \\(\\Phi\\) c.d.f. normal distribution \\(x_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\). Crosses give model-implied probabilities \\(y_i=1\\) (conditional \\(x_i\\)).\n","code":""},{"path":"binary-choice-models.html","id":"latent","chapter":"7 Binary-choice models","heading":"7.1 Interpretation in terms of latent variable, and utility-based models","text":"probit model interpretation terms latent variables, , turn, often exploited structural models, called Random Utility Models (RUM). structural models, assumed agents take decision selecting outcome provides larger utility (agent \\(\\), two possible outcomes: \\(y_i=0\\) \\(y_i=1\\)). Part utility observed econometrician —depends covariates \\(\\mathbf{x}_i\\)— part latent.probit model, :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = \\Phi(\\boldsymbol\\theta'\\mathbf{x}_i) = \\mathbb{P}(-\\varepsilon_{}<\\boldsymbol\\theta'\\mathbf{x}_i),\n\\]\n\\(\\varepsilon_{} \\sim \\mathcal{N}(0,1)\\). :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = \\mathbb{P}(0< y_i^*),\n\\]\n\\(y_i^* = \\boldsymbol\\theta'\\mathbf{x}_i + \\varepsilon_i\\), \\(\\varepsilon_{} \\sim \\mathcal{N}(0,1)\\). Variable \\(y_i^*\\) can interpreted (latent) variable determines \\(y_i\\) (since \\(y_i = \\mathbb{}_{\\{y_i^*>0\\}}\\)).Figure 7.4 illustrates situation.\nFigure 7.4: Distribution \\(y_i^*\\) conditional \\(\\mathbf{x}_i\\).\nAssume agent (\\(\\)) chooses \\(y_i=1\\) utility associated choice (\\(U_{,1}\\)) higher one associated \\(y_i=0\\) (\\(U_{,0}\\)). Assume utility agent \\(\\), chooses outcome \\(j\\) (\\(\\\\{0,1\\}\\)), given \n\\[\nU_{,j} = V_{,j} + \\varepsilon_{,j},\n\\]\n\\(V_{,j}\\) deterministic component utility associated choice \\(\\varepsilon_{,j}\\) random (agent-specific) component. Moreover, posit \\(V_{,j} = \\boldsymbol\\theta_j'\\mathbf{x}_i\\). :\n\\[\\begin{eqnarray}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta) &=& \\mathbb{P}(\\boldsymbol\\theta_1'\\mathbf{x}_i+\\varepsilon_{,1}>\\boldsymbol\\theta_0'\\mathbf{x}_i+\\varepsilon_{,0}) \\nonumber\\\\\n&=& F(\\boldsymbol\\theta_1'\\mathbf{x}_i-\\boldsymbol\\theta_0'\\mathbf{x}_i) = F([\\boldsymbol\\theta_1-\\boldsymbol\\theta_0]'\\mathbf{x}_i),\\tag{7.4}\n\\end{eqnarray}\\]\n\\(F\\) c.d.f. \\(\\varepsilon_{,0}-\\varepsilon_{,1}\\).Note difference \\(\\boldsymbol\\theta_1-\\boldsymbol\\theta_0\\) identifiable (opposed \\(\\boldsymbol\\theta_1\\) \\(\\boldsymbol\\theta_0\\)). Moreover, replacing \\(U\\) \\(aU\\) (\\(>0\\)) gives model; scaling issue can solved fixing variance \\(\\varepsilon_{,0}-\\varepsilon_{,1}\\).Example 7.1  (Migration income) RUM approach used Nakosteen Zimmer (1980) study migration choices. model based comparison marginal costs benefits associated migration. main ingredients approach follows:Wage can earned present location: \\(y_p^* = \\boldsymbol\\theta_p'\\mathbf{x}_p + \\varepsilon_p\\).Migration cost: \\(C^*= \\boldsymbol\\theta_c'\\mathbf{x}_c + \\varepsilon_c\\).Wage earned elsewhere: \\(y_m^* = \\boldsymbol\\theta_m'\\mathbf{x}_m + \\varepsilon_m\\).context, agents decision migrate \\(y_m^* > y_p^* + C^*\\), .e. \n\\[\ny^* = y_m^* -  y_p^* - C^* =  \\boldsymbol\\theta'\\mathbf{x} + \\underbrace{\\varepsilon}_{=\\varepsilon_m - \\varepsilon_c - \\varepsilon_p}>0,\n\\]\n\\(\\mathbf{x}\\) union \\(\\mathbf{x}_i\\)s, \\(\\\\{p,m,c\\}\\).","code":""},{"path":"binary-choice-models.html","id":"Avregressors","chapter":"7 Binary-choice models","heading":"7.2 Alternative-Varying Regressors","text":"cases, regressors may depend considered alternative (\\(0\\) \\(1\\)). instance:modeling decision participate labour force (), wage depends alternative. Typically, zero considered agent decided work (strictly positive otherwise).context choice transportation mode, “time cost” depends considered transportation mode.terms utility, :\n\\[\nV_{,j} = {\\theta^{(u)}_{j}}'\\mathbf{u}_{,j} + {\\theta^{(v)}_{j}}'\\mathbf{v}_{},\n\\]\n\\(\\mathbf{u}_{,j}\\)’s regressors associated agent \\(\\), taking different values different choices (\\(j=0\\) \\(j=1\\)). case, Eq. (7.4) becomes:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta)  = F\\left({\\theta^{(u)}_{1}}'\\mathbf{u}_{,1}-{\\theta^{(u)}_{0}}'\\mathbf{u}_{,0}+[\\boldsymbol\\theta_1^{(v)}-\\boldsymbol\\theta_0^{(v)}]'\\mathbf{v}_i\\right),\\tag{7.5}\n\\end{equation}\\]\n, \\(\\theta^{(u)}_{1}=\\theta^{(u)}_{0}=\\theta^{(u)}\\) —customary— get:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta)  = F\\left({\\theta^{(u)}_{1}}'(\\mathbf{u}_{,1}-\\mathbf{u}_{,0})+[\\boldsymbol\\theta_1^{(v)}-\\boldsymbol\\theta_0^{(v)}]'\\mathbf{v}_i\\right).\\tag{7.6}\n\\end{equation}\\]Example 7.2  (Fishing-mode dataset) fishing-mode dataset used Cameron Trivedi (2005) (Chapters 14 15) contains alternative-specific variables. Specifically, individual, price catch rate depend fishing model. table reported , lines price catch correspond prices catch rates associated chosen alternative.","code":"\nlibrary(mlogit)\ndata(\"Fishing\",package=\"mlogit\")\nstargazer::stargazer(Fishing,type=\"text\")## \n## ==========================================================\n## Statistic       N     Mean    St. Dev.    Min      Max    \n## ----------------------------------------------------------\n## price.beach   1,182  103.422   103.641   1.290   843.186  \n## price.pier    1,182  103.422   103.641   1.290   843.186  \n## price.boat    1,182  55.257    62.713    2.290   666.110  \n## price.charter 1,182  84.379    63.545   27.290   691.110  \n## catch.beach   1,182   0.241     0.191    0.068    0.533   \n## catch.pier    1,182   0.162     0.160    0.001    0.452   \n## catch.boat    1,182   0.171     0.210   0.0002    0.737   \n## catch.charter 1,182   0.629     0.706    0.002    2.310   \n## income        1,182 4,099.337 2,461.964 416.667 12,500.000\n## ----------------------------------------------------------"},{"path":"binary-choice-models.html","id":"estimation","chapter":"7 Binary-choice models","heading":"7.3 Estimation","text":"models can estimated Maximum Likelihood approaches (see Section 6.2).simplify exposition, consider \\(\\mathbf{x}_i\\) vectors covariates deterministic. Moreover, assume r.v. independent across entities \\(\\). write likelihood case? easily checked :\n\\[\nf(y_i|\\mathbf{x}_i;\\boldsymbol\\theta) =   g(\\boldsymbol\\theta'\\mathbf{x}_i)^{y_i}(1-g(\\boldsymbol\\theta'\\mathbf{x}_i))^{1-y_i}.\n\\]Therefore, observations \\((\\mathbf{x}_i,y_i)\\) independent across entities \\(\\), obtain:\n\\[\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^{n}y_i \\log[g(\\boldsymbol\\theta'\\mathbf{x}_i)] + (1-y_i)\\log[1-g(\\boldsymbol\\theta'\\mathbf{x}_i)].\n\\]likelihood equation reads (FOC optimization program, see Def. 6.7):\n\\[\n\\dfrac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta} = \\mathbf{0},\n\\]\n:\n\\[\n\\sum_{=1}^{n} y_i \\mathbf{x}_i\\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}{g(\\boldsymbol\\theta'\\mathbf{x}_i)} - (1-y_i) \\mathbf{x}_i \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}{1-g(\\boldsymbol\\theta'\\mathbf{x}_i)} = \\mathbf{0}.\n\\]nonlinear (multivariate) equation can solved numerically. regularity conditions (Hypotheses 6.1), approximately (Prop. 6.4):\n\\[\n\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}(\\boldsymbol\\theta_0,\\mathbf{}(\\boldsymbol\\theta_0)^{-1}),\n\\]\n\n\\[\n\\mathbf{}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right) = n \\mathcal{}_Y(\\boldsymbol\\theta_0).\n\\]finite samples, can e.g. approximate \\(\\mathbf{}(\\boldsymbol\\theta_0)^{-1}\\) Eq. (6.10):\n\\[\n\\mathbf{}(\\boldsymbol\\theta_0)^{-1} \\approx -\\left(\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_{MLE};\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right)^{-1}.\n\\]Probit case (see Table 7.1), can shown :\n\\[\\begin{eqnarray*}\n&&\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = - \\sum_{=1}^{n} g'(\\boldsymbol\\theta'\\mathbf{x}_i) [\\mathbf{x}_i \\mathbf{x}_i'] \\times \\\\\n&&\\left[y_i \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i) + \\boldsymbol\\theta'\\mathbf{x}_ig(\\boldsymbol\\theta'\\mathbf{x}_i)}{g(\\boldsymbol\\theta'\\mathbf{x}_i)^2} + (1-y_i) \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i) - \\boldsymbol\\theta'\\mathbf{x}_i (1 - g(\\boldsymbol\\theta'\\mathbf{x}_i))}{(1-g(\\boldsymbol\\theta'\\mathbf{x}_i))^2}\\right].\n\\end{eqnarray*}\\]Logit case (see Table 7.1), can shown :\n\\[\n\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = - \\sum_{=1}^{n} g'(\\boldsymbol\\theta'\\mathbf{x}_i) \\mathbf{x}_i\\mathbf{x}_i',\n\\]\n\\(g'(x)=\\dfrac{\\exp(-x)}{(1 + \\exp(-x))^2}\\).Remark , since \\(g'(x)>0\\), \\(-\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})/\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'\\) positive definite.","code":""},{"path":"binary-choice-models.html","id":"marginalFX","chapter":"7 Binary-choice models","heading":"7.4 Marginal effects","text":"measure marginal effects, .e. effect probability \\(y_i=1\\) marginal increase \\(x_{,k}\\)? object given :\n\\[\n\\frac{\\partial \\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta)}{\\partial x_{,k}} = \\underbrace{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}_{>0}\\theta_k,\n\\]\nsign \\(\\theta_k\\) function \\(g\\) monotonously increasing.agent \\(\\), marginal effect consistently estimated \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\). important see marginal effect depends \\(\\mathbf{x}_i\\): respective increases 1 unit \\(x_{,k}\\) (entity \\(\\)) \\(x_{j,k}\\) (entity \\(j\\)) necessarily effect \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta)\\) \\(\\mathbb{P}(y_j=1|\\mathbf{x}_j;\\boldsymbol\\theta)\\). address issue, one can compute measures “average” marginal effect. two main solutions. explanatory variable \\(k\\):Denoting \\(\\hat{\\mathbf{x}}\\) sample average \\(\\mathbf{x}_i\\)s, compute \\(g'(\\boldsymbol\\theta_{MLE}'\\hat{\\mathbf{x}})\\theta_{MLE,k}\\).Compute average (across \\(\\)) \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\).","code":""},{"path":"binary-choice-models.html","id":"goodness-of-fit-1","chapter":"7 Binary-choice models","heading":"7.5 Goodness of fit","text":"obvious version “\\(R^2\\)” binary-choice models. Existing measures called pseudo-\\(R^2\\) measures.Denoting \\(\\log \\mathcal{L}_0(\\mathbf{y})\\) (maximum) log-likelihood obtained model containing constant term (.e. \\(\\mathbf{x}_i = 1\\) \\(\\)), McFadden’s pseudo-\\(R^2\\) given :\n\\[\nR^2_{MF} = 1 - \\frac{\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\log \\mathcal{L}_0(\\mathbf{y})}.\n\\]\nIntuitively, \\(R^2_{MF}=0\\) explanatory variables convey information outcome \\(y\\). Indeed, case, model better reference model, simply captures fraction \\(y_i\\)’s equal 1.Example 7.3  (Credit defaults (Lending-club dataset)) example makes use credit data package AEC. objective model default probabilities borrowers.Let us first represent relationship fraction households defaulted loan annual income:previous figure suggests effect annual income probability default non-monotonous. therefore include quadratic term one specification (namely eq1 ).consider three specifications. first one (eq0), explanatory variables, trivial. just used compute pseudo-\\(R^2\\). second (eq1), consider covariates (loan amount, ratio amount annual income, number --30 days past-due incidences delinquency borrower’s credit file past 2 years, quadratic function annual income). third model (eq2), add credit rating.Let us compute pseudo R2 last two models:Let us now compute (average) marginal effects, using method ii Section 7.4:issue annual_inc variable. Indeed, previous computation realize variable appears twice among explanatory variables (log(annual_inc) (log(annual_inc)^2)). address , one can proceed follows: (1) construct new counterfactual dataset annual incomes increased 1%, (2) use model compute model-implied probabilities default new dataset (3), subtract probabilities resulting original dataset counterfactual probabilities:negative sign means , average across entities considered analysis, 1% increase annual income results decrease default probability. average effect however pretty low. get economic sense size effect, let us compute average effect associated unit increase number delinquencies:can employ likelihood ratio test (see Def. 6.8) see two variables associated annual income jointly statistically significant (context eq1):computation gives p-value 0.0436.Example 7.4  (Replicating Table 14.2 Cameron Trivedi (2005)) following lines codes replicate Table 14.2 Cameron Trivedi (2005) (see Example 7.2).","code":"\nlibrary(AEC)\ncredit$Default <- 0\ncredit$Default[credit$loan_status == \"Charged Off\"] <- 1\ncredit$Default[credit$loan_status ==\n                 \"Does not meet the credit policy. Status:Charged Off\"] <- 1\ncredit$amt2income <- credit$loan_amnt/credit$annual_inc\nplot(as.factor(credit$Default)~log(credit$annual_inc),\n     ylevels=2:1,ylab=\"Default status\",xlab=\"log(annual income)\")\neq0 <- glm(Default ~ 1,data=credit,family=binomial(link=\"probit\"))\neq1 <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs + \n             log(annual_inc)+ I(log(annual_inc)^2),\n           data=credit,family=binomial(link=\"probit\"))\neq2 <- glm(Default ~ grade + log(loan_amnt) + amt2income + delinq_2yrs + \n             log(annual_inc)+ I(log(annual_inc)^2),\n           data=credit,family=binomial(link=\"probit\"))\nstargazer::stargazer(eq0,eq1,eq2,type=\"text\",no.space = TRUE)## \n## ====================================================\n##                           Dependent variable:       \n##                     --------------------------------\n##                                 Default             \n##                        (1)        (2)        (3)    \n## ----------------------------------------------------\n## gradeB                                     0.400*** \n##                                            (0.055)  \n## gradeC                                     0.587*** \n##                                            (0.057)  \n## gradeD                                     0.820*** \n##                                            (0.061)  \n## gradeE                                     0.874*** \n##                                            (0.091)  \n## gradeF                                     1.230*** \n##                                            (0.147)  \n## gradeG                                     1.439*** \n##                                            (0.227)  \n## log(loan_amnt)                  -0.149**  -0.194*** \n##                                 (0.060)    (0.061)  \n## amt2income                      1.266***   1.222*** \n##                                 (0.383)    (0.393)  \n## delinq_2yrs                     0.096***    0.009   \n##                                 (0.034)    (0.035)  \n## log(annual_inc)                 -1.444**    -0.874  \n##                                 (0.569)    (0.586)  \n## I(log(annual_inc)2)             0.064**     0.038   \n##                                 (0.025)    (0.026)  \n## Constant            -1.231***   7.937***    4.749   \n##                      (0.017)    (3.060)    (3.154)  \n## ----------------------------------------------------\n## Observations          9,156      9,156      9,156   \n## Log Likelihood      -3,157.696 -3,120.625 -2,981.343\n## Akaike Inf. Crit.   6,317.392  6,253.250  5,986.686 \n## ====================================================\n## Note:                    *p<0.1; **p<0.05; ***p<0.01\nlogL0 <- logLik(eq0);logL1 <- logLik(eq1);logL2 <- logLik(eq2)\npseudoR2_eq1 <- 1 - logL1/logL0 # pseudo R2\npseudoR2_eq2 <- 1 - logL2/logL0 # pseudo R2\nc(pseudoR2_eq1,pseudoR2_eq2)## [1] 0.01173993 0.05584870\nmean(dnorm(predict(eq2)),na.rm=TRUE)*eq2$coefficients##          (Intercept)               gradeB               gradeC \n##          0.840731198          0.070747353          0.103944305 \n##               gradeD               gradeE               gradeF \n##          0.145089219          0.154773742          0.217702041 \n##               gradeG       log(loan_amnt)           amt2income \n##          0.254722161         -0.034289921          0.216251992 \n##          delinq_2yrs      log(annual_inc) I(log(annual_inc)^2) \n##          0.001574178         -0.154701321          0.006813694\nnew_credit <- credit\nnew_credit$annual_inc <- 1.01 * new_credit$annual_inc\nbas_predict_eq2  <- predict(eq2, newdata = credit, type = \"response\")\n# This is equivalent to pnorm(predict(eq2, newdata = credit))\nnew_predict_eq2  <- predict(eq2, newdata = new_credit, type = \"response\")\nmean(new_predict_eq2 - bas_predict_eq2)## [1] -6.562126e-05\nnew_credit <- credit\nnew_credit$delinq_2yrs <- credit$delinq_2yrs + 1\nnew_predict_eq2  <- predict(eq2, newdata = new_credit, type = \"response\")\nmean(new_predict_eq2 - bas_predict_eq2)## [1] 0.001582332\neq1restr <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs,\n                data=credit,family=binomial(link=\"probit\"))\nLRstat <- 2*(logL1 - logLik(eq1restr))\npvalue <- 1 - c(pchisq(LRstat,df=2))\ndata.reduced <- subset(Fishing,mode %in% c(\"charter\",\"pier\"))\ndata.reduced$lnrelp <- log(data.reduced$price.charter/data.reduced$price.pier)\ndata.reduced$y <- 1*(data.reduced$mode==\"charter\")\n# check first line of Table 14.1:\nprice.charter.y0 <- mean(data.reduced$pcharter[data.reduced$y==0])\nprice.charter.y1 <- mean(data.reduced$pcharter[data.reduced$y==1])\nprice.charter    <- mean(data.reduced$pcharter)\n# Run probit regression:\nreg.probit <- glm(y ~ lnrelp,\n                  data=data.reduced,\n                  family=binomial(link=\"probit\"))\n# Run Logit regression:\nreg.logit <- glm(y ~ lnrelp,\n                 data=data.reduced,\n                 family=binomial(link=\"logit\"))\n# Run OLS regression:\nreg.OLS <- lm(y ~ lnrelp,\n              data=data.reduced)\n# Replicates Table 14.2 of Cameron and Trivedi:\nstargazer::stargazer(reg.logit, reg.probit, reg.OLS,no.space = TRUE,\n                     type=\"text\")## \n## ================================================================\n##                                 Dependent variable:             \n##                     --------------------------------------------\n##                                          y                      \n##                     logistic   probit             OLS           \n##                        (1)       (2)              (3)           \n## ----------------------------------------------------------------\n## lnrelp              -1.823*** -1.056***        -0.243***        \n##                      (0.145)   (0.075)          (0.010)         \n## Constant            2.053***  1.194***          0.784***        \n##                      (0.169)   (0.088)          (0.013)         \n## ----------------------------------------------------------------\n## Observations           630       630              630           \n## R2                                               0.463          \n## Adjusted R2                                      0.462          \n## Log Likelihood      -206.827  -204.411                          \n## Akaike Inf. Crit.    417.654   412.822                          \n## Residual Std. Error                         0.330 (df = 628)    \n## F Statistic                             542.123*** (df = 1; 628)\n## ================================================================\n## Note:                                *p<0.1; **p<0.05; ***p<0.01"},{"path":"binary-choice-models.html","id":"predictions-and-roc-curves","chapter":"7 Binary-choice models","heading":"7.6 Predictions and ROC curves","text":"compute model-implied predicted outcomes? case \\(y_i\\), predicted outcomes \\(\\hat{y}_i\\) need valued \\(\\{0,1\\}\\). natural choice consists considering \\(\\hat{y}_i=1\\) \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) > 0.5\\), .e., taking cutoff \\(c=0.5\\). exist, though, situations relevant. instance, may models predicted probabilities small, less others. context, model-implied probability 10% (say) characterize “high-risk” entity. However, using cutoff 50% identify level riskiness.receiver operating characteristics (ROC) curve consitutes general approach. idea remain agnostic consider possible values cutoff \\(c\\). works follows. potential cutoff \\(c \\[0,1]\\), compute (plot):fraction \\(y = 1\\) values correctly classified (True Positive Rate) againstThe fraction \\(y = 0\\) values incorrectly specified (False Positive Rate).curve mechanically starts (0,0) —corresponds \\(c=1\\)— terminates (1,1) –situation \\(c=0\\).case predictive ability (worst situation), ROC curve straight line (0,0) (1,1).Example 7.5  (ROC fishing-mode dataset) Figure 7.5 shows ROC curve associated probit model estimated Example 7.4.\nFigure 7.5: Application ROC methodology fishing-mode dataset.\n","code":"\nlibrary(pROC)\npredict_model <- predict.glm(reg.probit,type = \"response\")\nroc(data.reduced$y, predict_model, percent=T,\n    boot.n=1000, ci.alpha=0.9, stratified=T, plot=TRUE, grid=TRUE,\n    show.thres=TRUE, legacy.axes = TRUE, reuse.auc = TRUE,\n    print.auc = TRUE, print.thres.col = \"blue\", ci=TRUE,\n    ci.type=\"bars\", print.thres.cex = 0.7, col = 'red',\n    main = paste(\"ROC curve using\",\"(N = \",nrow(data.reduced),\")\") )"},{"path":"TS.html","id":"TS","chapter":"8 Time Series","heading":"8 Time Series","text":"","code":""},{"path":"TS.html","id":"introduction-to-time-series","chapter":"8 Time Series","heading":"8.1 Introduction to time series","text":"time series infinite sequence random variables indexed time: \\(\\{y_t\\}_{t=-\\infty}^{+\\infty}=\\{\\dots, y_{-2},y_{-1},y_{0},y_{1},\\dots,y_t,\\dots\\}\\), \\(y_i \\\\mathbb{R}^k\\). practice, observe samples, typically: \\(\\{y_{1},\\dots,y_T\\}\\).Standard time series models built using shocks often denote \\(\\varepsilon_t\\). Typically, \\(\\mathbb{E}(\\varepsilon_t)=0\\). many models, shocks supposed ..d., exist (less restrictive) notions shocks. particular, definition many processes based whote noises:Definition 8.1  (White noise) process \\(\\{\\varepsilon_t\\}_{t \\] -\\infty,+\\infty[}\\) white noise , \\(t\\):\\(\\mathbb{E}(\\varepsilon_t)=0\\),\\(\\mathbb{E}(\\varepsilon_t^2)=\\sigma^2<\\infty\\) andfor \\(s\\ne t\\), \\(\\mathbb{E}(\\varepsilon_t \\varepsilon_s)=0\\).Another type shocks commonly used Martingale Difference Sequences:Definition 8.2  (Martingale Difference Sequence) process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) martingale difference sequence (MDS) \\(\\mathbb{E}(|\\varepsilon_{t}|)<\\infty\\) , \\(t\\),\n\\[\n\\underbrace{\\mathbb{E}_{t-1}(\\varepsilon_{t})}_{\\mbox{Expectation conditional past}}=0.\n\\]definition, \\(y_t\\) martingale, \\(y_{t}-y_{t-1}\\) MDS.Example 8.1  (ARCH process) Autoregressive conditional heteroskedasticity (ARCH) process example shock satisfies MDS definition ..d.:\n\\[\n\\varepsilon_{t} = \\sigma_t \\times z_{t},\n\\]\n\\(z_t \\sim ..d.\\,\\mathcal{N}(0,1)\\) \\(\\sigma_t^2 = w + \\alpha \\varepsilon_{t-1}^2\\).Example 8.2  white noise process necessarily MDS. instance following process:\n\\[\n\\varepsilon_{t} = z_t + z_{t-1}z_{t-2},\n\\]\n\\(z_t \\sim ..d.\\mathcal{N}(0,1)\\).Let us now introduce lag operator. lag operator, denoted \\(L\\), defined time series space defined :\n\\[\\begin{equation}\nL: \\{y_t\\}_{t=-\\infty}^{+\\infty} \\rightarrow \\{w_t\\}_{t=-\\infty}^{+\\infty} \\quad \\mbox{} \\quad w_t = y_{t-1}.\\tag{8.1}\n\\end{equation}\\]: \\(L^2 y_t = y_{t-2}\\) , generally, \\(L^k y_t = y_{t-k}\\).Consider time series \\(y_t\\) defined \\(y_t = \\mu + \\phi y_{t-1} + \\varepsilon_t\\), \\(\\varepsilon_t\\)’s ..d. \\(\\mathcal{N}(0,\\sigma^2)\\). Using lag operator, dynamics \\(y_t\\) can expressed follows:\n\\[\n(1-\\phi L) y_t = \\mu + \\varepsilon_t.\n\\]easily checked \\(L^2 y_t = y_{t-2}\\) , generally, \\(L^k y_t = y_{t-k}\\).exists, unconditional (marginal) mean random variable \\(y_t\\) given :\n\\[\n\\mu_t := \\mathbb{E}(y_t) = \\int_{-\\infty}^{\\infty} y_t f_{Y_t}(y_t) dy_t,\n\\]\n\\(f_{Y_t}\\) unconditional (marginal) density \\(y_t\\). Similarly, exists, unconditional (marginal) variance random variable \\(y_t\\) :\n\\[\n\\mathbb{V}ar(y_t) = \\int_{-\\infty}^{\\infty} (y_t - \\mathbb{E}(y_t))^2 f_{Y_t}(y_t) dy_t.\n\\]Definition 8.3  (Autocovariance) \\(j^{th}\\) autocovariance \\(y_t\\) given :\n\\[\\begin{eqnarray*}\n\\gamma_{j,t} &:=& \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} [y_t - \\mathbb{E}(y_t)][y_{t-j} - \\mathbb{E}(y_{t-j})] \\times\\\\\n&& f_{Y_t,Y_{t-1},\\dots,Y_{t-j}}(y_t,y_{t-1},\\dots,y_{t-j}) dy_t dy_{t-1} \\dots dy_{t-j} \\\\\n&=& \\mathbb{E}([y_t - \\mathbb{E}(y_t)][y_{t-j} - \\mathbb{E}(y_{t-j})]),\n\\end{eqnarray*}\\]\n\\(f_{Y_t,Y_{t-1},\\dots,Y_{t-j}}(y_t,y_{t-1},\\dots,y_{t-j})\\) joint distribution \\(y_t,y_{t-1},\\dots,y_{t-j}\\).particular, \\(\\gamma_{0,t} = \\mathbb{V}ar(y_t)\\).Definition 8.4  (Covariance stationarity) process \\(y_t\\) covariance stationary —weakly stationary— , \\(t\\) \\(j\\),\n\\[\n\\mathbb{E}(y_t) = \\mu \\quad \\mbox{} \\quad \\mathbb{E}\\{(y_t - \\mu)(y_{t-j} - \\mu)\\} = \\gamma_j.\n\\]Figure 8.1 displays simulation process covariance stationary. process follows \\(y_t = 0.1t + \\varepsilon_t\\), \\(\\varepsilon_t \\sim\\,..d.\\,\\mathcal{N}(0,1)\\). Indeed, process, : \\(\\mathbb{E}(y_t)=0.1t\\), depends \\(t\\).\nFigure 8.1: Example process covariance stationary (\\(y_t = 0.1t + \\varepsilon_t\\), \\(\\varepsilon_t \\sim \\mathcal{N}(0,1)\\)).\nDefinition 8.5  (Strict stationarity) process \\(y_t\\) strictly stationary , \\(t\\) sets integers \\(J=\\{j_1,\\dots,j_n\\}\\), distribution \\((y_{t},y_{t+j_1},\\dots,y_{t+j_n})\\) depends \\(J\\) \\(t\\).following process covariance stationary strictly stationary:\n\\[\ny_t = \\mathbb{}_{\\{t<1000\\}}\\varepsilon_{1,t}+\\mathbb{}_{\\{t\\ge1000\\}}\\varepsilon_{2,t},\n\\]\n\\(\\varepsilon_{1,t} \\sim \\mathcal{N}(0,1)\\) \\(\\varepsilon_{2,t} \\sim \\sqrt{\\frac{\\nu - 2}{\\nu}} t(\\nu)\\) \\(\\nu = 4\\).\nFigure 8.2: Example process covariance stationary strictly stationary. red lines delineate 99% confidence interval standard normal distribution (\\(\\pm 2.58\\)).\nProposition 8.1  \\(y_t\\) covariance stationary, \\(\\gamma_j = \\gamma_{-j}\\).Proof. Since \\(y_t\\) covariance stationary, covariance \\(y_t\\) \\(y_{t-j}\\) (.e \\(\\gamma_j\\)) \\(y_{t+j}\\) \\(y_{t+j-j}\\) (.e. \\(\\gamma_{-j}\\)).Definition 8.6  (Auto-correlation) \\(j^{th}\\) auto-correlation covariance-stationary process :\n\\[\n\\rho_j = \\frac{\\gamma_j}{\\gamma_0}.\n\\]Consider long historical time series Swiss GDP growth, taken Jordà, Schularick, Taylor (2017) dataset.18\nFigure 8.3: Annual growth rate Swiss GDP, based Jorda-Schularick-Taylor Macrohistory Database.\n\nFigure 8.4: order \\(j\\), slope blue line , approximately, \\(\\hat{\\gamma}_j/\\widehat{\\mathbb{V}ar}(y_t)\\), hats indicate sample moments.\nDefinition 8.7  (Mean ergodicity) covariance-stationary process \\(y_t\\) ergodic mean :\n\\[\n\\mbox{plim}_{T \\rightarrow +\\infty} \\frac{1}{T}\\sum_{t=1}^T y_t = \\mathbb{E}(y_t).\n\\]Definition 8.8  (Second-moment ergodicity) covariance-stationary process \\(y_t\\) ergodic second moments , \\(j\\):\n\\[\n\\mbox{plim}_{T \\rightarrow +\\infty} \\frac{1}{T}\\sum_{t=1}^T (y_t-\\mu) (y_{t-j}-\\mu) = \\gamma_j.\n\\]noted ergodicity stationarity different properties. Typically process \\(\\{x_t\\}\\) , \\(\\forall t\\), \\(x_t \\equiv y\\), \\(y \\sim\\,\\mathcal{N}(0,1)\\) (say), \\(\\{x_t\\}\\) stationary ergodic.Theorem 8.1  (Central Limit Theorem covariance-stationary processes) process \\(y_t\\) covariance stationary series autocovariances absolutely summable (\\(\\sum_{j=-\\infty}^{+\\infty} |\\gamma_j| <\\infty\\)), :\n\\[\\begin{eqnarray}\n\\bar{y}_T \\overset{m.s.}{\\rightarrow} \\mu &=& \\mathbb{E}(y_t) \\tag{8.2}\\\\\n\\mbox{lim}_{T \\rightarrow +\\infty} T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] &=& \\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\tag{8.3}\\\\\n\\sqrt{T}(\\bar{y}_T - \\mu) &\\overset{d}{\\rightarrow}& \\mathcal{N}\\left(0,\\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\right) \\tag{8.4}.\n\\end{eqnarray}\\][Mean square (m.s.) distribution (d.) convergences: see Definitions 9.17 9.15.]Proof. Proposition 9.8, Eq. (8.3) implies Eq. (8.2). Eq. (8.3), see Appendix 9.5. Eq. (8.4), see Anderson (1971), p. 429.Definition 8.9  (Long-run variance) assumptions Theorem 8.1, limit appearing Eq. (8.3) exists called long-run variance. denoted \\(S\\), .e.:\n\\[\nS = \\Sigma_{j=-\\infty}^{+\\infty} \\gamma_j  = \\mbox{lim}_{T \\rightarrow +\\infty} T \\mathbb{E}[(\\bar{y}_T - \\mu)^2].\n\\]\\(y_t\\) ergodic second moments (see Def. 8.8), natural estimator \\(S\\) :\n\\[\\begin{equation}\n\\hat\\gamma_0 + 2 \\sum_{\\nu=1}^{q} \\hat\\gamma_\\nu, \\tag{8.5}\n\\end{equation}\\]\n\\(\\hat\\gamma_\\nu = \\frac{1}{T}\\sum_{\\nu+1}^{T} (y_t - \\bar{y})(y_{t-\\nu} - \\bar{y})\\).However, small samples, Eq. (8.5) necessarily result positive definite matrix. Newey West (1987) proposed estimator defect. estimator given :\n\\[\\begin{equation}\nS^{NW}=\\hat\\gamma_0 + 2 \\sum_{\\nu=1}^{q}\\left(1-\\frac{\\nu}{q+1}\\right) \\hat\\gamma_\\nu.\\tag{8.6}\n\\end{equation}\\]Loosely speaking, Theorem 8.1 says , given sample size, higher “persistency” process, lower accuracy sample mean estimate population mean. illustrate, consider three processes feature marginal variance (equal one, say), different autocorrelations: 0%, 70%, 99.9%. Figure 8.5 displays simulated paths three processes. indeed appears , larger autocorrelation process, sample mean (dashed red line) population mean (red solid line).type simulations can performed using ShinyApp (use panel “AR(1) Simulation”).\nFigure 8.5: three samples simulated using following data generating process: \\(x_t = \\mu + \\rho (x_{t-1}-\\mu) + \\sqrt{1-\\rho^2}\\varepsilon_t\\), \\(\\varepsilon_t \\sim \\mathcal{N}(0,1)\\). Case : \\(\\rho = 0\\); Case B: \\(\\rho = 0.7\\); Case C: \\(\\rho = 0.999\\). three cases, \\(\\mathbb{E}(x_t)=\\mu=2\\) \\(\\mathbb{V}ar(x_t)=1\\).\n","code":""},{"path":"TS.html","id":"univariate-processes","chapter":"8 Time Series","heading":"8.2 Univariate processes","text":"","code":""},{"path":"TS.html","id":"moving-average-ma-processes","chapter":"8 Time Series","heading":"8.2.1 Moving Average (MA) processes","text":"Definition 8.10  Consider white noise process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) (Def. 8.1). \\(y_t\\) first-order moving average process , \\(t\\):\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta \\varepsilon_{t-1}.\n\\]\\(\\mathbb{E}(\\varepsilon_t^2)=\\sigma^2\\), easily obtained unconditional mean variances \\(y_t\\) :\n\\[\n\\mathbb{E}(y_t) = \\mu, \\quad \\mathbb{V}ar(y_t) = (1+\\theta^2)\\sigma^2.\n\\]first auto-covariance :\n\\[\n\\gamma_1=\\mathbb{E}\\{(y_t - \\mu)(y_{t-1} - \\mu)\\} = \\theta \\sigma^2.\n\\]Higher-order auto-covariances zero (\\(\\gamma_j=0\\) \\(j>1\\)). Therefore: MA(1) process covariance-stationary (Def. 8.4).MA(1) process, autocorrelation order \\(j\\) (see Def. 8.6) given :\n\\[\n\\rho_j =\n\\left\\{\n\\begin{array}{lll}\n1 &\\mbox{ }& j=0,\\\\\n\\theta / (1 + \\theta^2) &\\mbox{ }& j = 1\\\\\n0 &\\mbox{ }& j>1.\n\\end{array}\n\\right.\n\\]Notice process \\(y_t\\) defined :\n\\[\ny_t = \\mu + \\varepsilon_t +\\theta \\varepsilon_{t-1},\n\\]\n\\(\\mathbb{V}ar(\\varepsilon_t)=\\sigma^2\\), mean autocovariances \n\\[\ny_t = \\mu + \\varepsilon^*_t +\\frac{1}{\\theta}\\varepsilon^*_{t-1},\n\\]\n\\(\\mathbb{V}ar(\\varepsilon^*_t)=\\theta^2\\sigma^2\\). , even perfectly know mean auto-covariances process, possible identify specification one used generate data. one two specifications said fundamental, one satisfies \\(|\\theta_1|<1\\).Definition 8.11  (MA(q) process) \\(q^{th}\\) order Moving Average process defined :\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q}.\n\\]\n\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (Def. 8.1).Proposition 8.2  (Covariance-stationarity MA(q) process) Finite-order Moving Average processes covariance-stationary.Moreover, autocovariances MA(q) process (defined Def. 8.11) given :\n\\[\\begin{equation}\n\\gamma_j = \\left\\{ \\begin{array}{ll} \\sigma^2(\\theta_j\\theta_0 + \\theta_{j+1}\\theta_{1} +  \\dots + \\theta_{q}\\theta_{q-j}) &\\mbox{} \\quad j \\\\{0,\\dots,q\\} \\\\ 0 &\\mbox{} \\quad j>q, \\end{array} \\right.\\tag{8.7}\n\\end{equation}\\]\nuse notation \\(\\theta_0=1\\), \\(\\mathbb{V}ar(\\varepsilon_t)=\\sigma^2\\).Proof. unconditional expectation \\(y_t\\) depend time, since \\(\\mathbb{E}(y_t)=\\mu\\). Let’s turn autocovariances. can extend series \\(\\theta_j\\)’s setting \\(\\theta_j=0\\) \\(j>q\\). :\n\\[\\begin{eqnarray*}\n\\mathbb{E}((y_t-\\mu)(y_{t-j}-\\mu)) &=& \\mathbb{E}\\left[(\\theta_0 \\varepsilon_t +\\theta_1 \\varepsilon_{t-1} + \\dots +\\theta_j \\varepsilon_{t-j}+\\theta_{j+1} \\varepsilon_{t-j-1} + \\dots) \\right.\\times \\\\\n&&\\left. (\\theta_0 \\varepsilon_{t-j} +\\theta_1 \\varepsilon_{t-j-1} + \\dots)\\right].\n\\end{eqnarray*}\\]\nuse fact \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_s)=0\\) \\(t \\ne s\\) (\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process).Figure 8.6 displays simulated paths two MA processes (MA(1) MA(4)). simulations can produced using panel “ARMA(p,q)” web interface.\nFigure 8.6: Simulation MA processes.\norder \\(q\\) MA(q) process gets infinite? notion infinite-order Moving Average process exists important time series analysis. (infinite) sequence \\(\\theta_j\\) satisfy conditions process well-defined (see Theorem 8.2 ). conditions relate “summability” \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) (see Definition 8.12).Definition 8.12  (Absolute square summability) sequence \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) absolutely summable \\(\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty\\), square summable \\(\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty\\).According Prop. 9.8, absolute summability implies square summability.Theorem 8.2  (Existence condition infinite MA process) \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) square summable (see Def. 8.12) \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (see Def. 8.1), \n\\[\n\\mu + \\sum_{=0}^{+\\infty} \\theta_{} \\varepsilon_{t-}\n\\]\ndefines well-behaved [covariance-stationary] process, called infinite-order MA process (MA(\\(\\infty\\))).Proof. See Appendix 3.Hamilton. “Well behaved” means \\(\\Sigma_{=0}^{T} \\theta_{t-} \\varepsilon_{t-}\\) converges mean square (Def. 9.15) random variable \\(Z_t\\). proof makes use fact :\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N}^{M}\\theta_{} \\varepsilon_{t-}\\right)^2\\right] = \\sum_{=N}^{M}|\\theta_{}|^2 \\sigma^2,\n\\]\n, \\(\\{\\theta_{}\\}\\) square summable, \\(\\forall \\eta>0\\), \\(\\exists N\\) s.t. right-hand-side term last equation lower \\(\\eta\\) \\(M \\ge N\\) (static Cauchy criterion, Theorem 9.2). implies \\(\\Sigma_{=0}^{T} \\theta_{} \\varepsilon_{t-}\\) converges mean square (stochastic Cauchy criterion, see Theorem 9.3).Proposition 8.3  (First two moments infinite MA process) \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) absolutely summable, .e. \\(\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty\\), \\(y_t = \\mu + \\sum_{=0}^{+\\infty} \\theta_{} \\varepsilon_{t-}\\) exists (Theorem 8.2) :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y_t) &=& \\mu\\\\\n\\gamma_0 = \\mathbb{E}([y_t-\\mu]^2) &=& \\sigma^2(\\theta_0^2 +\\theta_1^2 + \\dots)\\\\\n\\gamma_j = \\mathbb{E}([y_t-\\mu][y_{t-j}-\\mu]) &=& \\sigma^2(\\theta_0\\theta_j + \\theta_{1}\\theta_{j+1} + \\dots).\n\\end{eqnarray*}\\]Process \\(y_t\\) absolutely summable auto-covariances, implies results Theorem 8.1 (Central Limit) apply.Proof. absolute summability \\(\\{\\theta_{}\\}\\) fact \\(\\mathbb{E}(\\varepsilon^2)<\\infty\\) imply order integration summation interchangeable (see Hamilton, 1994, Footnote p. 52), proves (). (ii), see end Appendix 3.Hamilton (1994).","code":"\nlibrary(AEC)\nT <- 100;nb.sim <- 1\ny.0 <- c(0)\nc <- 1;phi <- c(0);sigma <- 1\ntheta <- c(1,1) # MA(1) specification\ny.sim <- sim.arma(c,phi,theta,sigma,T,y.0,nb.sim)\npar(mfrow=c(1,2))\npar(plt=c(.2,.9,.2,.85))\nplot(y.sim[,1],xlab=\"\",ylab=\"\",type=\"l\",lwd=2,\n     main=expression(paste(theta[0],\"=1, \",theta[1],\"=1\",sep=\"\")))\nabline(h=c)\ntheta <- c(1,1,1,1,1) # MA(4) specification\ny.sim <- sim.arma(c,phi,theta,sigma,T,y.0,nb.sim)\nplot(y.sim[,1],xlab=\"\",ylab=\"\",type=\"l\",lwd=2,\n     main=expression(paste(theta[0],\"=...=\",theta[4],\"=1\",sep=\"\")))\nabline(h=c)"},{"path":"TS.html","id":"ARsection","chapter":"8 Time Series","heading":"8.2.2 Auto-Regressive (AR) processes","text":"Definition 8.13  (First-order AR process (AR(1))) Consider white noise process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) (see Def. 8.1). Process \\(y_t\\) AR(1) process defined following difference equation:\n\\[\ny_t = c + \\phi y_{t-1} + \\varepsilon_t.\n\\]kind process can simulated using panel “AR(1) Simulation” web interface.\\(|\\phi|\\ge1\\), \\(y_t\\) stationary. Indeed, :\n\\[\ny_{t+k} = c + \\varepsilon_{t+k} + \\phi  ( c + \\varepsilon_{t+k-1})+ \\phi^2  ( c + \\varepsilon_{t+k-2})+ \\dots + \\phi^{k-1}  ( c + \\varepsilon_{t+1}) + \\phi^k y_t.\n\\]\nTherefore, conditional variance\n\\[\n\\mathbb{V}ar_t(y_{t+k}) = \\sigma^2(1 + \\phi^2 + \\phi^4 + \\dots + \\phi^{2(k-1)})\n\\]\nconverge large \\(k\\)’s. implies \\(\\mathbb{V}ar(y_{t})\\) exist.contrast, \\(|\\phi| < 1\\), one can see :\n\\[\ny_t = c + \\varepsilon_t + \\phi  ( c + \\varepsilon_{t-1})+ \\phi^2  ( c + \\varepsilon_{t-2})+ \\dots + \\phi^k  ( c + \\varepsilon_{t-k}) + \\dots\n\\]\nHence, \\(|\\phi| < 1\\), unconditional mean variance \\(y_t\\) :\n\\[\n\\mathbb{E}(y_t) = \\frac{c}{1-\\phi} =: \\mu \\quad \\mbox{} \\quad \\mathbb{V}ar(y_t) = \\frac{\\sigma^2}{1-\\phi^2}.\n\\]Let us compute \\(j^{th}\\) autocovariance AR(1) process:\n\\[\\begin{eqnarray*}\n\\mathbb{E}([y_{t} - \\mu][y_{t-j} - \\mu]) &=& \\mathbb{E}([\\varepsilon_t + \\phi  \\varepsilon_{t-1}+ \\phi^2 \\varepsilon_{t-2} + \\dots + \\color{red}{\\phi^j \\varepsilon_{t-j}} + \\color{blue}{\\phi^{j+1} \\varepsilon_{t-j-1}} \\dots]\\times \\\\\n&&[\\color{red}{\\varepsilon_{t-j}} + \\color{blue}{\\phi \\varepsilon_{t-j-1}} + \\phi^2 \\varepsilon_{t-j-2} + \\dots + \\phi^k \\varepsilon_{t-j-k} + \\dots])\\\\\n&=& \\mathbb{E}(\\color{red}{\\phi^j \\varepsilon_{t-j}^2}+\\color{blue}{\\phi^{j+2} \\varepsilon_{t-j-1}^2}+\\phi^{j+4} \\varepsilon_{t-j-2}^2+\\dots)\\\\\n&=& \\frac{\\phi^j \\sigma^2}{1 - \\phi^2}.\n\\end{eqnarray*}\\]Therefore \\(\\rho_j = \\phi^j\\).precedes, :Proposition 8.4  (Covariance-stationarity AR(1) process) AR(1) process, defined Def. 8.13, covariance-stationary iff \\(|\\phi|<1\\).Definition 8.14  (AR(p) process) Consider white noise process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) (see Def. 8.1). Process \\(y_t\\) \\(p^{th}\\)-order autoregressive process (AR(p)) dynamics defined following difference equation (\\(\\phi_p \\ne 0\\)):\n\\[\\begin{equation}\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t.\\tag{8.8}\n\\end{equation}\\]see, covariance-stationarity process \\(y_t\\) hinges matrix \\(F\\) defined :\n\\[\\begin{equation}\nF = \\left[\n\\begin{array}{ccccc}\n\\phi_1 & \\phi_2 & \\dots& & \\phi_p \\\\\n1 & 0 &\\dots && 0 \\\\\n0 & 1 &\\dots && 0 \\\\\n\\vdots &  & \\ddots && \\vdots \\\\\n0 & 0 &\\dots &1& 0 \\\\\n\\end{array}\n\\right].\\tag{8.9}\n\\end{equation}\\]Note matrix \\(F\\) \\(y_t\\) follows Eq. (8.8), process \\(\\mathbf{y}_t\\) follows:\n\\[\n\\mathbf{y}_t = \\mathbf{c} + F \\mathbf{y}_{t-1} + \\boldsymbol\\xi_t\n\\]\n\n\\[\n\\mathbf{c} =\n\\left[\\begin{array}{c}\nc\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right],\n\\quad\n\\boldsymbol\\xi_t =\n\\left[\\begin{array}{c}\n\\varepsilon_t\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right],\n\\quad\n\\mathbf{y}_t =\n\\left[\\begin{array}{c}\ny_t\\\\\ny_{t-1}\\\\\n\\vdots\\\\\ny_{t-p+1}\n\\end{array}\\right].\n\\]Proposition 8.5  (eigenvalues matrix F) eigenvalues \\(F\\) (defined Eq. (8.9)) solutions :\n\\[\\begin{equation}\n\\lambda^p - \\phi_1 \\lambda^{p-1} - \\dots - \\phi_{p-1}\\lambda - \\phi_p = 0.\\tag{8.10}\n\\end{equation}\\]Proposition 8.6  (Covariance-stationarity AR(p) process) four statements equivalent:Process \\(\\{y_t\\}\\), defined Def. 8.14, covariance-stationary.eigenvalues \\(F\\) (defined Eq. (8.9)) lie strictly within unit circle.roots Eq. (8.11) () lie strictly outside unit circle.\n\\[\\begin{equation}\n1 - \\phi_1 z - \\dots - \\phi_{p-1}z^{p-1} - \\phi_p z^p = 0.\\tag{8.11}\n\\end{equation}\\]roots Eq. (8.12) () lie strictly inside unit circle.\n\\[\\begin{equation}\n\\lambda^p - \\phi_1 \\lambda^{p-1} - \\dots - \\phi_{p-1}\\lambda - \\phi_p = 0.\\tag{8.12}\n\\end{equation}\\]Proof. consider case eigenvalues \\(F\\) distinct; Jordan decomposition can used general case. eigenvalues \\(F\\) distinct, \\(F\\) admits following spectral decomposition: \\(F = PDP^{-1}\\), \\(D\\) diagonal. Using notations introduced Eq. (8.9), :\n\\[\n\\mathbf{y}_{t} = \\mathbf{c} + F \\mathbf{y}_{t-1} + \\boldsymbol\\xi_{t}.\n\\]\nLet’s introduce \\(\\mathbf{d} = P^{-1}\\mathbf{c}\\), \\(\\mathbf{z}_t = P^{-1}\\mathbf{y}_t\\) \\(\\boldsymbol\\eta_t = P^{-1}\\boldsymbol\\xi_t\\). :\n\\[\n\\mathbf{z}_{t} = \\mathbf{d} + D \\mathbf{z}_{t-1} + \\boldsymbol\\eta_{t}.\n\\]\n\\(D\\) diagonal, different component \\(\\mathbf{z}_t\\), denoted \\(z_{,t}\\), follow AR(1) processes. (scalar) autoregressive parameters AR(1) processes diagonal entries \\(D\\) –also eigenvalues \\(F\\)– denote \\(\\lambda_i\\).Process \\(y_t\\) covariance-stationary iff \\(\\mathbf{y}_{t}\\) also covariance-stationary, case iff \\(z_{,t}\\), \\(\\[1,p]\\), covariance-stationary. Prop. 8.4, process \\(z_{,t}\\) covariance-stationary iff \\(|\\lambda_i|<1\\). proves () equivalent (ii). Prop. 8.5 proves (ii) equivalent (iv). Finally, easily seen (iii) equivalent (iv) (long \\(\\phi_p \\ne 0\\)).Using lag operator (see Eq (8.1)), \\(y_t\\) covariance-stationary AR(p) process (Def. 8.14), can write:\n\\[\ny_t = \\mu + \\psi(L)\\varepsilon_t,\n\\]\n\n\\[\\begin{equation}\n\\psi(L) = (1 - \\phi_1 L - \\dots - \\phi_p L^p)^{-1},\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\mu = \\mathbb{E}(y_t) = \\dfrac{c}{1-\\phi_1 -\\dots - \\phi_p}.\\tag{8.13}\n\\end{equation}\\]following lines codes, compute eigenvalues \\(F\\) matrices associated following processes (\\(\\varepsilon_t\\) white noise):\n\\[\\begin{eqnarray*}\nx_t &=& 0.9 x_{t-1} -0.2 x_{t-2} + \\varepsilon_t\\\\\ny_t &=& 1.1 y_{t-1} -0.3 y_{t-2} + \\varepsilon_t\\\\\nw_t &=& 1.4 w_{t-1} -0.7 w_{t-2} + \\varepsilon_t\\\\\nz_t &=& 0.9 z_{t-1} +0.2 z_{t-2} + \\varepsilon_t\n\\end{eqnarray*}\\]absolute values eigenvalues associated process \\(w_t\\) equal 0.837. Therefore, according Proposition 8.6, processes \\(x_t\\), \\(y_t\\), \\(w_t\\) covariance-stationary, \\(z_t\\) (absolute value one eigenvalues \\(F\\) matrix associated process larger 1).computation autocovariances \\(y_t\\) based -called Yule-Walker equations (Eq. (8.14)). Let’s rewrite Eq. (8.8):\n\\[\n(y_t-\\mu) = \\phi_1 (y_{t-1}-\\mu) + \\phi_2 (y_{t-2}-\\mu) + \\dots + \\phi_p (y_{t-p}-\\mu) + \\varepsilon_t.\n\\]\nMultiplying sides \\(y_{t-j}-\\mu\\) taking expectations leads (Yule-Walker) equations:\n\\[\\begin{equation}\n\\gamma_j = \\left\\{\n\\begin{array}{l}\n\\phi_1 \\gamma_{j-1}+\\phi_2 \\gamma_{j-2}+ \\dots + \\phi_p \\gamma_{j-p} \\quad \\quad j>0\\\\\n\\phi_1 \\gamma_{1}+\\phi_2 \\gamma_{2}+ \\dots + \\phi_p \\gamma_{p} + \\sigma^2 \\quad \\quad j=0.\n\\end{array}\n\\right.\\tag{8.14}\n\\end{equation}\\]\nUsing \\(\\gamma_j = \\gamma_{-j}\\) (Prop. 8.1), one can express \\((\\gamma_0,\\gamma_1,\\dots,\\gamma_{p})\\) functions \\((\\sigma^2,\\phi_1,\\dots,\\phi_p)\\).","code":"\nF <- matrix(c(.9,1,-.2,0),2,2)\nlambda_x <- eigen(F)$values\nF[1,] <- c(1.1,-.3)\nlambda_y <- eigen(F)$values\nF[1,] <- c(1.4,-.7)\nlambda_w <- eigen(F)$values\nF[1,] <- c(.9,.2)\nlambda_z <- eigen(F)$values\nrbind(lambda_x,lambda_y,lambda_w,lambda_z)##                         [,1]                  [,2]\n## lambda_x 0.500000+0.0000000i  0.4000000+0.0000000i\n## lambda_y 0.600000+0.0000000i  0.5000000+0.0000000i\n## lambda_w 0.700000+0.4582576i  0.7000000-0.4582576i\n## lambda_z 1.084429+0.0000000i -0.1844289+0.0000000i"},{"path":"TS.html","id":"PACFapproach","chapter":"8 Time Series","heading":"8.2.3 PACF approach to identify AR/MA processes","text":"seen \\(k^{th}\\)-order auto-correlation MA(q) process null \\(k>q\\). exploited, practice, determine order MA process. Moreover, since case AR process, can used distinguish AR MA process.exists equivalent approach determine whether process can modeled AR process; based partial auto-correlations:Definition 8.15  (Partial auto-correlation) time series context, partial auto-correlation (\\(\\phi_{h,h}\\)) process \\(\\{y_t\\}\\) defined partial correlation \\(y_{t+h}\\) \\(y_t\\) given \\(y_{t+h-1},\\dots,y_{t+1}\\). (see Def. 9.5 definition partial correlation.)\\(h>p\\), regression \\(y_{t+h}\\) \\(y_{t+h-1},\\dots,y_{t+1}\\) :\n\\[\ny_{t+h} = c + \\phi_1 y_{t+h-1}+\\dots+ \\phi_p  y_{t+h-p} + \\varepsilon_{t+h}.\n\\]\nresiduals latter regressions (\\(\\varepsilon_{t+h}\\)) uncorrelated \\(y_t\\). partial autocorrelation zero \\(h>p\\).Besides, can shown \\(\\phi_{p,p}=\\phi_p\\). Hence \\(\\phi_{p,p}=\\phi_p\\) \\(\\phi_{h,h}=0\\) \\(h>p\\). can used determine order AR process. contrast (importantly) \\(y_t\\) follows MA(q) process, \\(\\phi_{k,k}\\) asymptotically approaches zero instead cutting abruptly.illustrated , functions acf pacf can conveniently used employ (P)ACF approach. (Note also use function sim.arma simulate ARMA processes.)\nFigure 8.7: ACF/PACF analysis two processes (MA process left, AR right).\n","code":"\nlibrary(AEC)\npar(mfrow=c(3,2))\npar(plt=c(.2,.9,.2,.95))\ntheta <- c(1,2,1);phi=0\ny.sim <- sim.arma(c=0,phi,theta,sigma=1,T=1000,y.0=0,nb.sim=1)\npar(mfg=c(1,1));plot(y.sim,type=\"l\",lwd=2)\npar(mfg=c(2,1));acf(y.sim)\npar(mfg=c(3,1));pacf(y.sim)\ntheta <- c(1);phi=0.9\ny.sim <- sim.arma(c=0,phi,theta,sigma=1,T=1000,y.0=0,nb.sim=1)\npar(mfg=c(1,2));plot(y.sim,type=\"l\",lwd=2)\npar(mfg=c(2,2));acf(y.sim)\npar(mfg=c(3,2));pacf(y.sim)"},{"path":"TS.html","id":"forecasting","chapter":"8 Time Series","heading":"8.3 Forecasting","text":"Forecasting always important part time series field (De Gooijer Hyndman (2006)). Macroeconomic forecasts done many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. institutions interested point estimates (\\(\\sim\\) likely value) variable interest. also sometimes need measure uncertainty (\\(\\sim\\) dispersion likely outcomes) associated point estimates.19Forecasts produced professional forecasters available web pages:Philly Fed Survey Professional Forecasters.ECB Survey Professional Forecasters.IMF World Economic Outlook.OECD Global Economic Outlook.European Commission Economic Forecasts.formalize forecasting problem? Assume current date \\(t\\). want forecast value variable \\(y_t\\) take date \\(t+1\\) (.e., \\(y_{t+1}\\)) based observation set variables gathered vector \\(x_t\\) (\\(x_t\\) may contain lagged values \\(y_t\\)).forecaster aims minimizing (function ) forecast error. usal consider following (quadratic) loss function:\n\\[\n\\underbrace{\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)}_{\\mbox{Mean square error (MSE)}}\n\\]\n\\(y^*_{t+1}\\) forecast \\(y_{t+1}\\) (function \\(x_t\\)).Proposition 8.7  (Smallest MSE) smallest MSE obtained MSEthe expectation \\(y_{t+1}\\) conditional \\(x_t\\).Proof. See Appendix 9.5.Proposition 8.8  Among class linear forecasts, smallest MSE obtained linear projection \\(y_{t+1}\\) \\(x_t\\).\nprojection, denoted \\(\\hat{P}(y_{t+1}|x_t):=\\boldsymbol\\alpha'x_t\\), satisfies:\n\\[\\begin{equation}\n\\mathbb{E}\\left( [y_{t+1} - \\boldsymbol\\alpha'x_t]x_t \\right)=\\mathbf{0}.\\tag{8.15}\n\\end{equation}\\]Proof. Consider function \\(f:\\) \\(\\boldsymbol\\alpha \\rightarrow \\mathbb{E}\\left( [y_{t+1} - \\boldsymbol\\alpha'x_t]^2 \\right)\\). :\n\\[\nf(\\boldsymbol\\alpha) = \\mathbb{E}\\left( y_{t+1}^2 - 2 y_t x_t'\\boldsymbol\\alpha + \\boldsymbol\\alpha'x_t x_t'\\boldsymbol\\alpha] \\right).\n\\]\n\\(\\partial f(\\boldsymbol\\alpha)/\\partial \\boldsymbol\\alpha = \\mathbb{E}(-2 y_{t+1} x_t + 2 x_t x_t'\\boldsymbol\\alpha)\\). function minimised \\(\\partial f(\\boldsymbol\\alpha)/\\partial \\boldsymbol\\alpha =0\\).Eq. (8.15) implies \\(\\mathbb{E}\\left( y_{t+1}x_t \\right)=\\mathbb{E}\\left(x_tx_t' \\right)\\boldsymbol\\alpha\\). (Note \\(x_t x_t'\\boldsymbol\\alpha=x_t (x_t'\\boldsymbol\\alpha)=(\\boldsymbol\\alpha'x_t) x_t'\\).)Hence, \\(\\mathbb{E}\\left(x_tx_t' \\right)\\) nonsingular,\n\\[\\begin{equation}\n\\boldsymbol\\alpha=[\\mathbb{E}\\left(x_tx_t' \\right)]^{-1}\\mathbb{E}\\left( y_{t+1}x_t \\right).\\tag{8.16}\n\\end{equation}\\]MSE :\n\\[\n\\mathbb{E}([y_{t+1} - \\boldsymbol\\alpha'x_t]^2) = \\mathbb{E}{(y_{t+1}^2)} - \\mathbb{E}\\left( y_{t+1}x_t' \\right)[\\mathbb{E}\\left(x_tx_t' \\right)]^{-1}\\mathbb{E}\\left(x_ty_{t+1} \\right).\n\\]Consider regression \\(y_{t+1} = \\boldsymbol\\beta'\\mathbf{x}_t + \\varepsilon_{t+1}\\). OLS estimate :\n\\[\n\\mathbf{b} = \\left[ \\underbrace{ \\frac{1}{T} \\sum_{=1}^T \\mathbf{x}_t\\mathbf{x}_t'}_{\\mathbf{m}_1} \\right]^{-1}\\left[  \\underbrace{ \\frac{1}{T} \\sum_{=1}^T \\mathbf{x}_t'y_{t+1}}_{\\mathbf{m}_2} \\right].\n\\]\n\\(\\{x_t,y_t\\}\\) covariance-stationary ergodic second moments sample moments (\\(\\mathbf{m}_1\\) \\(\\mathbf{m}_2\\)) converges probability associated population moments \\(\\mathbf{b} \\overset{p}{\\rightarrow} \\boldsymbol\\alpha\\) (\\(\\boldsymbol\\alpha\\) defined Eq. (8.16)).Example 8.3  (Forecasting MA(q) process) Consider MA(q) process:\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q},\n\\]\n\\(\\{\\varepsilon_t\\}\\) white noise sequence (Def. 8.1).:20\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots) =\\\\\n&&\\left\\{\n\\begin{array}{lll}\n\\mu + \\theta_h \\varepsilon_{t} + \\dots + \\theta_q \\varepsilon_{t-q+h}  \\quad && \\quad h \\[1,q]\\\\\n\\mu \\quad && \\quad h > q\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]\n\n\\[\\begin{eqnarray*}\n&&\\mathbb{V}ar(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots)= \\mathbb{E}\\left( [y_{t+h} - \\mathbb{E}(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots)]^2 \\right) =\\\\\n&&\\left\\{\n\\begin{array}{lll}\n\\sigma^2(1+\\theta_1^2+\\dots+\\theta_{h-1}^2) \\quad && \\quad h \\[1,q]\\\\\n\\sigma^2(1+\\theta_1^2+\\dots+\\theta_q^2) \\quad && \\quad h>q.\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]Example 8.4  (Forecasting AR(p) process) (See web interface.) Consider AR(p) process:\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t,\n\\]\n\\(\\{\\varepsilon_t\\}\\) white noise sequence (Def. 8.1).Using notation Eq. (8.9), :\n\\[\n\\mathbf{y}_t - \\boldsymbol\\mu = F (\\mathbf{y}_{t-1}- \\boldsymbol\\mu) + \\boldsymbol\\xi_t,\n\\]\n\\(\\boldsymbol\\mu = [\\mu,\\dots,\\mu]'\\) (\\(\\mu\\) defined Eq. (8.13)). Hence:\n\\[\n\\mathbf{y}_{t+h} - \\boldsymbol\\mu = \\boldsymbol\\xi_{t+h} + F \\boldsymbol\\xi_{t+h-1} + \\dots + F^{h-1} \\boldsymbol\\xi_{t+1} + F^h (\\mathbf{y}_{t}- \\mu).\n\\]\nTherefore:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{y}_{t+h}|y_{t},y_{t-1},\\dots) &=& \\boldsymbol\\mu + F^{h}(\\mathbf{y}_t - \\boldsymbol\\mu)\\\\\n\\mathbb{V}ar\\left( [\\mathbf{y}_{t+h} - \\mathbb{E}(\\mathbf{y}_{t+h}|y_{t},y_{t-1},\\dots)] \\right) &=& \\Sigma + F\\Sigma F' + \\dots + F^{h-1}\\Sigma (F^{h-1})',\n\\end{eqnarray*}\\]\n:\n\\[\n\\Sigma = \\left[\n\\begin{array}{ccc}\n\\sigma^2  & 0& \\dots\\\\\n0  & 0 & \\\\\n\\vdots  & & \\ddots \\\\\n\\end{array}\n\\right].\n\\]Alternative approach: Taking (conditional) expectations sides \n\\[\ny_{t+h} - \\mu = \\phi_1 (y_{t+h-1} - \\mu) + \\phi_2 (y_{t+h-2} - \\mu) + \\dots + \\phi_p (y_{t-p} - \\mu) + \\varepsilon_{t+h},\n\\]\nobtain:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y_{t+h}|y_{t},y_{t-1},\\dots) &=& \\mu + \\phi_1\\left(\\mathbb{E}[y_{t+h-1}|y_{t},y_{t-1},\\dots] - \\mu\\right)+\\\\\n&&\\phi_2\\left(\\mathbb{E}[y_{t+h-2}|y_{t},y_{t-1},\\dots] - \\mu\\right) + \\dots +\\\\\n&& \\phi_p\\left(\\mathbb{E}[y_{t+h-p}|y_{t},y_{t-1},\\dots] - \\mu\\right),\n\\end{eqnarray*}\\]\ncan exploited recursively.recursion begins \\(\\mathbb{E}(y_{t-k}|y_{t},y_{t-1},\\dots)=y_{t-k}\\) (\\(k \\ge 0\\)).Assessing performances forecasting modelOnce one fitted model given dataset (length \\(T\\), say), one compute MSE (mean square errors) evaluate performance model. MSE -sample one. easy reduce -sample MSE. Typically, model estimated OLS, adding covariates mechanically reduces MSE (see Props. 4.4 4.5). , even additional data irrelevant, \\(R^2\\) regression increases. Adding irrelevant variables increases (-sample) \\(R^2\\) bound increase --sample MSE.Therefore, important analyse --sample performances forecasting model:Estimate model sample reduced size (\\(1,\\dots,T^*\\), \\(T^*<T\\))Use remaining available periods (\\(T^*+1,\\dots,T\\)) compute --sample forecasting errors (compute MSE). --sample exercise, important make sure data used produce forecasts (date \\(T^*\\)) indeed available date \\(T^*\\).Diebold-Mariano testHow compare different forecasting approaches? Diebold Mariano (1995) proposed simple test address question.Assume want compare approaches B. historical data sets implemented approaches past, providing two sets forecasting errors: \\(\\{e^{}_t\\}_{t=1,\\dots,T}\\) \\(\\{e^{B}_t\\}_{t=1,\\dots,T}\\).may case forecasts serve specific purpose , instance, dislike positive forecasting errors care less negative errors. assume able formalise means loss function \\(L(e)\\). instance:dislike large positive errors, may set \\(L(e)=\\exp(e)\\).concerned positive negative errors (indifferently), may set \\(L(e)=e^2\\) (standard approach).Let us define sequence \\(\\{d_t\\}_{t=1,\\dots,T} \\equiv \\{L(e^{}_t)-L(e^{B}_t)\\}_{t=1,\\dots,T}\\) assume sequence covariance stationary. consider following null hypothesis: \\(H_0:\\) \\(\\bar{d}=0\\), \\(\\bar{d}\\) denotes population mean \\(d_t\\)s. \\(H_0\\) assumption covariance-stationarity \\(d_t\\), (Theorem @ref{(hm:CLTcovstat)):\n\\[\n\\sqrt{T} \\bar{d}_T \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\right),\n\\]\n\\(\\gamma_j\\)s autocovariances \\(d_t\\).Hence, assuming \\(\\hat{\\sigma}^2\\) consistent estimate \\(\\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\) (instance one given Newey West (1987)), , \\(H_0\\):\n\\[\nDM_T := \\sqrt{T}\\frac{\\bar{d}_T}{\\sqrt{\\hat{\\sigma}^2}} \\overset{d}{\\rightarrow}  \\mathcal{N}(0,1).\n\\]\n\\(DM_T\\) test statistics. test size \\(\\alpha\\), critical region :21\n\\[\n]-\\infty,-\\Phi^{-1}(1-\\alpha/2)] \\cup [\\Phi^{-1}(1-\\alpha/2),+\\infty[,\n\\]\n\\(\\Phi\\) c.d.f. standard normal distribution.Example 8.5  (Forecasting Swiss GDP growth) use long historical time series Swiss GDP growth taken Jordà, Schularick, Taylor (2017) dataset.22We want forecast GDP growth. envision two specifications : AR(1) specification (one advocated AIC criteria), ARMA(2,2) specification. interested 2-year-ahead forecasts (.e., \\(h=2\\) since data yearly).alternative = \"greater\" alternative hypothesis method 2 accurate method 1. Since reject null (p-value 0.795), led use sophisticated model (ARMA(2,2)) keep simple AR(1) model.Assume now want compare AR(1) process multivariate (VAR) model. consider bivariate VAR, GDP growth complemented CPI-based inflation rate., find alternative model (VAR(1) model) better AR(1) model forecast GDP growth.","code":"\nlibrary(AEC)\nlibrary(forecast)## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo\ndata <- subset(JST,iso==\"CHE\")\nT <- dim(data)[1]\ny <- c(NaN,log(data$gdp[2:T]/data$gdp[1:(T-1)]))\nfirst.date <- T-50\ne1 <- NULL; e2 <- NULL;h<-2\nfor(T.star in first.date:(T-h)){\n  estim.model.1 <- arima(y[1:T.star],order=c(1,0,0))\n  estim.model.2 <- arima(y[1:T.star],order=c(2,0,2))\n  e1 <- c(e1,y[T.star+h] - predict(estim.model.1,n.ahead=h)$pred[h])\n  e2 <- c(e2,y[T.star+h] - predict(estim.model.2,n.ahead=h)$pred[h])\n}\nres.DM <- dm.test(e1,e2,h = h,alternative = \"greater\")\nres.DM## \n##  Diebold-Mariano Test\n## \n## data:  e1e2\n## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =\n## 0.7946\n## alternative hypothesis: greater\nlibrary(vars)\ninfl <- c(NaN,log(data$cpi[2:T]/data$cpi[1:(T-1)]))\ny_var <- cbind(y,infl)\ne3 <- NULL\nfor(T.star in first.date:(T-h)){\n  estim.model.3 <- VAR(y_var[2:T.star,],p=1)\n  e3 <- c(e3,y[T.star+h] - predict(estim.model.3,n.ahead=h)$fcst$y[h,1])\n}\nres.DM <- dm.test(e1,e2,h = h,alternative = \"greater\")\nres.DM## \n##  Diebold-Mariano Test\n## \n## data:  e1e2\n## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =\n## 0.7946\n## alternative hypothesis: greater"},{"path":"append.html","id":"append","chapter":"9 Appendix","heading":"9 Appendix","text":"","code":""},{"path":"append.html","id":"PCAapp","chapter":"9 Appendix","heading":"9.1 Principal component analysis (PCA)","text":"Principal component analysis (PCA) classical easy--use statistical method reduce dimension large datasets containing variables linearly driven relatively small number factors. approach widely used data analysis image compression.Suppose \\(T\\) observations \\(n\\)-dimensional random vector \\(x\\), denoted \\(x_{1},x_{2},\\ldots,x_{T}\\). suppose component \\(x\\) mean zero.Let denote \\(X\\) matrix given \\(\\left[\\begin{array}{cccc} x_{1} & x_{2} & \\ldots & x_{T}\\end{array}\\right]'\\). Denote \\(j^{th}\\) column \\(X\\) \\(X_{j}\\).want find linear combination \\(x_{}\\)’s (\\(x.u\\)), \\(\\left\\Vert u\\right\\Vert =1\\), “maximum variance.” , want solve:\n\\[\\begin{equation}\n\\begin{array}{clll}\n\\underset{u}{\\arg\\max} & u'X'Xu. \\\\\n\\mbox{s.t. } & \\left\\Vert u \\right\\Vert =1\n\\end{array}\\tag{9.1}\n\\end{equation}\\]Since \\(X'X\\) positive definite matrix, admits following decomposition:\n\\[\\begin{eqnarray*}\nX'X & = & PDP'\\\\\n& = & P\\left[\\begin{array}{ccc}\n\\lambda_{1}\\\\\n& \\ddots\\\\\n&  & \\lambda_{n}\n\\end{array}\\right]P',\n\\end{eqnarray*}\\]\n\\(P\\) orthogonal matrix whose columns eigenvectors \\(X'X\\).can order eigenvalues \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X'X\\) positive definite, eigenvalues positive.)Since \\(P\\) orthogonal, \\(u'X'Xu=u'PDP'u=y'Dy\\) \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, \\(y_{}^{2}\\leq 1\\) \\(\\leq n\\).consequence:\n\\[\ny'Dy=\\sum_{=1}^{n}y_{}^{2}\\lambda_{}\\leq\\lambda_{1}\\sum_{=1}^{n}y_{}^{2}=\\lambda_{1}.\n\\]easily seen maximum reached \\(y=\\left[1,0,\\cdots,0\\right]'\\). Therefore, maximum optimization program (Eq. (9.1)) obtained \\(u=P\\left[1,0,\\cdots,0\\right]'\\). , \\(u\\) eigenvector \\(X'X\\) associated larger eigenvalue (first column \\(P\\)).Let us denote \\(F\\) vector given matrix product \\(XP\\). columns \\(F\\), denoted \\(F_{j}\\), called factors. :\n\\[\nF'F=P'X'XP=D.\n\\]\nTherefore, particular, \\(F_{j}\\)’s orthogonal.Since \\(X=FP'\\), \\(X_{j}\\)’s linear combinations factors. Let us denote \\(\\hat{X}_{,j}\\) part \\(X_{}\\) explained factor \\(F_{j}\\), :\n\\[\\begin{eqnarray*}\n\\hat{X}_{,j} & = & p_{ij}F_{j}\\\\\nX_{} & = & \\sum_{j}\\hat{X}_{,j}=\\sum_{j}p_{ij}F_{j}.\n\\end{eqnarray*}\\]Consider share variance explained—\\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))—first factor \\(F_{1}\\):\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}'_{,1}\\hat{X}_{,1}}{\\sum_{}X_{}'X_{}} & = & \\frac{\\sum_{}p_{i1}F'_{1}F_{1}p_{i1}}{tr(X'X)} = \\frac{\\sum_{}p_{i1}^{2}\\lambda_{1}}{tr(X'X)} = \\frac{\\lambda_{1}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Intuitively, first eigenvalue large, means first factor captures large share fluctutaions \\(n\\) \\(X_{}\\)’s.token, easily seen fraction variance \\(n\\) variables explained factor \\(j\\) given :\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}'_{,j}\\hat{X}_{,j}}{\\sum_{}X_{}'X_{}} & = & \\frac{\\lambda_{j}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Let us illustrate PCA term structure yields. term strucutre yields (yield curve) know driven small number factors (e.g., Litterman Scheinkman (1991)). One can typically employ PCA recover factors. data used example taken Fred database (tickers: “DGS6MO”,“DGS1”, …). second plot shows factor loardings, indicate first factor level factor (loadings = black line), second factor slope factor (loadings = blue line), third factor curvature factor (loadings = red line).run PCA, one simply apply function prcomp matrix data:Let us know visualize results. first plot Figure 9.1 shows share total variance explained different principal components (PCs). second plot shows facotr loadings. two bottom plots show yields (black) fitted linear combinations first two PCs .\nFigure 9.1: PCA results. dataset contains 8 time series U.S. interest rates different maturities.\n","code":"\nlibrary(AEC)\nUSyields <- USyields[complete.cases(USyields),]\nyds <- USyields[c(\"Y1\",\"Y2\",\"Y3\",\"Y5\",\"Y7\",\"Y10\",\"Y20\",\"Y30\")]\nPCA.yds <- prcomp(yds,center=TRUE,scale. = TRUE)\npar(mfrow=c(2,2))\npar(plt=c(.1,.95,.2,.8))\nbarplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2),\n        main=\"Share of variance expl. by PC's\")\naxis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x))\nnb.PC <- 2\nplot(-PCA.yds$rotation[,1],type=\"l\",lwd=2,ylim=c(-1,1),\n     main=\"Factor loadings (1st 3 PCs)\",xaxt=\"n\",xlab=\"\")\naxis(1, at=1:dim(yds)[2], labels=colnames(yds))\nlines(PCA.yds$rotation[,2],type=\"l\",lwd=2,col=\"blue\")\nlines(PCA.yds$rotation[,3],type=\"l\",lwd=2,col=\"red\")\nY1.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y1\",1:2]\nY1.hat <- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat\nplot(USyields$date,USyields$Y1,type=\"l\",lwd=2,\n     main=\"Fit of 1-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y1.hat,col=\"blue\",lty=2,lwd=2)\nY10.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y10\",1:2]\nY10.hat <- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat\nplot(USyields$date,USyields$Y10,type=\"l\",lwd=2,\n     main=\"Fit of 10-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y10.hat,col=\"blue\",lty=2,lwd=2)"},{"path":"append.html","id":"LinAlgebra","chapter":"9 Appendix","heading":"9.2 Linear algebra: definitions and results","text":"Definition 9.1  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 9.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 9.2  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 9.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 9.3  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 9.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 9.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 9.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 9.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 9.6  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Definition 9.4  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 9.7  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Proposition 9.8  (Square absolute summability) :\n\\[\n\\underbrace{\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty}_{\\mbox{Square summability}}.\n\\]Proof. See Appendix 3.Hamilton. Idea: Absolute summability implies exist \\(N\\) , \\(j>N\\), \\(|\\theta_j| < 1\\) (deduced Cauchy criterion, Theorem 9.2 therefore \\(\\theta_j^2 < |\\theta_j|\\).","code":""},{"path":"append.html","id":"variousResults","chapter":"9 Appendix","heading":"9.3 Statistical analysis: definitions and results","text":"","code":""},{"path":"append.html","id":"moments-and-statistics","chapter":"9 Appendix","heading":"9.3.1 Moments and statistics","text":"Definition 9.5  (Partial correlation) partial correlation \\(y\\) \\(z\\), controlling variables \\(\\mathbf{X}\\) sample correlation \\(y^*\\) \\(z^*\\), latter two variables residuals regressions \\(y\\) \\(\\mathbf{X}\\) \\(z\\) \\(\\mathbf{X}\\), respectively.correlation denoted \\(r_{yz}^\\mathbf{X}\\). definition, :\n\\[\\begin{equation}\nr_{yz}^\\mathbf{X} = \\frac{\\mathbf{z^*}'\\mathbf{y^*}}{\\sqrt{(\\mathbf{z^*}'\\mathbf{z^*})(\\mathbf{y^*}'\\mathbf{y^*})}}.\\tag{9.2}\n\\end{equation}\\]Definition 9.6  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).skewness \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]kurtosis \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Theorem 9.1  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 3.2  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) asymptotic level equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha,\n\\]\n\\(S_n\\) test statistic \\(\\Theta\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\\\Theta\\).Definition 3.3  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1,\n\\]\n\\(S_n\\) test statistic \\(\\Theta^c\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\notin \\Theta^c\\).Definition 9.7  (Kullback discrepancy) Given two p.d.f. \\(f\\) \\(f^*\\), Kullback discrepancy defined :\n\\[\n(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy.\n\\]Proposition 9.9  (Properties Kullback discrepancy) :\\((f,f^*) \\ge 0\\)\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).Proof. \\(x \\rightarrow -\\log(x)\\) convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves ()). Since \\(x \\rightarrow -\\log(x)\\) strictly convex, equality () holds \\(f(Y)/f^*(Y)\\) constant (proves (ii)).Definition 9.8  (Characteristic function) real-valued random variable \\(X\\), characteristic function defined :\n\\[\n\\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)].\n\\]","code":""},{"path":"append.html","id":"standard-distributions","chapter":"9 Appendix","heading":"9.3.2 Standard distributions","text":"Definition 9.9  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 9.4 quantiles.)Definition 9.10  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 9.2 quantiles.)Definition 9.11  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 9.3 quantiles.)\nFigure 9.2: Pdf Cauchy distribution (\\(\\mu=0\\), \\(\\gamma=1\\)).\nProposition 9.10  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.Definition 9.13  (Generalized Extreme Value (GEV) distribution) vector disturbances \\(\\boldsymbol\\varepsilon=[\\varepsilon_{1,1},\\dots,\\varepsilon_{1,K_1},\\dots,\\varepsilon_{J,1},\\dots,\\varepsilon_{J,K_J}]'\\) follows Generalized Extreme Value (GEV) distribution c.d.f. :\n\\[\nF(\\boldsymbol\\varepsilon,\\boldsymbol\\rho) = \\exp(-G(e^{-\\varepsilon_{1,1}},\\dots,e^{-\\varepsilon_{J,K_J}};\\boldsymbol\\rho))\n\\]\n\n\\[\\begin{eqnarray*}\nG(\\mathbf{Y};\\boldsymbol\\rho) &\\equiv&  G(Y_{1,1},\\dots,Y_{1,K_1},\\dots,Y_{J,1},\\dots,Y_{J,K_J};\\boldsymbol\\rho) \\\\\n&=& \\sum_{j=1}^J\\left(\\sum_{k=1}^{K_j} Y_{jk}^{1/\\rho_j}\n\\right)^{\\rho_j}\n\\end{eqnarray*}\\]","code":""},{"path":"append.html","id":"StochConvergences","chapter":"9 Appendix","heading":"9.3.3 Stochastic convergences","text":"Proposition 9.11  (Chebychev's inequality) \\(\\mathbb{E}(|X|^r)\\) finite \\(r>0\\) :\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}.\n\\]\nparticular, \\(r=2\\):\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}.\n\\]Proof. Remark \\(\\varepsilon^r \\mathbb{}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) take expectation sides.Definition 9.14  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 9.15  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 9.16  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 9.17  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Proposition 9.12  (Rules limiting distributions (Slutsky)) :Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Proposition 9.13  (Implications stochastic convergences) :\n\\[\\begin{align*}\n&\\boxed{\\overset{L^s}{\\rightarrow}}& &\\underset{1 \\le r \\le s}{\\Rightarrow}& &\\boxed{\\overset{L^r}{\\rightarrow}}&\\\\\n&& && &\\Downarrow&\\\\\n&\\boxed{\\overset{.s.}{\\rightarrow}}& &\\Rightarrow& &\\boxed{\\overset{p}{\\rightarrow}}& \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}.\n\\end{align*}\\]Proof. (fact \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting \\(F\\) \\(F_n\\) c.d.f. \\(X\\) \\(X_n\\), respectively:\n\\[\\begin{eqnarray*}\nF_n(x) &=& \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X > x+\\varepsilon)\\\\\n&\\le& F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\\tag{9.3}\n\\end{eqnarray*}\\]\nBesides,\n\\[\\begin{eqnarray*}\nF(x-\\varepsilon) &=& \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n > x)\\\\\n&\\le& F_n(x) + \\mathbb{P}(|X_n - X|>\\varepsilon),\n\\end{eqnarray*}\\]\nimplies:\n\\[\\begin{equation}\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x).\\tag{9.4}\n\\end{equation}\\]\nEqs. (9.3) (9.4) imply:\n\\[\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x)  \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\n\\]\nTaking limits \\(n \\rightarrow \\infty\\) yields\n\\[\nF(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x)  \\le F(x+\\varepsilon).\n\\]\nresult obtained taking limits \\(\\varepsilon \\rightarrow 0\\) (\\(F\\) continuous \\(x\\)).Proposition 9.14  (Convergence distribution constant) \\(X_n\\) converges distribution constant \\(c\\), \\(X_n\\) converges probability \\(c\\).Proof. \\(\\varepsilon>0\\), \\(\\mathbb{P}(X_n < c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) .e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) \\(\\mathbb{P}(X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\),\ngives result.Example 9.1  (Convergence probability $L^r$) Let \\(\\{x_n\\}_{n \\\\mathbb{N}}\\) series random variables defined :\n\\[\nx_n = n u_n,\n\\]\n\\(u_n\\) independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\).\\(x_n \\overset{p}{\\rightarrow} 0\\) \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\).Theorem 9.2  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 9.3  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]","code":""},{"path":"append.html","id":"CLTappend","chapter":"9 Appendix","heading":"9.3.4 Central limit theorem","text":"Theorem 9.4  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. 9.8) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 2.2  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n&&\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n\\\\\n&=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).","code":""},{"path":"append.html","id":"GaussianVar","chapter":"9 Appendix","heading":"9.4 Some properties of Gaussian variables","text":"Proposition 9.15  \\(\\mathbf{}\\) idempotent \\(\\mathbf{x}\\) Gaussian, \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}\\) independent \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\).Proof. \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\), two Gaussian vectors \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{}\\mathbf{x}\\) independent. implies independence function \\(\\mathbf{L}\\mathbf{x}\\) function \\(\\mathbf{}\\mathbf{x}\\). results follows observation \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}=(\\mathbf{}\\mathbf{x})'(\\mathbf{}\\mathbf{x})\\), function \\(\\mathbf{}\\mathbf{x}\\).Proposition 9.16  (Bayesian update vector Gaussian variables) \n\\[\n\\left[\n\\begin{array}{c}\nY_1\\\\\nY_2\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\n\\left(0,\n\\left[\\begin{array}{cc}\n\\Omega_{11} & \\Omega_{12}\\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right]\n\\right),\n\\]\n\n\\[\nY_{2}|Y_{1} \\sim \\mathcal{N}\n\\left(\n\\Omega_{21}\\Omega_{11}^{-1}Y_{1},\\Omega_{22}-\\Omega_{21}\\Omega_{11}^{-1}\\Omega_{12}\n\\right).\n\\]\n\\[\nY_{1}|Y_{2} \\sim \\mathcal{N}\n\\left(\n\\Omega_{12}\\Omega_{22}^{-1}Y_{2},\\Omega_{11}-\\Omega_{12}\\Omega_{22}^{-1}\\Omega_{21}\n\\right).\n\\]Proposition 9.17  (Truncated distributions) \\(X\\) random variable distributed according p.d.f. \\(f\\), c.d.f. \\(F\\), infinite support. p.d.f. \\(X|\\le X < b\\) \n\\[\ng(x) = \\frac{f(x)}{F(b)-F()}\\mathbb{}_{\\{\\le x < b\\}},\n\\]\n\\(<b\\).partiucular, Gaussian variable \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\nf(X=x|\\le X<b) = \\dfrac{\\dfrac{1}{\\sigma}\\phi\\left(\\dfrac{x - \\mu}{\\sigma}\\right)}{Z}.\n\\]\n\\(Z = \\Phi(\\beta)-\\Phi(\\alpha)\\), \\(\\alpha = \\dfrac{- \\mu}{\\sigma}\\) \\(\\beta = \\dfrac{b - \\mu}{\\sigma}\\).Moreover:\n\\[\\begin{eqnarray}\n\\mathbb{E}(X|\\le X<b) &=& \\mu - \\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\sigma. \\tag{9.5}\n\\end{eqnarray}\\]also :\n\\[\\begin{eqnarray}\n&& \\mathbb{V}ar(X|\\le X<b) \\nonumber\\\\\n&=& \\sigma^2\\left[\n1 -  \\frac{\\beta\\phi\\left(\\beta\\right)-\\alpha\\phi\\left(\\alpha\\right)}{Z} -  \\left(\\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\right)^2 \\right] \\tag{9.6}\n\\end{eqnarray}\\]particular, \\(b \\rightarrow \\infty\\), get:\n\\[\\begin{equation}\n\\mathbb{V}ar(X|< X) = \\sigma^2\\left[1 + \\alpha\\lambda(-\\alpha) - \\lambda(-\\alpha)^2 \\right], \\tag{9.7}\n\\end{equation}\\]\n\\(\\lambda(x)=\\dfrac{\\phi(x)}{\\Phi(x)}\\) called inverse Mills ratio.Consider case \\(\\rightarrow - \\infty\\) (.e. conditioning set \\(X<b\\)) \\(\\mu=0\\), \\(\\sigma=1\\). Eq. (9.5) gives \\(\\mathbb{E}(X|X<b) = - \\lambda(b) = - \\dfrac{\\phi(b)}{\\Phi(b)}\\), \\(\\lambda\\) function computing inverse Mills ratio.\nFigure 9.3: \\(\\mathbb{E}(X|X<b)\\) function \\(b\\) \\(X\\sim \\mathcal{N}(0,1)\\) (black).\nProposition 9.18  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"AppendixProof","chapter":"9 Appendix","heading":"9.5 Proofs","text":"Proof Proposition 6.4Proof. Assumptions () (ii) (set Assumptions 6.1) imply \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\)).\\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) can interpreted sample mean r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) ..d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – exists (Assumption iv).latter convergence uniform (Assumption v), solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges solution limit problem:\n\\[\n\\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy.\n\\]Properties Kullback information measure (see Prop. 9.9), together identifiability assumption (ii) implies solution limit problem unique equal \\(\\boldsymbol\\theta_0\\).Consider r.v. sequence \\(\\boldsymbol\\theta\\) converges \\(\\boldsymbol\\theta_0\\). Taylor expansion score neighborood \\(\\boldsymbol\\theta_0\\) yields :\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\]\\(\\boldsymbol\\theta_{MLE}\\) converges \\(\\boldsymbol\\theta_0\\) satisfies likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}\\). Therefore:\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]\nequivalently:\n\\[\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]law large numbers, : \\(\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\mathbf{}(\\boldsymbol\\theta_0) = \\mathcal{}_Y(\\boldsymbol\\theta_0)\\).Besides, :\n\\[\\begin{eqnarray*}\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} &=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\\n&=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right)\n\\end{eqnarray*}\\]\nconverges \\(\\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0))\\) CLT.Collecting preceding results leads (b). fact \\(\\boldsymbol\\theta_{MLE}\\) achieves FDCR bound proves (c).Proof Proposition 6.5Proof. \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (6.9)). Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields :\n\\[\\begin{equation}\n\\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{9.8}\n\\end{equation}\\]\n\\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore:\n\\[\\begin{equation}\n\\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{9.9}\n\\end{equation}\\]\nHence\n\\[\n\\sqrt{n} \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right).\n\\]\nTaking quadratic form, obtain:\n\\[\nn h(\\hat{\\boldsymbol\\theta}_{n})'  \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]fact test asymptotic level \\(\\alpha\\) directly stems precedes. Consistency test: Consider \\(\\theta_0 \\\\Theta\\). MLE consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (9.8) still valid. implies \\(\\xi^W_n\\) converges \\(+\\infty\\) therefore \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\).Proof Proposition 6.6Proof. Notations: “\\(\\approx\\)” means “equal term converges 0 probability”. \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes unconstrained one.combine two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (definition) get:\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{9.10}\n\\end{equation}\\]\nBesides, (using definition information matrix):\n\\[\\begin{equation}\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{9.11}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{9.12}\n\\end{equation}\\]\nTaking difference multiplying \\(\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\):\n\\[\\begin{equation}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\mathcal{}(\\boldsymbol\\theta_0).\\tag{9.13}\n\\end{equation}\\]\nEqs. (9.10) (9.13) yield :\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}.\\tag{9.14}\n\\end{equation}\\]Recall \\(\\hat{\\boldsymbol\\theta}^0_n\\) MLE \\(\\boldsymbol\\theta_0\\) constraint \\(h(\\boldsymbol\\theta)=0\\). vector Lagrange multipliers \\(\\hat\\lambda_n\\) associated program satisfies:\n\\[\\begin{equation}\n\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{9.15}\n\\end{equation}\\]\nSubstituting latter equation Eq. (9.14) gives:\n\\[\\begin{eqnarray*}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) &\\approx&\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\\\\n&\\approx&\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}},\n\\end{eqnarray*}\\]\nyields:\n\\[\\begin{equation}\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{9.16}\n\\end{equation}\\]\nfollows, Eq. (9.9), :\n\\[\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\\right).\n\\]\nTaking quadratic form last equation gives:\n\\[\n\\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]\nUsing Eq. (9.15), appears left-hand side term last equation \\(\\xi^{LM}\\) defined Eq. (6.15). Consistency: see Remark 17.3 Gouriéroux Monfort (1995).Proof Proposition 6.7Proof. Let us first demonstrate asymptotic equivalence \\(\\xi^{LM}\\) \\(\\xi^{LR}\\).second-order Taylor expansions \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y})\\) \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y})\\) :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0) \\\\\n&& - \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\\\\n&& - \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nTaking difference, obtain:\n\\[\\begin{eqnarray*}\n\\xi_n^{LR} &\\approx& 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\\\\n&& - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nUsing \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (9.12)), :\n\\[\\begin{eqnarray*}\n\\xi_n^{LR} &\\approx&\n2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)'\\mathcal{}(\\boldsymbol\\theta_0)\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n)\n+ n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\\\\n&& - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nsecond three terms sum, replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) develop associated product. leads :\n\\[\\begin{equation}\n\\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)' \\mathcal{}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{9.17}\n\\end{equation}\\]\ndifference Eqs. (9.11) (9.12) implies:\n\\[\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n),\n\\]\n, associated Eq. @(eq:lr10), gives:\n\\[\n\\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}.\n\\]\nHence \\(\\xi_n^{LR}\\) asymptotic distribution \\(\\xi_n^{LM}\\).Let’s show LR test consistent. , note :\n\\[\\begin{eqnarray*}\n\\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\mathbf{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\mathbf{y})}{n} &=& \\frac{1}{n} \\sum_{=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)]\\\\\n&\\rightarrow& \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)],\n\\end{eqnarray*}\\]\n\\(\\boldsymbol\\theta_\\infty\\), pseudo true value, \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (definition \\(H_1\\)). Kullback inequality asymptotic identifiability \\(\\boldsymbol\\theta_0\\), follows \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] >0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) \\(H_1\\).Let us now demonstrate equivalence \\(\\xi^{LM} \\xi^{W}\\).(using Eq. (eq:multiplier)):\n\\[\n\\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n.\n\\]\nSince, \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (9.16) therefore implies :\n\\[\n\\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)' \\left(\n\\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\nh(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W},\n\\]\ngives result.Proof Eq. (8.3)Proof. :\n\\[\\begin{eqnarray*}\n&&T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\\n&=& T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s<t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\\n&=& \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\\n&&+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\\n&=&  \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} .\n\\end{eqnarray*}\\]\nTherefore:\n\\[\\begin{eqnarray*}\n&& T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\\\\n&=& - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots\n\\end{eqnarray*}\\]\n:\n\\[\\begin{eqnarray*}\n&& \\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\\\\n&\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]\\(q \\le T\\), :\n\\[\\begin{eqnarray*}\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\\n&&2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots  + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\\n&\\le& \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\\n&&2|\\gamma_{q+1}| + \\dots  + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]Consider \\(\\varepsilon > 0\\). fact autocovariances absolutely summable implies exists \\(q_0\\) (Cauchy criterion, Theorem 9.2):\n\\[\n2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots < \\varepsilon/2.\n\\]\n, \\(T > q_0\\), comes :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2.\n\\]\n\\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) \n\\[\n\\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2.\n\\]\n, \\(T>f(q_0)\\) \\(T>q_0\\), .e. \\(T>\\max(f(q_0),q_0)\\), :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon.\n\\]Proof Proposition 8.7Proof. :\n\\[\\begin{eqnarray}\n\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\\n&=&  \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)\\nonumber\\\\\n&& + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right). \\tag{9.18}\n\\end{eqnarray}\\]\nLet us focus last term. :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right)\\\\\n&=& \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\\mbox{function $x_t$}}}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.\n\\end{eqnarray*}\\]Therefore, Eq. (9.18) becomes:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\\n&=&  \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ depend $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ depends $y^*_{t+1}$}}.\n\\end{eqnarray*}\\]\nimplies \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) always larger \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), therefore minimized second term equal zero, \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\).Proof Proposition ??Proof. Using Proposition 9.18, obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.","code":""},{"path":"append.html","id":"statistical-tables","chapter":"9 Appendix","heading":"9.6 Statistical Tables","text":"Table 9.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 9.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 9.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 9.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
