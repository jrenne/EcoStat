<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Appendix | Econometrics and Statistics</title>
  <meta name="description" content="This course covers various topics in econometrics. Simulations and real-data-based examples are provided." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Appendix | Econometrics and Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This course covers various topics in econometrics. Simulations and real-data-based examples are provided." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Appendix | Econometrics and Statistics" />
  
  <meta name="twitter:description" content="This course covers various topics in econometrics. Simulations and real-data-based examples are provided." />
  

<meta name="author" content="Jean-Paul Renne" />


<meta name="date" content="2023-01-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="TS.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Econometrics and Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>1</b> Basic statistical results</a>
<ul>
<li class="chapter" data-level="1.1" data-path="Basics.html"><a href="Basics.html#cumulative-and-probability-density-funtions-c.d.f.-and-p.d.f."><i class="fa fa-check"></i><b>1.1</b> Cumulative and probability density funtions (c.d.f. and p.d.f.)</a></li>
<li class="chapter" data-level="1.2" data-path="Basics.html"><a href="Basics.html#more-examples-in-demopersp"><i class="fa fa-check"></i><b>1.2</b> More examples in demo(persp) !!</a></li>
<li class="chapter" data-level="1.3" data-path="Basics.html"><a href="Basics.html#section"><i class="fa fa-check"></i><b>1.3</b> ———–</a></li>
<li class="chapter" data-level="1.4" data-path="Basics.html"><a href="Basics.html#more-examples-in-demopersp-1"><i class="fa fa-check"></i><b>1.4</b> More examples in demo(persp) !!</a></li>
<li class="chapter" data-level="1.5" data-path="Basics.html"><a href="Basics.html#section-1"><i class="fa fa-check"></i><b>1.5</b> ———–</a></li>
<li class="chapter" data-level="1.6" data-path="Basics.html"><a href="Basics.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>1.6</b> Law of iterated expectations</a></li>
<li class="chapter" data-level="1.7" data-path="Basics.html"><a href="Basics.html#law-of-total-variance"><i class="fa fa-check"></i><b>1.7</b> Law of total variance</a></li>
<li class="chapter" data-level="1.8" data-path="Basics.html"><a href="Basics.html#about-consistent-estimators"><i class="fa fa-check"></i><b>1.8</b> About consistent estimators</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="TCL.html"><a href="TCL.html"><i class="fa fa-check"></i><b>2</b> Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="TCL.html"><a href="TCL.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.1</b> Law of large numbers</a></li>
<li class="chapter" data-level="2.2" data-path="TCL.html"><a href="TCL.html#central-limit-theorem-clt"><i class="fa fa-check"></i><b>2.2</b> Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="2.3" data-path="TCL.html"><a href="TCL.html#comparison-of-sample-means"><i class="fa fa-check"></i><b>2.3</b> Comparison of sample means</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Tests.html"><a href="Tests.html"><i class="fa fa-check"></i><b>3</b> Statistical tests</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Tests.html"><a href="Tests.html#size-and-power-of-a-test"><i class="fa fa-check"></i><b>3.1</b> Size and power of a test</a></li>
<li class="chapter" data-level="3.2" data-path="Tests.html"><a href="Tests.html#the-different-types-of-statistical-tests"><i class="fa fa-check"></i><b>3.2</b> The different types of statistical tests</a></li>
<li class="chapter" data-level="3.3" data-path="Tests.html"><a href="Tests.html#asymptotic-properties-of-statistical-tests"><i class="fa fa-check"></i><b>3.3</b> Asymptotic properties of statistical tests</a></li>
<li class="chapter" data-level="3.4" data-path="Tests.html"><a href="Tests.html#example-normality-tests"><i class="fa fa-check"></i><b>3.4</b> Example: Normality tests</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ChapterLS.html"><a href="ChapterLS.html"><i class="fa fa-check"></i><b>4</b> Linear Regressions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ChapterLS.html"><a href="ChapterLS.html#linearHyp"><i class="fa fa-check"></i><b>4.1</b> Hypotheses</a></li>
<li class="chapter" data-level="4.2" data-path="ChapterLS.html"><a href="ChapterLS.html#LSquares"><i class="fa fa-check"></i><b>4.2</b> Least square estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ChapterLS.html"><a href="ChapterLS.html#derivation-of-the-ols-formula"><i class="fa fa-check"></i><b>4.2.1</b> Derivation of the OLS formula</a></li>
<li class="chapter" data-level="4.2.2" data-path="ChapterLS.html"><a href="ChapterLS.html#properties-of-the-ols-estimate-small-sample"><i class="fa fa-check"></i><b>4.2.2</b> Properties of the OLS estimate (small sample)</a></li>
<li class="chapter" data-level="4.2.3" data-path="ChapterLS.html"><a href="ChapterLS.html#goodness-of-fit"><i class="fa fa-check"></i><b>4.2.3</b> Goodness of fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="ChapterLS.html"><a href="ChapterLS.html#inference-and-confidence-intervals-in-small-sample"><i class="fa fa-check"></i><b>4.2.4</b> Inference and confidence intervals (in small sample)</a></li>
<li class="chapter" data-level="4.2.5" data-path="ChapterLS.html"><a href="ChapterLS.html#Ftest"><i class="fa fa-check"></i><b>4.2.5</b> Testing a set of linear restrictions</a></li>
<li class="chapter" data-level="4.2.6" data-path="ChapterLS.html"><a href="ChapterLS.html#largeSample"><i class="fa fa-check"></i><b>4.2.6</b> Large Sample Properties</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ChapterLS.html"><a href="ChapterLS.html#CommonPitfalls"><i class="fa fa-check"></i><b>4.3</b> Common pitfalls in linear regressions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ChapterLS.html"><a href="ChapterLS.html#multicollinearity"><i class="fa fa-check"></i><b>4.3.1</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.3.2" data-path="ChapterLS.html"><a href="ChapterLS.html#Omitted"><i class="fa fa-check"></i><b>4.3.2</b> Omitted variables</a></li>
<li class="chapter" data-level="4.3.3" data-path="ChapterLS.html"><a href="ChapterLS.html#irrelevant"><i class="fa fa-check"></i><b>4.3.3</b> Irrelevant variable</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ChapterLS.html"><a href="ChapterLS.html#IV"><i class="fa fa-check"></i><b>4.4</b> Instrumental Variables</a></li>
<li class="chapter" data-level="4.5" data-path="ChapterLS.html"><a href="ChapterLS.html#general-regression-model-grm-and-robust-covariance-matrices"><i class="fa fa-check"></i><b>4.5</b> General Regression Model (GRM) and robust covariance matrices</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ChapterLS.html"><a href="ChapterLS.html#presentation-of-the-general-regression-model-grm"><i class="fa fa-check"></i><b>4.5.1</b> Presentation of the General Regression Model (GRM)</a></li>
<li class="chapter" data-level="4.5.2" data-path="ChapterLS.html"><a href="ChapterLS.html#GLS"><i class="fa fa-check"></i><b>4.5.2</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="4.5.3" data-path="ChapterLS.html"><a href="ChapterLS.html#asymptotic-properties-of-the-ols-estimator-in-the-grm-framework"><i class="fa fa-check"></i><b>4.5.3</b> Asymptotic properties of the OLS estimator in the GRM framework</a></li>
<li class="chapter" data-level="4.5.4" data-path="ChapterLS.html"><a href="ChapterLS.html#HAC"><i class="fa fa-check"></i><b>4.5.4</b> HAC-robust covariance matrices</a></li>
<li class="chapter" data-level="4.5.5" data-path="ChapterLS.html"><a href="ChapterLS.html#Clusters"><i class="fa fa-check"></i><b>4.5.5</b> Cluster-robust covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="ChapterLS.html"><a href="ChapterLS.html#shrinkage-methods"><i class="fa fa-check"></i><b>4.6</b> Shrinkage methods</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Panel.html"><a href="Panel.html"><i class="fa fa-check"></i><b>5</b> Panel regressions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Panel.html"><a href="Panel.html#specification-and-notations"><i class="fa fa-check"></i><b>5.1</b> Specification and notations</a></li>
<li class="chapter" data-level="5.2" data-path="Panel.html"><a href="Panel.html#three-standard-cases"><i class="fa fa-check"></i><b>5.2</b> Three standard cases</a></li>
<li class="chapter" data-level="5.3" data-path="Panel.html"><a href="Panel.html#FixedEffect"><i class="fa fa-check"></i><b>5.3</b> Estimation of Fixed-Effects Models</a></li>
<li class="chapter" data-level="5.4" data-path="Panel.html"><a href="Panel.html#RandomEffect"><i class="fa fa-check"></i><b>5.4</b> Estimation of random effects models</a></li>
<li class="chapter" data-level="5.5" data-path="Panel.html"><a href="Panel.html#DynPanel"><i class="fa fa-check"></i><b>5.5</b> Dynamic Panel Regressions</a></li>
<li class="chapter" data-level="5.6" data-path="Panel.html"><a href="Panel.html#introduction-to-program-evaluation"><i class="fa fa-check"></i><b>5.6</b> Introduction to program evaluation</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="Panel.html"><a href="Panel.html#presentation-of-the-problem"><i class="fa fa-check"></i><b>5.6.1</b> Presentation of the problem</a></li>
<li class="chapter" data-level="5.6.2" data-path="Panel.html"><a href="Panel.html#randomized-controlled-trials-rcts"><i class="fa fa-check"></i><b>5.6.2</b> Randomized controlled trials (RCTs)</a></li>
<li class="chapter" data-level="5.6.3" data-path="Panel.html"><a href="Panel.html#difference-in-difference-did-approach"><i class="fa fa-check"></i><b>5.6.3</b> Difference-in-Difference (DiD) approach</a></li>
<li class="chapter" data-level="5.6.4" data-path="Panel.html"><a href="Panel.html#application-of-the-did-approach"><i class="fa fa-check"></i><b>5.6.4</b> Application of the DiD approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="estimation-methods.html"><a href="estimation-methods.html"><i class="fa fa-check"></i><b>6</b> Estimation Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="estimation-methods.html"><a href="estimation-methods.html#secGMM"><i class="fa fa-check"></i><b>6.1</b> Generalized Method of Moments (GMM)</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="estimation-methods.html"><a href="estimation-methods.html#definition-of-the-gmm-estimator"><i class="fa fa-check"></i><b>6.1.1</b> Definition of the GMM estimator</a></li>
<li class="chapter" data-level="6.1.2" data-path="estimation-methods.html"><a href="estimation-methods.html#asymptotic-distribution-of-the-gmm-estimator"><i class="fa fa-check"></i><b>6.1.2</b> Asymptotic distribution of the GMM estimator</a></li>
<li class="chapter" data-level="6.1.3" data-path="estimation-methods.html"><a href="estimation-methods.html#overidentif"><i class="fa fa-check"></i><b>6.1.3</b> Testing hypotheses in the GMM framework</a></li>
<li class="chapter" data-level="6.1.4" data-path="estimation-methods.html"><a href="estimation-methods.html#example-estimation-of-the-stochastic-discount-factor-s.d.f."><i class="fa fa-check"></i><b>6.1.4</b> Example: Estimation of the Stochastic Discount Factor (s.d.f.)</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="estimation-methods.html"><a href="estimation-methods.html#secMLE"><i class="fa fa-check"></i><b>6.2</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="estimation-methods.html"><a href="estimation-methods.html#intuition"><i class="fa fa-check"></i><b>6.2.1</b> Intuition</a></li>
<li class="chapter" data-level="6.2.2" data-path="estimation-methods.html"><a href="estimation-methods.html#definition-and-properties"><i class="fa fa-check"></i><b>6.2.2</b> Definition and properties</a></li>
<li class="chapter" data-level="6.2.3" data-path="estimation-methods.html"><a href="estimation-methods.html#to-sum-up-mle-in-practice"><i class="fa fa-check"></i><b>6.2.3</b> To sum up – MLE in practice</a></li>
<li class="chapter" data-level="6.2.4" data-path="estimation-methods.html"><a href="estimation-methods.html#example-mle-estimation-of-a-mixture-of-gaussian-distribution"><i class="fa fa-check"></i><b>6.2.4</b> Example: MLE estimation of a mixture of Gaussian distribution</a></li>
<li class="chapter" data-level="6.2.5" data-path="estimation-methods.html"><a href="estimation-methods.html#TestMLE"><i class="fa fa-check"></i><b>6.2.5</b> Test procedures</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimation-methods.html"><a href="estimation-methods.html#bayesian-approach"><i class="fa fa-check"></i><b>6.3</b> Bayesian approach</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="estimation-methods.html"><a href="estimation-methods.html#introduction"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="estimation-methods.html"><a href="estimation-methods.html#monte-carlo-markov-chains"><i class="fa fa-check"></i><b>6.3.2</b> Monte-Carlo Markov Chains</a></li>
<li class="chapter" data-level="6.3.3" data-path="estimation-methods.html"><a href="estimation-methods.html#example-ar1-specification"><i class="fa fa-check"></i><b>6.3.3</b> Example: AR(1) specification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="microeconometrics.html"><a href="microeconometrics.html"><i class="fa fa-check"></i><b>7</b> Microeconometrics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="microeconometrics.html"><a href="microeconometrics.html#binary-choice-models"><i class="fa fa-check"></i><b>7.1</b> Binary-choice models</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="microeconometrics.html"><a href="microeconometrics.html#latent"><i class="fa fa-check"></i><b>7.1.1</b> Interpretation in terms of latent variable, and utility-based models</a></li>
<li class="chapter" data-level="7.1.2" data-path="microeconometrics.html"><a href="microeconometrics.html#Avregressors"><i class="fa fa-check"></i><b>7.1.2</b> Alternative-Varying Regressors</a></li>
<li class="chapter" data-level="7.1.3" data-path="microeconometrics.html"><a href="microeconometrics.html#estimation"><i class="fa fa-check"></i><b>7.1.3</b> Estimation</a></li>
<li class="chapter" data-level="7.1.4" data-path="microeconometrics.html"><a href="microeconometrics.html#marginalFX"><i class="fa fa-check"></i><b>7.1.4</b> Marginal effects</a></li>
<li class="chapter" data-level="7.1.5" data-path="microeconometrics.html"><a href="microeconometrics.html#goodness-of-fit-1"><i class="fa fa-check"></i><b>7.1.5</b> Goodness of fit</a></li>
<li class="chapter" data-level="7.1.6" data-path="microeconometrics.html"><a href="microeconometrics.html#predictions-and-roc-curves"><i class="fa fa-check"></i><b>7.1.6</b> Predictions and ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="microeconometrics.html"><a href="microeconometrics.html#multiple-choice-models"><i class="fa fa-check"></i><b>7.2</b> Multiple Choice Models</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="microeconometrics.html"><a href="microeconometrics.html#ordered-case"><i class="fa fa-check"></i><b>7.2.1</b> Ordered case</a></li>
<li class="chapter" data-level="7.2.2" data-path="microeconometrics.html"><a href="microeconometrics.html#MNL"><i class="fa fa-check"></i><b>7.2.2</b> General multinomial logit model</a></li>
<li class="chapter" data-level="7.2.3" data-path="microeconometrics.html"><a href="microeconometrics.html#nested-logits"><i class="fa fa-check"></i><b>7.2.3</b> Nested logits</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="microeconometrics.html"><a href="microeconometrics.html#tobit"><i class="fa fa-check"></i><b>7.3</b> Tobit models</a></li>
<li class="chapter" data-level="7.4" data-path="microeconometrics.html"><a href="microeconometrics.html#SSM"><i class="fa fa-check"></i><b>7.4</b> Sample Selection Models</a></li>
<li class="chapter" data-level="7.5" data-path="microeconometrics.html"><a href="microeconometrics.html#models-of-count-data"><i class="fa fa-check"></i><b>7.5</b> Models of Count Data</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="microeconometrics.html"><a href="microeconometrics.html#poisson-model"><i class="fa fa-check"></i><b>7.5.1</b> Poisson model</a></li>
<li class="chapter" data-level="7.5.2" data-path="microeconometrics.html"><a href="microeconometrics.html#negative-binomial-model"><i class="fa fa-check"></i><b>7.5.2</b> Negative binomial model</a></li>
<li class="chapter" data-level="7.5.3" data-path="microeconometrics.html"><a href="microeconometrics.html#hurdle-model"><i class="fa fa-check"></i><b>7.5.3</b> Hurdle model</a></li>
<li class="chapter" data-level="7.5.4" data-path="microeconometrics.html"><a href="microeconometrics.html#zero-inflated-model"><i class="fa fa-check"></i><b>7.5.4</b> Zero-inflated model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="TS.html"><a href="TS.html"><i class="fa fa-check"></i><b>8</b> Time Series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="TS.html"><a href="TS.html#introduction-to-time-series"><i class="fa fa-check"></i><b>8.1</b> Introduction to time series</a></li>
<li class="chapter" data-level="8.2" data-path="TS.html"><a href="TS.html#univariate-processes"><i class="fa fa-check"></i><b>8.2</b> Univariate processes</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="TS.html"><a href="TS.html#moving-average-ma-processes"><i class="fa fa-check"></i><b>8.2.1</b> Moving Average (MA) processes</a></li>
<li class="chapter" data-level="8.2.2" data-path="TS.html"><a href="TS.html#ARsection"><i class="fa fa-check"></i><b>8.2.2</b> Auto-Regressive (AR) processes</a></li>
<li class="chapter" data-level="8.2.3" data-path="TS.html"><a href="TS.html#ar-ma-processes"><i class="fa fa-check"></i><b>8.2.3</b> AR-MA processes</a></li>
<li class="chapter" data-level="8.2.4" data-path="TS.html"><a href="TS.html#PACFapproach"><i class="fa fa-check"></i><b>8.2.4</b> PACF approach to identify AR/MA processes</a></li>
<li class="chapter" data-level="8.2.5" data-path="TS.html"><a href="TS.html#wold-decomposition"><i class="fa fa-check"></i><b>8.2.5</b> Wold decomposition</a></li>
<li class="chapter" data-level="8.2.6" data-path="TS.html"><a href="TS.html#impulse-response-functions-irfs-in-arma-models"><i class="fa fa-check"></i><b>8.2.6</b> Impulse Response Functions (IRFs) in ARMA models</a></li>
<li class="chapter" data-level="8.2.7" data-path="TS.html"><a href="TS.html#ARMAIRF"><i class="fa fa-check"></i><b>8.2.7</b> ARMA processes with exogenous variables (ARMA-X)</a></li>
<li class="chapter" data-level="8.2.8" data-path="TS.html"><a href="TS.html#estimARMA"><i class="fa fa-check"></i><b>8.2.8</b> Maximum Likelihood Estimation of ARMA processes</a></li>
<li class="chapter" data-level="8.2.9" data-path="TS.html"><a href="TS.html#specification-choice"><i class="fa fa-check"></i><b>8.2.9</b> Specification choice</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="TS.html"><a href="TS.html#VAR"><i class="fa fa-check"></i><b>8.3</b> Multivariate models</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="TS.html"><a href="TS.html#definition-of-vars-and-svarma-models"><i class="fa fa-check"></i><b>8.3.1</b> Definition of VARs (and SVARMA) models</a></li>
<li class="chapter" data-level="8.3.2" data-path="TS.html"><a href="TS.html#IRFSVARMA"><i class="fa fa-check"></i><b>8.3.2</b> IRFs in SVARMA</a></li>
<li class="chapter" data-level="8.3.3" data-path="TS.html"><a href="TS.html#covariance-stationary-varma-models"><i class="fa fa-check"></i><b>8.3.3</b> Covariance-stationary VARMA models</a></li>
<li class="chapter" data-level="8.3.4" data-path="TS.html"><a href="TS.html#estimVAR"><i class="fa fa-check"></i><b>8.3.4</b> VAR estimation</a></li>
<li class="chapter" data-level="8.3.5" data-path="TS.html"><a href="TS.html#BlockGranger"><i class="fa fa-check"></i><b>8.3.5</b> Block exogeneity and Granger causality</a></li>
<li class="chapter" data-level="8.3.6" data-path="TS.html"><a href="TS.html#identification-problem-and-standard-identification-techniques"><i class="fa fa-check"></i><b>8.3.6</b> Identification problem and standard identification techniques</a></li>
<li class="chapter" data-level="8.3.7" data-path="TS.html"><a href="TS.html#Signs"><i class="fa fa-check"></i><b>8.3.7</b> Sign restrictions</a></li>
<li class="chapter" data-level="8.3.8" data-path="TS.html"><a href="TS.html#forecast-error-variance-maximization"><i class="fa fa-check"></i><b>8.3.8</b> Forecast error variance maximization</a></li>
<li class="chapter" data-level="8.3.9" data-path="TS.html"><a href="TS.html#NonGaussian"><i class="fa fa-check"></i><b>8.3.9</b> Identification based on non-normality of the shocks</a></li>
<li class="chapter" data-level="8.3.10" data-path="TS.html"><a href="TS.html#factor-augmented-var-favar"><i class="fa fa-check"></i><b>8.3.10</b> Factor-Augmented VAR (FAVAR)</a></li>
<li class="chapter" data-level="8.3.11" data-path="TS.html"><a href="TS.html#Projections"><i class="fa fa-check"></i><b>8.3.11</b> Projection Methods</a></li>
<li class="chapter" data-level="8.3.12" data-path="TS.html"><a href="TS.html#Inference"><i class="fa fa-check"></i><b>8.3.12</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="TS.html"><a href="TS.html#forecasting"><i class="fa fa-check"></i><b>8.4</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="append.html"><a href="append.html"><i class="fa fa-check"></i><b>9</b> Appendix</a>
<ul>
<li class="chapter" data-level="9.1" data-path="append.html"><a href="append.html#PCAapp"><i class="fa fa-check"></i><b>9.1</b> Principal component analysis (PCA)</a></li>
<li class="chapter" data-level="9.2" data-path="append.html"><a href="append.html#LinAlgebra"><i class="fa fa-check"></i><b>9.2</b> Linear algebra: definitions and results</a></li>
<li class="chapter" data-level="9.3" data-path="append.html"><a href="append.html#variousResults"><i class="fa fa-check"></i><b>9.3</b> Statistical analysis: definitions and results</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="append.html"><a href="append.html#moments-and-statistics"><i class="fa fa-check"></i><b>9.3.1</b> Moments and statistics</a></li>
<li class="chapter" data-level="9.3.2" data-path="append.html"><a href="append.html#standard-distributions"><i class="fa fa-check"></i><b>9.3.2</b> Standard distributions</a></li>
<li class="chapter" data-level="9.3.3" data-path="append.html"><a href="append.html#StochConvergences"><i class="fa fa-check"></i><b>9.3.3</b> Stochastic convergences</a></li>
<li class="chapter" data-level="9.3.4" data-path="append.html"><a href="append.html#CLTappend"><i class="fa fa-check"></i><b>9.3.4</b> Central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="append.html"><a href="append.html#GaussianVar"><i class="fa fa-check"></i><b>9.4</b> Some properties of Gaussian variables</a></li>
<li class="chapter" data-level="9.5" data-path="append.html"><a href="append.html#AppendixProof"><i class="fa fa-check"></i><b>9.5</b> Proofs</a></li>
<li class="chapter" data-level="9.6" data-path="append.html"><a href="append.html#additional-codes"><i class="fa fa-check"></i><b>9.6</b> Additional codes</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="append.html"><a href="append.html#App:GEV"><i class="fa fa-check"></i><b>9.6.1</b> Simulating GEV distributions</a></li>
<li class="chapter" data-level="9.6.2" data-path="append.html"><a href="append.html#IRFDELTA"><i class="fa fa-check"></i><b>9.6.2</b> Computing the covariance matrix of IRF using the delta method</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="append.html"><a href="append.html#statistical-tables"><i class="fa fa-check"></i><b>9.7</b> Statistical Tables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometrics and Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="append" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Appendix<a href="append.html#append" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="PCAapp" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Principal component analysis (PCA)<a href="append.html#PCAapp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Principal component analysis (PCA)</strong> is a classical and easy-to-use statistical method to reduce the dimension of large datasets containing variables that are linearly driven by a relatively small number of factors. This approach is widely used in data analysis and image compression.</p>
<p>Suppose that we have <span class="math inline">\(T\)</span> observations of a <span class="math inline">\(n\)</span>-dimensional random vector <span class="math inline">\(x\)</span>, denoted by <span class="math inline">\(x_{1},x_{2},\ldots,x_{T}\)</span>. We suppose that each component of <span class="math inline">\(x\)</span> is of mean zero.</p>
<p>Let denote with <span class="math inline">\(X\)</span> the matrix given by <span class="math inline">\(\left[\begin{array}{cccc} x_{1} &amp; x_{2} &amp; \ldots &amp; x_{T}\end{array}\right]&#39;\)</span>. Denote the <span class="math inline">\(j^{th}\)</span> column of <span class="math inline">\(X\)</span> by <span class="math inline">\(X_{j}\)</span>.</p>
<p>We want to find the linear combination of the <span class="math inline">\(x_{i}\)</span>’s (<span class="math inline">\(x.u\)</span>), with <span class="math inline">\(\left\Vert u\right\Vert =1\)</span>, with “maximum variance.” That is, we want to solve:
<span class="math display" id="eq:PCA11">\[\begin{equation}
\begin{array}{clll}
\underset{u}{\arg\max} &amp; u&#39;X&#39;Xu. \\
\mbox{s.t. } &amp; \left| u\right| =1
\end{array}\tag{9.1}
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(X&#39;X\)</span> is a positive definite matrix, it admits the following decomposition:
<span class="math display">\[\begin{eqnarray*}
X&#39;X &amp; = &amp; PDP&#39;\\
&amp; = &amp; P\left[\begin{array}{ccc}
\lambda_{1}\\
&amp; \ddots\\
&amp;  &amp; \lambda_{n}
\end{array}\right]P&#39;,
\end{eqnarray*}\]</span>
where <span class="math inline">\(P\)</span> is an orthogonal matrix whose columns are the eigenvectors of <span class="math inline">\(X&#39;X\)</span>.</p>
<p>We can order the eigenvalues such that <span class="math inline">\(\lambda_{1}\geq\ldots\geq\lambda_{n}\)</span>. (Since <span class="math inline">\(X&#39;X\)</span> is positive definite, all these eigenvalues are positive.)</p>
<p>Since <span class="math inline">\(P\)</span> is orthogonal, we have <span class="math inline">\(u&#39;X&#39;Xu=u&#39;PDP&#39;u=y&#39;Dy\)</span> where <span class="math inline">\(\left\Vert y\right\Vert =1\)</span>. Therefore, we have <span class="math inline">\(y_{i}^{2}\leq 1\)</span> for any <span class="math inline">\(i\leq n\)</span>.</p>
<p>As a consequence:
<span class="math display">\[
y&#39;Dy=\sum_{i=1}^{n}y_{i}^{2}\lambda_{i}\leq\lambda_{1}\sum_{i=1}^{n}y_{i}^{2}=\lambda_{1}.
\]</span></p>
<p>It is easily seen that the maximum is reached for <span class="math inline">\(y=\left[1,0,\cdots,0\right]&#39;\)</span>. Therefore, the maximum of the optimization program (Eq. <a href="append.html#eq:PCA11">(9.1)</a>) is obtained for <span class="math inline">\(u=P\left[1,0,\cdots,0\right]&#39;\)</span>. That is, <span class="math inline">\(u\)</span> is the eigenvector of <span class="math inline">\(X&#39;X\)</span> that is associated with its larger eigenvalue (first column of <span class="math inline">\(P\)</span>).</p>
<p>Let us denote with <span class="math inline">\(F\)</span> the vector that is given by the matrix product <span class="math inline">\(XP\)</span> (note that its last column is equal to <span class="math inline">\(Xu\)</span>). The columns of <span class="math inline">\(F\)</span>, denoted by <span class="math inline">\(F_{j}\)</span>, are called <strong>factors</strong>. We have:
<span class="math display">\[
F&#39;F=P&#39;X&#39;XP=D.
\]</span>
Therefore, in particular, the <span class="math inline">\(F_{j}\)</span>’s are orthogonal.</p>
<p>Since <span class="math inline">\(X=FP&#39;\)</span>, the <span class="math inline">\(X_{j}\)</span>’s are linear combinations of the factors. Let us then denote with <span class="math inline">\(\hat{X}_{i,j}\)</span> the part of <span class="math inline">\(X_{i}\)</span> that is explained by factor <span class="math inline">\(F_{j}\)</span>, we have:
<span class="math display">\[\begin{eqnarray*}
\hat{X}_{i,j} &amp; = &amp; p_{ij}F_{j}\\
X_{i} &amp; = &amp; \sum_{j}\hat{X}_{i,j}=\sum_{j}p_{ij}F_{j}.
\end{eqnarray*}\]</span></p>
<p>Consider the share of variance that is explained –through the <span class="math inline">\(n\)</span> variables (<span class="math inline">\(X_{1},\ldots,X_{n}\)</span>)– by the first factor <span class="math inline">\(F_{1}\)</span>:
<span class="math display">\[\begin{eqnarray*}
\frac{\sum_{i}\hat{X}_{i,1}\hat{X}&#39;_{i,1}}{\sum_{i}X_{i}X&#39;_{i}} &amp; = &amp; \frac{\sum_{i}p_{i1}F_{1}F&#39;_{1}p_{i1}}{tr(X&#39;X)} = \frac{\sum_{i}p_{i1}^{2}\lambda_{1}}{tr(X&#39;X)} = \frac{\lambda_{1}}{\sum_{i}\lambda_{i}}.
\end{eqnarray*}\]</span></p>
<p>Intuitively, if the first eigenvalue is large, it means that the first factor embed a large share of the fluctutaions of the <span class="math inline">\(n\)</span> <span class="math inline">\(X_{i}\)</span>’s.</p>
<p>Let us illustrate PCA on the term structure of yields. The term strucutre of yields (or yield curve) is know to be driven by only a small number of factors (e.g., <span class="citation">Litterman and Scheinkman (<a href="#ref-Litterman_Scheinkman_1991" role="doc-biblioref">1991</a>)</span>). One can typically employ PCA to recover such factors. The data used in the example below are taken from the <a href="https://fred.stlouisfed.org">Fred database</a> (tickers: “DGS6MO”,“DGS1”, …). The second plot shows the factor loardings, that indicate that the first factor is a level factor (loadings = black line), the second factor is a slope factor (loadings = blue line), the third factor is a curvature factor (loadings = red line).</p>
<p>To run a PCA, one simply has to apply function <code>prcomp</code> to a matrix of data:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="append.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(AEC)</span>
<span id="cb1-2"><a href="append.html#cb1-2" aria-hidden="true" tabindex="-1"></a>USyields <span class="ot">&lt;-</span> USyields[<span class="fu">complete.cases</span>(USyields),]</span>
<span id="cb1-3"><a href="append.html#cb1-3" aria-hidden="true" tabindex="-1"></a>yds <span class="ot">&lt;-</span> USyields[<span class="fu">c</span>(<span class="st">&quot;Y1&quot;</span>,<span class="st">&quot;Y2&quot;</span>,<span class="st">&quot;Y3&quot;</span>,<span class="st">&quot;Y5&quot;</span>,<span class="st">&quot;Y7&quot;</span>,<span class="st">&quot;Y10&quot;</span>,<span class="st">&quot;Y20&quot;</span>,<span class="st">&quot;Y30&quot;</span>)]</span>
<span id="cb1-4"><a href="append.html#cb1-4" aria-hidden="true" tabindex="-1"></a>PCA.yds <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(yds,<span class="at">center=</span><span class="cn">TRUE</span>,<span class="at">scale. =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Let us know visualize some results. The first plot of Figure <a href="append.html#fig:USydsPCA1">9.1</a> shows the share of total variance explained by the different principal components (PCs). The second plot shows the facotr loadings. The two bottom plots show how yields (in black) are fitted by linear combinations of the first two PCs only.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="append.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb2-2"><a href="append.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">plt=</span><span class="fu">c</span>(.<span class="dv">1</span>,.<span class="dv">95</span>,.<span class="dv">2</span>,.<span class="dv">8</span>))</span>
<span id="cb2-3"><a href="append.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(PCA.yds<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="fu">sum</span>(PCA.yds<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb2-4"><a href="append.html#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span><span class="st">&quot;Share of variance expl. by PC&#39;s&quot;</span>)</span>
<span id="cb2-5"><a href="append.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(yds)[<span class="dv">2</span>], <span class="at">labels=</span><span class="fu">colnames</span>(PCA.yds<span class="sc">$</span>x))</span>
<span id="cb2-6"><a href="append.html#cb2-6" aria-hidden="true" tabindex="-1"></a>nb.PC <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb2-7"><a href="append.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="sc">-</span>PCA.yds<span class="sc">$</span>rotation[,<span class="dv">1</span>],<span class="at">type=</span><span class="st">&quot;l&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="cb2-8"><a href="append.html#cb2-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Factor loadings (1st 3 PCs)&quot;</span>,<span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb2-9"><a href="append.html#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(yds)[<span class="dv">2</span>], <span class="at">labels=</span><span class="fu">colnames</span>(yds))</span>
<span id="cb2-10"><a href="append.html#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(PCA.yds<span class="sc">$</span>rotation[,<span class="dv">2</span>],<span class="at">type=</span><span class="st">&quot;l&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb2-11"><a href="append.html#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(PCA.yds<span class="sc">$</span>rotation[,<span class="dv">3</span>],<span class="at">type=</span><span class="st">&quot;l&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb2-12"><a href="append.html#cb2-12" aria-hidden="true" tabindex="-1"></a>Y1.hat <span class="ot">&lt;-</span> PCA.yds<span class="sc">$</span>x[,<span class="dv">1</span><span class="sc">:</span>nb.PC] <span class="sc">%*%</span> PCA.yds<span class="sc">$</span>rotation[<span class="st">&quot;Y1&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span>
<span id="cb2-13"><a href="append.html#cb2-13" aria-hidden="true" tabindex="-1"></a>Y1.hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(USyields<span class="sc">$</span>Y1) <span class="sc">+</span> <span class="fu">sd</span>(USyields<span class="sc">$</span>Y1) <span class="sc">*</span> Y1.hat</span>
<span id="cb2-14"><a href="append.html#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(USyields<span class="sc">$</span>date,USyields<span class="sc">$</span>Y1,<span class="at">type=</span><span class="st">&quot;l&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb2-15"><a href="append.html#cb2-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Fit of 1-year yields (2 PCs)&quot;</span>,</span>
<span id="cb2-16"><a href="append.html#cb2-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Obs (black) / Fitted by 2PCs (dashed blue)&quot;</span>)</span>
<span id="cb2-17"><a href="append.html#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(USyields<span class="sc">$</span>date,Y1.hat,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb2-18"><a href="append.html#cb2-18" aria-hidden="true" tabindex="-1"></a>Y10.hat <span class="ot">&lt;-</span> PCA.yds<span class="sc">$</span>x[,<span class="dv">1</span><span class="sc">:</span>nb.PC] <span class="sc">%*%</span> PCA.yds<span class="sc">$</span>rotation[<span class="st">&quot;Y10&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span>
<span id="cb2-19"><a href="append.html#cb2-19" aria-hidden="true" tabindex="-1"></a>Y10.hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(USyields<span class="sc">$</span>Y10) <span class="sc">+</span> <span class="fu">sd</span>(USyields<span class="sc">$</span>Y10) <span class="sc">*</span> Y10.hat</span>
<span id="cb2-20"><a href="append.html#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(USyields<span class="sc">$</span>date,USyields<span class="sc">$</span>Y10,<span class="at">type=</span><span class="st">&quot;l&quot;</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb2-21"><a href="append.html#cb2-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Fit of 10-year yields (2 PCs)&quot;</span>,</span>
<span id="cb2-22"><a href="append.html#cb2-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Obs (black) / Fitted by 2PCs (dashed blue)&quot;</span>)</span>
<span id="cb2-23"><a href="append.html#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(USyields<span class="sc">$</span>date,Y10.hat,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: left-aligned"><span style="display:block;" id="fig:USydsPCA1"></span>
<img src="EcoStat_files/figure-html/USydsPCA1-1.png" alt="Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities." width="672" />
<p class="caption">
Figure 9.1: Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities.
</p>
</div>
</div>
<div id="LinAlgebra" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Linear algebra: definitions and results<a href="append.html#LinAlgebra" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:determinant" class="definition"><strong>Definition 9.1  (Eigenvalues) </strong></span>The eigenvalues of of a matrix <span class="math inline">\(M\)</span> are the numbers <span class="math inline">\(\lambda\)</span> for which:
<span class="math display">\[
|M - \lambda I| = 0,
\]</span>
where <span class="math inline">\(| \bullet |\)</span> is the determinant operator.</p>
</div>
<div class="proposition">
<p><span id="prp:determinant" class="proposition"><strong>Proposition 9.1  (Properties of the determinant) </strong></span>We have:</p>
<ul>
<li><span class="math inline">\(|MN|=|M|\times|N|\)</span>.</li>
<li><span class="math inline">\(|M^{-1}|=|M|^{-1}\)</span>.</li>
<li>If <span class="math inline">\(M\)</span> admits the diagonal representation <span class="math inline">\(M=TDT^{-1}\)</span>, where <span class="math inline">\(D\)</span> is a diagonal matrix whose diagonal entries are <span class="math inline">\(\{\lambda_i\}_{i=1,\dots,n}\)</span>, then:
<span class="math display">\[
|M - \lambda I |=\prod_{i=1}^n (\lambda_i - \lambda).
\]</span></li>
</ul>
</div>
<div class="definition">
<p><span id="def:MoorPenrose" class="definition"><strong>Definition 9.2  (Moore-Penrose inverse) </strong></span>If <span class="math inline">\(M \in \mathbb{R}^{m \times n}\)</span>, then its Moore-Penrose pseudo inverse (exists and) is the unique matrix <span class="math inline">\(M^* \in \mathbb{R}^{n \times m}\)</span> that satisfies:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(M M^* M = M\)</span></li>
<li><span class="math inline">\(M^* M M^* = M^*\)</span></li>
<li><span class="math inline">\((M M^*)&#39;=M M^*\)</span>
.iv <span class="math inline">\((M^* M)&#39;=M^* M\)</span>.</li>
</ol>
</div>
<div class="proposition">
<ul>
<li><span id="prp:MoorPenrose" class="proposition"><strong>Proposition 9.2  (Properties of the Moore-Penrose inverse) </strong></span></li>
<li>If <span class="math inline">\(M\)</span> is invertible then <span class="math inline">\(M^* = M^{-1}\)</span>.</li>
<li>The pseudo-inverse of a zero matrix is its transpose.
*
The pseudo-inverse of the pseudo-inverse is the original matrix.</li>
</ul>
</div>
<div class="definition">
<p><span id="def:idempotent" class="definition"><strong>Definition 9.3  (Idempotent matrix) </strong></span>Matrix <span class="math inline">\(M\)</span> is idempotent if <span class="math inline">\(M^2=M\)</span>.</p>
<p>If <span class="math inline">\(M\)</span> is a symmetric idempotent matrix, then <span class="math inline">\(M&#39;M=M\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:rootsidempotent" class="proposition"><strong>Proposition 9.3  (Roots of an idempotent matrix) </strong></span>The eigenvalues of an idempotent matrix are either 1 or 0.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>If <span class="math inline">\(\lambda\)</span> is an eigenvalue of an idempotent matrix <span class="math inline">\(M\)</span> then <span class="math inline">\(\exists x \ne 0\)</span> s.t. <span class="math inline">\(Mx=\lambda x\)</span>. Hence <span class="math inline">\(M^2x=\lambda M x \Rightarrow (1-\lambda)Mx=0\)</span>. Either all element of <span class="math inline">\(Mx\)</span> are zero, in which case <span class="math inline">\(\lambda=0\)</span> or at least one element of <span class="math inline">\(Mx\)</span> is nonzero, in which case <span class="math inline">\(\lambda=1\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:chi2idempotent" class="proposition"><strong>Proposition 9.4  (Idempotent matrix and chi-square distribution) </strong></span>The rank of a symmetric idempotent matrix is equal to its trace.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>The result follows from Prop. <a href="append.html#prp:rootsidempotent">9.3</a>, combined with the fact that the rank of a symmetric matrix is equal to the number of its nonzero eigenvalues.</p>
</div>
<div class="proposition">
<p><span id="prp:constrainedLS" class="proposition"><strong>Proposition 9.5  (Constrained least squares) </strong></span>The solution of the following optimisation problem:
<span class="math display">\[\begin{eqnarray*}
\underset{\boldsymbol\beta}{\min} &amp;&amp; || \bv{y} - \bv{X}\boldsymbol\beta ||^2 \\
&amp;&amp; \mbox{subject to } \bv{R}\boldsymbol\beta = \bv{q}
\end{eqnarray*}\]</span>
is given by:
<span class="math display">\[
\boxed{\boldsymbol\beta^r = \boldsymbol\beta_0 - (\bv{X}&#39;\bv{X})^{-1} \bv{R}&#39;\{\bv{R}(\bv{X}&#39;\bv{X})^{-1}\bv{R}&#39;\}^{-1}(\bv{R}\boldsymbol\beta_0 - \bv{q}),}
\]</span>
where <span class="math inline">\(\boldsymbol\beta_0=(\bv{X}&#39;\bv{X})^{-1}\bv{X}&#39;\bv{y}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>See for instance <a href="http://jackman.stanford.edu/classes/350B/07/ftestforWeb.pdf">Jackman, 2007</a>.</p>
</div>
<div class="proposition">
<p><span id="prp:inversepartitioned" class="proposition"><strong>Proposition 9.6  (Inverse of a partitioned matrix) </strong></span>We have:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\left[ \begin{array}{cc} \bv{A}_{11} &amp; \bv{A}_{12} \\ \bv{A}_{21} &amp; \bv{A}_{22} \end{array}\right]^{-1} = \\
&amp;&amp;\left[ \begin{array}{cc} (\bv{A}_{11} - \bv{A}_{12}\bv{A}_{22}^{-1}\bv{A}_{21})^{-1} &amp; - \bv{A}_{11}^{-1}\bv{A}_{12}(\bv{A}_{22} - \bv{A}_{21}\bv{A}_{11}^{-1}\bv{A}_{12})^{-1} \\
-(\bv{A}_{22} - \bv{A}_{21}\bv{A}_{11}^{-1}\bv{A}_{12})^{-1}\bv{A}_{21}\bv{A}_{11}^{-1} &amp; (\bv{A}_{22} - \bv{A}_{21}\bv{A}_{11}^{-1}\bv{A}_{12})^{-1} \end{array} \right].
\end{eqnarray*}\]</span></p>
</div>
<div class="definition">
<p><span id="def:FOD" class="definition"><strong>Definition 9.4  (Matrix derivatives) </strong></span>Consider a fonction <span class="math inline">\(f: \mathbb{R}^K \rightarrow \mathbb{R}\)</span>. Its first-order derivative is:
<span class="math display">\[
\frac{\partial f}{\partial \bv{b}}(\bv{b}) =
\left[\begin{array}{c}
\frac{\partial f}{\partial b_1}(\bv{b})\\
\vdots\\
\frac{\partial f}{\partial b_K}(\bv{b})
\end{array}
\right].
\]</span>
We use the notation:
<span class="math display">\[
\frac{\partial f}{\partial \bv{b}&#39;}(\bv{b}) = \left(\frac{\partial f}{\partial \bv{b}}(\bv{b})\right)&#39;.
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:partial" class="proposition"><strong>Proposition 9.7  </strong></span>We have:</p>
<ul>
<li>If <span class="math inline">\(f(\bv{b}) = A&#39; \bv{b}\)</span> where <span class="math inline">\(A\)</span> is a <span class="math inline">\(K \times 1\)</span> vector then <span class="math inline">\(\frac{\partial f}{\partial \bv{b}}(\bv{b}) = A\)</span>.</li>
<li>If <span class="math inline">\(f(\bv{b}) = \bv{b}&#39;A\bv{b}\)</span> where <span class="math inline">\(A\)</span> is a <span class="math inline">\(K \times K\)</span> matrix, then <span class="math inline">\(\frac{\partial f}{\partial \bv{b}}(\bv{b}) = 2A\bv{b}\)</span>.</li>
</ul>
</div>
<div class="proposition">
<p><span id="prp:absMs" class="proposition"><strong>Proposition 9.8  (Square and absolute summability) </strong></span>We have:
<span class="math display">\[
\underbrace{\sum_{i=0}^{\infty}|\theta_i| &lt; + \infty}_{\mbox{Absolute summability}} \Rightarrow \underbrace{\sum_{i=0}^{\infty} \theta_i^2 &lt; + \infty}_{\mbox{Square summability}}.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>See Appendix 3.A in Hamilton. Idea: Absolute summability implies that there exist <span class="math inline">\(N\)</span> such that, for <span class="math inline">\(j&gt;N\)</span>, <span class="math inline">\(|\theta_j| &lt; 1\)</span> (deduced from Cauchy criterion, Theorem <a href="append.html#thm:cauchycritstatic">9.2</a> and therefore <span class="math inline">\(\theta_j^2 &lt; |\theta_j|\)</span>.</p>
</div>
</div>
<div id="variousResults" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Statistical analysis: definitions and results<a href="append.html#variousResults" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="moments-and-statistics" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Moments and statistics<a href="append.html#moments-and-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:partialcorrel" class="definition"><strong>Definition 9.5  (Partial correlation) </strong></span>The <strong>partial correlation</strong> between <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span>, controlling for some variables <span class="math inline">\(\bv{X}\)</span> is the sample correlation between <span class="math inline">\(y^*\)</span> and <span class="math inline">\(z^*\)</span>, where the latter two variables are the residuals in regressions of <span class="math inline">\(y\)</span> on <span class="math inline">\(\bv{X}\)</span> and of <span class="math inline">\(z\)</span> on <span class="math inline">\(\bv{X}\)</span>, respectively.</p>
<p>This correlation is denoted by <span class="math inline">\(r_{yz}^\bv{X}\)</span>. By definition, we have:
<span class="math display" id="eq:pc">\[\begin{equation}
r_{yz}^\bv{X} = \frac{\bv{z^*}&#39;\bv{y^*}}{\sqrt{(\bv{z^*}&#39;\bv{z^*})(\bv{y^*}&#39;\bv{y^*})}}.\tag{9.2}
\end{equation}\]</span></p>
</div>
<div class="definition">
<p><span id="def:skewnesskurtosis" class="definition"><strong>Definition 9.6  (Skewness and kurtosis) </strong></span>Let <span class="math inline">\(Y\)</span> be a random variable whose fourth moment exists. The expectation of <span class="math inline">\(Y\)</span> is denoted by <span class="math inline">\(\mu\)</span>.</p>
<ul>
<li>The skewness of <span class="math inline">\(Y\)</span> is given by:
<span class="math display">\[
\frac{\mathbb{E}[(Y-\mu)^3]}{\{\mathbb{E}[(Y-\mu)^2]\}^{3/2}}.
\]</span></li>
<li>The kurtosis of <span class="math inline">\(Y\)</span> is given by:
<span class="math display">\[
\frac{\mathbb{E}[(Y-\mu)^4]}{\{\mathbb{E}[(Y-\mu)^2]\}^{2}}.
\]</span></li>
</ul>
</div>
<div class="theorem">
<p><span id="thm:CauchySchwarz" class="theorem"><strong>Theorem 9.1  (Cauchy-Schwarz inequality) </strong></span>We have:
<span class="math display">\[
|\mathbb{C}ov(X,Y)| \le \sqrt{\mathbb{V}ar(X)\mathbb{V}ar(Y)}
\]</span>
and, if <span class="math inline">\(X \ne =\)</span> and <span class="math inline">\(Y \ne 0\)</span>, the equality holds iff <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the same up to an affine transformation.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>If <span class="math inline">\(\mathbb{V}ar(X)=0\)</span>, this is trivial. If this is not the case, then let’s define <span class="math inline">\(Z\)</span> as <span class="math inline">\(Z = Y - \frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X\)</span>. It is easily seen that <span class="math inline">\(\mathbb{C}ov(X,Z)=0\)</span>. Then, the variance of <span class="math inline">\(Y=Z+\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X\)</span> is equal to the sum of the variance of <span class="math inline">\(Z\)</span> and of the variance of <span class="math inline">\(\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X\)</span>, that is:
<span class="math display">\[
\mathbb{V}ar(Y) = \mathbb{V}ar(Z) + \left(\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}\right)^2\mathbb{V}ar(X) \ge \left(\frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}\right)^2\mathbb{V}ar(X).
\]</span>
The equality holds iff <span class="math inline">\(\mathbb{V}ar(Z)=0\)</span>, i.e. iff <span class="math inline">\(Y = \frac{\mathbb{C}ov(X,Y)}{\mathbb{V}ar(X)}X+cst\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:asmyptlevel" class="definition"><strong>Definition 9.7  (Asymptotic level) </strong></span>An asymptotic test with critical region <span class="math inline">\(\Omega_n\)</span> has an asymptotic level equal to <span class="math inline">\(\alpha\)</span> if:
<span class="math display">\[
\underset{\theta \in \Theta}{\mbox{sup}} \quad \underset{n \rightarrow \infty}{\mbox{lim}} \mathbb{P}_\theta (S_n \in \Omega_n) = \alpha,
\]</span>
where <span class="math inline">\(S_n\)</span> is the test statistic and <span class="math inline">\(\Theta\)</span> is such that the null hypothesis <span class="math inline">\(H_0\)</span> is equivalent to <span class="math inline">\(\theta \in \Theta\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:asmyptconsisttest" class="definition"><strong>Definition 9.8  (Asymptotically consistent test) </strong></span>An asymptotic test with critical region <span class="math inline">\(\Omega_n\)</span> is consistent if:
<span class="math display">\[
\forall \theta \in \Theta^c, \quad \mathbb{P}_\theta (S_n \in \Omega_n) \rightarrow 1,
\]</span>
where <span class="math inline">\(S_n\)</span> is the test statistic and <span class="math inline">\(\Theta^c\)</span> is such that the null hypothesis <span class="math inline">\(H_0\)</span> is equivalent to <span class="math inline">\(\theta \notin \Theta^c\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:Kullback" class="definition"><strong>Definition 9.9  (Kullback discrepancy) </strong></span>Given two p.d.f. <span class="math inline">\(f\)</span> and <span class="math inline">\(f^*\)</span>, the Kullback discrepancy is defined by:
<span class="math display">\[
I(f,f^*) = \mathbb{E}^* \left( \log \frac{f^*(Y)}{f(Y)} \right) = \int \log \frac{f^*(y)}{f(y)} f^*(y) dy.
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:Kullback" class="proposition"><strong>Proposition 9.9  (Properties of the Kullback discrepancy) </strong></span>We have:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(I(f,f^*) \ge 0\)</span></li>
<li><span class="math inline">\(I(f,f^*) = 0\)</span> iff <span class="math inline">\(f \equiv f^*\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span><span class="math inline">\(x \rightarrow -\log(x)\)</span> is a convex function. Therefore <span class="math inline">\(\mathbb{E}^*(-\log f(Y)/f^*(Y)) \ge -\log \mathbb{E}^*(f(Y)/f^*(Y)) = 0\)</span> (proves (i)). Since <span class="math inline">\(x \rightarrow -\log(x)\)</span> is strictly convex, equality in (i) holds if and only if <span class="math inline">\(f(Y)/f^*(Y)\)</span> is constant (proves (ii)).</p>
</div>
<div class="definition">
<p><span id="def:characteristic" class="definition"><strong>Definition 9.10  (Characteristic function) </strong></span>For any real-valued random variable <span class="math inline">\(X\)</span>, the characteristic function is defined by:
<span class="math display">\[
\phi_X: u \rightarrow \mathbb{E}[\exp(iuX)].
\]</span></p>
</div>
</div>
<div id="standard-distributions" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Standard distributions<a href="append.html#standard-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:fstatistics" class="definition"><strong>Definition 9.11  (F distribution) </strong></span>Consider <span class="math inline">\(n=n_1+n_2\)</span> i.i.d. <span class="math inline">\(\mathcal{N}(0,1)\)</span> r.v. <span class="math inline">\(X_i\)</span>. If the r.v. <span class="math inline">\(F\)</span> is defined by:
<span class="math display">\[
F = \frac{\sum_{i=1}^{n_1} X_i^2}{\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\frac{n_2}{n_1}
\]</span>
then <span class="math inline">\(F \sim \mathcal{F}(n_1,n_2)\)</span>. (See Table <a href="append.html#tab:Fstat">9.4</a> for quantiles.)</p>
</div>
<div class="definition">
<p><span id="def:tStudent" class="definition"><strong>Definition 9.12  (Student-t distribution) </strong></span><span class="math inline">\(Z\)</span> follows a Student-t (or <span class="math inline">\(t\)</span>) distribution with <span class="math inline">\(\nu\)</span> degrees of freedom (d.f.) if:
<span class="math display">\[
Z = X_0 \bigg/ \sqrt{\frac{\sum_{i=1}^{\nu}X_i^2}{\nu}}, \quad X_i \sim i.i.d. \mathcal{N}(0,1).
\]</span>
We have <span class="math inline">\(\mathbb{E}(Z)=0\)</span>, and <span class="math inline">\(\mathbb{V}ar(Z)=\frac{\nu}{\nu-2}\)</span> if <span class="math inline">\(\nu&gt;2\)</span>. (See Table <a href="append.html#tab:Student">9.2</a> for quantiles.)</p>
</div>
<div class="definition">
<p><span id="def:chi2" class="definition"><strong>Definition 9.13  (Chi-square distribution) </strong></span><span class="math inline">\(Z\)</span> follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(\nu\)</span> d.f. if <span class="math inline">\(Z = \sum_{i=1}^{\nu}X_i^2\)</span> where <span class="math inline">\(X_i \sim i.i.d. \mathcal{N}(0,1)\)</span>.
We have <span class="math inline">\(\mathbb{E}(Z)=\nu\)</span>. (See Table <a href="append.html#tab:Chi2">9.3</a> for quantiles.)</p>
</div>
<div class="definition">
<span id="def:Cauchy" class="definition"><strong>Definition 9.14  (Cauchy distribution) </strong></span>The probability distribution function of the Cauchy distribution defined by a location parameter <span class="math inline">\(\mu\)</span> and a scale parameter <span class="math inline">\(\gamma\)</span> is:
<span class="math display">\[
f(x) = \frac{1}{\pi \gamma \left(1 + \left[\frac{x-\mu}{\gamma}\right]^2\right)}.
\]</span>
The mean and variance of this distribution are undefined.
<div class="figure" style="text-align: left-aligned"><span style="display:block;" id="fig:Cauchy"></span>
<img src="EcoStat_files/figure-html/Cauchy-1.png" alt="Pdf of the Cauchy distribution ($\mu=0$, $\gamma=1$)." width="95%" />
<p class="caption">
Figure 9.2: Pdf of the Cauchy distribution (<span class="math inline">\(\mu=0\)</span>, <span class="math inline">\(\gamma=1\)</span>).
</p>
</div>
</div>
<div class="proposition">
<p><span id="prp:waldtypeproduct" class="proposition"><strong>Proposition 9.10  (Inner product of a multivariate Gaussian variable) </strong></span>Let <span class="math inline">\(X\)</span> be a <span class="math inline">\(n\)</span>-dimensional multivariate Gaussian variable: <span class="math inline">\(X \sim \mathcal{N}(0,\Sigma)\)</span>. We have:
<span class="math display">\[
X&#39; \Sigma^{-1}X \sim \chi^2(n).
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>Because <span class="math inline">\(\Sigma\)</span> is a symmetrical definite positive matrix, it admits the spectral decomposition <span class="math inline">\(PDP&#39;\)</span> where <span class="math inline">\(P\)</span> is an orthogonal matrix (i.e. <span class="math inline">\(PP&#39;=Id\)</span>) and D is a diagonal matrix with non-negative entries. Denoting by <span class="math inline">\(\sqrt{D^{-1}}\)</span> the diagonal matrix whose diagonal entries are the inverse of those of <span class="math inline">\(D\)</span>, it is easily checked that the covariance matrix of <span class="math inline">\(Y:=\sqrt{D^{-1}}P&#39;X\)</span> is <span class="math inline">\(Id\)</span>. Therefore <span class="math inline">\(Y\)</span> is a vector of uncorrelated Gaussian variables. The properties of Gaussian variables imply that the components of <span class="math inline">\(Y\)</span> are then also independent. Hence <span class="math inline">\(Y&#39;Y=\sum_i Y_i^2 \sim \chi^2(n)\)</span>.</p>
<p>It remains to note that <span class="math inline">\(Y&#39;Y=X&#39;PD^{-1}P&#39;X=X&#39;\mathbb{V}ar(X)^{-1}X\)</span> to conclude.</p>
</div>
<div class="definition">
<p><span id="def:GEVdistri" class="definition"><strong>Definition 9.15  (Generalized Extreme Value (GEV) distribution) </strong></span>The vector of disturbances <span class="math inline">\(\boldsymbol\varepsilon=[\varepsilon_{1,1},\dots,\varepsilon_{1,K_1},\dots,\varepsilon_{J,1},\dots,\varepsilon_{J,K_J}]&#39;\)</span> follows the Generalized Extreme Value (GEV) distribution if its c.d.f. is:
<span class="math display">\[
F(\boldsymbol\varepsilon,\boldsymbol\rho) = \exp(-G(e^{-\varepsilon_{1,1}},\dots,e^{-\varepsilon_{J,K_J}};\boldsymbol\rho))
\]</span>
with
<span class="math display">\[\begin{eqnarray*}
G(\bv{Y};\boldsymbol\rho) &amp;\equiv&amp;  G(Y_{1,1},\dots,Y_{1,K_1},\dots,Y_{J,1},\dots,Y_{J,K_J};\boldsymbol\rho) \\
&amp;=&amp; \sum_{j=1}^J\left(\sum_{k=1}^{K_j} Y_{jk}^{1/\rho_j}
\right)^{\rho_j}
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="StochConvergences" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Stochastic convergences<a href="append.html#StochConvergences" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="proposition">
<p><span id="prp:chebychev" class="proposition"><strong>Proposition 9.11  (Chebychev's inequality) </strong></span>If <span class="math inline">\(\mathbb{E}(|X|^r)\)</span> is finite for some <span class="math inline">\(r&gt;0\)</span> then:
<span class="math display">\[
\forall \varepsilon &gt; 0, \quad \mathbb{P}(|X - c|&gt;\varepsilon) \le \frac{\mathbb{E}[|X - c|^r]}{\varepsilon^r}.
\]</span>
In particular, for <span class="math inline">\(r=2\)</span>:
<span class="math display">\[
\forall \varepsilon &gt; 0, \quad \mathbb{P}(|X - c|&gt;\varepsilon) \le \frac{\mathbb{E}[(X - c)^2]}{\varepsilon^2}.
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>Remark that <span class="math inline">\(\varepsilon^r \mathbb{I}_{\{|X| \ge \varepsilon\}} \le |X|^r\)</span> and take the expectation of both sides.</p>
</div>
<div class="definition">
<p><span id="def:convergenceproba" class="definition"><strong>Definition 9.16  (Convergence in probability) </strong></span>The random variable sequence <span class="math inline">\(x_n\)</span> converges in probability to a constant <span class="math inline">\(c\)</span> if <span class="math inline">\(\forall \varepsilon\)</span>, <span class="math inline">\(\lim_{n \rightarrow \infty} \mathbb{P}(|x_n - c|&gt;\varepsilon) = 0\)</span>.</p>
<p>It is denoted as: <span class="math inline">\(\mbox{plim } x_n = c\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:convergenceLr" class="definition"><strong>Definition 9.17  (Convergence in the Lr norm) </strong></span><span class="math inline">\(x_n\)</span> converges in the <span class="math inline">\(r\)</span>-th mean (or in the <span class="math inline">\(L^r\)</span>-norm) towards <span class="math inline">\(x\)</span>, if <span class="math inline">\(\mathbb{E}(|x_n|^r)\)</span> and <span class="math inline">\(\mathbb{E}(|x|^r)\)</span> exist and if
<span class="math display">\[
\lim_{n \rightarrow \infty} \mathbb{E}(|x_n - x|^r) = 0.
\]</span>
It is denoted as: <span class="math inline">\(x_n \overset{L^r}{\rightarrow} c\)</span>.</p>
<p>For <span class="math inline">\(r=2\)</span>, this convergence is called <strong>mean square convergence</strong>.</p>
</div>
<div class="definition">
<p><span id="def:convergenceAlmost" class="definition"><strong>Definition 9.18  (Almost sure convergence) </strong></span>The random variable sequence <span class="math inline">\(x_n\)</span> converges almost surely to <span class="math inline">\(c\)</span> if <span class="math inline">\(\mathbb{P}(\lim_{n \rightarrow \infty} x_n = c) = 1\)</span>.</p>
<p>It is denoted as: <span class="math inline">\(x_n \overset{a.s.}{\rightarrow} c\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:cvgceDistri" class="definition"><strong>Definition 9.19  (Convergence in distribution) </strong></span><span class="math inline">\(x_n\)</span> is said to converge in distribution (or in law) to <span class="math inline">\(x\)</span> if
<span class="math display">\[
\lim_{n \rightarrow \infty} F_{x_n}(s) = F_{x}(s)
\]</span>
for all <span class="math inline">\(s\)</span> at which <span class="math inline">\(F_X\)</span> –the cumulative distribution of <span class="math inline">\(X\)</span>– is continuous.</p>
<p>It is denoted as: <span class="math inline">\(x_n \overset{d}{\rightarrow} x\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:Slutsky" class="proposition"><strong>Proposition 9.12  (Rules for limiting distributions (Slutsky)) </strong></span>We have:</p>
<ol style="list-style-type: lower-roman">
<li><p><strong>Slutsky’s theorem:</strong> If <span class="math inline">\(x_n \overset{d}{\rightarrow} x\)</span> and <span class="math inline">\(y_n \overset{p}{\rightarrow} c\)</span> then
<span class="math display">\[\begin{eqnarray*}
x_n y_n &amp;\overset{d}{\rightarrow}&amp; x c \\
x_n + y_n &amp;\overset{d}{\rightarrow}&amp; x + c \\
x_n/y_n &amp;\overset{d}{\rightarrow}&amp; x / c \quad (\mbox{if }c \ne 0)
\end{eqnarray*}\]</span></p></li>
<li><p><strong>Continuous mapping theorem:</strong> If <span class="math inline">\(x_n \overset{d}{\rightarrow} x\)</span> and <span class="math inline">\(g\)</span> is a continuous function then <span class="math inline">\(g(x_n) \overset{d}{\rightarrow} g(x).\)</span></p></li>
</ol>
</div>
<div class="proposition">
<p><span id="prp:implicationsconv" class="proposition"><strong>Proposition 9.13  (Implications of stochastic convergences) </strong></span>We have:
<span class="math display">\[\begin{align*}
&amp;\boxed{\overset{L^s}{\rightarrow}}&amp; &amp;\underset{1 \le r \le s}{\Rightarrow}&amp; &amp;\boxed{\overset{L^r}{\rightarrow}}&amp;\\
&amp;&amp; &amp;&amp; &amp;\Downarrow&amp;\\
&amp;\boxed{\overset{a.s.}{\rightarrow}}&amp; &amp;\Rightarrow&amp; &amp;\boxed{\overset{p}{\rightarrow}}&amp; \Rightarrow \qquad \boxed{\overset{d}{\rightarrow}}.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>Proof</em>. </span>(of the fact that <span class="math inline">\(\left(\overset{p}{\rightarrow}\right) \Rightarrow \left( \overset{d}{\rightarrow}\right)\)</span>). Assume that <span class="math inline">\(X_n \overset{p}{\rightarrow} X\)</span>. Denoting by <span class="math inline">\(F\)</span> and <span class="math inline">\(F_n\)</span> the c.d.f. of <span class="math inline">\(X\)</span> and <span class="math inline">\(X_n\)</span>, respectively:
<span class="math display" id="eq:convgce1">\[\begin{eqnarray*}
F_n(x) &amp;=&amp; \mathbb{P}(X_n \le x,X\le x+\varepsilon) + \mathbb{P}(X_n \le x,X &gt; x+\varepsilon)\\
&amp;\le&amp; F(x+\varepsilon) + \mathbb{P}(|X_n - X|&gt;\varepsilon).\tag{9.3}
\end{eqnarray*}\]</span>
Besides,
<span class="math display">\[\begin{eqnarray*}
F(x-\varepsilon) &amp;=&amp; \mathbb{P}(X \le x-\varepsilon,X_n \le x) + \mathbb{P}(X \le x-\varepsilon,X_n &gt; x)\\
&amp;\le&amp; F_n(x) + \mathbb{P}(|X_n - X|&gt;\varepsilon),
\end{eqnarray*}\]</span>
which implies:
<span class="math display" id="eq:convgce2">\[\begin{equation}
F(x-\varepsilon) - \mathbb{P}(|X_n - X|&gt;\varepsilon) \le F_n(x).\tag{9.4}
\end{equation}\]</span>
Eqs. <a href="append.html#eq:convgce1">(9.3)</a> and <a href="append.html#eq:convgce2">(9.4)</a> imply:
<span class="math display">\[
F(x-\varepsilon) - \mathbb{P}(|X_n - X|&gt;\varepsilon) \le F_n(x)  \le F(x+\varepsilon) + \mathbb{P}(|X_n - X|&gt;\varepsilon).
\]</span>
Taking limits as <span class="math inline">\(n \rightarrow \infty\)</span> yields
<span class="math display">\[
F(x-\varepsilon) \le \underset{n \rightarrow \infty}{\mbox{lim inf}}\; F_n(x) \le \underset{n \rightarrow \infty}{\mbox{lim sup}}\; F_n(x)  \le F(x+\varepsilon).
\]</span>
The result is then obtained by taking limits as <span class="math inline">\(\varepsilon \rightarrow 0\)</span> (if <span class="math inline">\(F\)</span> is continuous at <span class="math inline">\(x\)</span>).</p>
</div>
<div class="proposition">
<p><span id="prp:cvgce11" class="proposition"><strong>Proposition 9.14  (Convergence in distribution to a constant) </strong></span>If <span class="math inline">\(X_n\)</span> converges in distribution to a constant <span class="math inline">\(c\)</span>, then <span class="math inline">\(X_n\)</span> converges in probability to <span class="math inline">\(c\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span>If <span class="math inline">\(\varepsilon&gt;0\)</span>, we have <span class="math inline">\(\mathbb{P}(X_n &lt; c - \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 0\)</span> i.e. <span class="math inline">\(\mathbb{P}(X_n \ge c - \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1\)</span> and <span class="math inline">\(\mathbb{P}(X_n &lt; c + \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1\)</span>. Therefore <span class="math inline">\(\mathbb{P}(c - \varepsilon \le X_n &lt; c + \varepsilon) \underset{n \rightarrow \infty}{\rightarrow} 1\)</span>,
which gives the result.</p>
</div>
<div class="example">
<p><span id="exm:plimButNotLr" class="example"><strong>Example 9.1  (Convergence in probability but not $L^r$) </strong></span>Let <span class="math inline">\(\{x_n\}_{n \in \mathbb{N}}\)</span> be a series of random variables defined by:
<span class="math display">\[
x_n = n u_n,
\]</span>
where <span class="math inline">\(u_n\)</span> are independent random variables s.t. <span class="math inline">\(u_n \sim \mathcal{B}(1/n)\)</span>.</p>
<p>We have <span class="math inline">\(x_n \overset{p}{\rightarrow} 0\)</span> but <span class="math inline">\(x_n \overset{L^r}{\nrightarrow} 0\)</span> because <span class="math inline">\(\mathbb{E}(|X_n-0|)=\mathbb{E}(X_n)=1\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:cauchycritstatic" class="theorem"><strong>Theorem 9.2  (Cauchy criterion (non-stochastic case)) </strong></span>We have that <span class="math inline">\(\sum_{i=0}^{T} a_i\)</span> converges (<span class="math inline">\(T \rightarrow \infty\)</span>) iff, for any <span class="math inline">\(\eta &gt; 0\)</span>, there exists an integer <span class="math inline">\(N\)</span> such that, for all <span class="math inline">\(M\ge N\)</span>,
<span class="math display">\[
\left|\sum_{i=N+1}^{M} a_i\right| &lt; \eta.
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:cauchycritstochastic" class="theorem"><strong>Theorem 9.3  (Cauchy criterion (stochastic case)) </strong></span>We have that <span class="math inline">\(\sum_{i=0}^{T} \theta_i \varepsilon_{t-i}\)</span> converges in mean square (<span class="math inline">\(T \rightarrow \infty\)</span>) to a random variable iff, for any <span class="math inline">\(\eta &gt; 0\)</span>, there exists an integer <span class="math inline">\(N\)</span> such that, for all <span class="math inline">\(M\ge N\)</span>,
<span class="math display">\[
\mathbb{E}\left[\left(\sum_{i=N+1}^{M} \theta_i \varepsilon_{t-i}\right)^2\right] &lt; \eta.
\]</span></p>
</div>
</div>
<div id="CLTappend" class="section level3 hasAnchor" number="9.3.4">
<h3><span class="header-section-number">9.3.4</span> Central limit theorem<a href="append.html#CLTappend" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:LLNappendix" class="theorem"><strong>Theorem 9.4  (Law of large numbers) </strong></span>The sample mean is a consistent estimator of the population mean.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span>Let’s denote by <span class="math inline">\(\phi_{X_i}\)</span> the characteristic function of a r.v. <span class="math inline">\(X_i\)</span>. If the mean of <span class="math inline">\(X_i\)</span> is <span class="math inline">\(\mu\)</span> then the Talyor expansion of the characteristic function is:
<span class="math display">\[
\phi_{X_i}(u) = \mathbb{E}(\exp(iuX)) = 1 + iu\mu + o(u).
\]</span>
The properties of the characteristic function (see Def. <a href="append.html#def:characteristic">9.10</a>) imply that:
<span class="math display">\[
\phi_{\frac{1}{n}(X_1+\dots+X_n)}(u) = \prod_{i=1}^{n} \left(1 + i\frac{u}{n}\mu + o\left(\frac{u}{n}\right) \right) \rightarrow e^{iu\mu}.
\]</span>
The facts that (a) <span class="math inline">\(e^{iu\mu}\)</span> is the characteristic function of the constant <span class="math inline">\(\mu\)</span> and (b) that a characteristic function uniquely characterises a distribution imply that the sample mean converges in distribution to the constant <span class="math inline">\(\mu\)</span>, which further implies that it converges in probability to <span class="math inline">\(\mu\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:LindbergLevyCLT" class="theorem"><strong>Theorem 9.5  (Lindberg-Levy Central limit theorem, CLT) </strong></span>If <span class="math inline">\(x_n\)</span> is an i.i.d. sequence of random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (<span class="math inline">\(\in ]0,+\infty[\)</span>), then:
<span class="math display">\[
\boxed{\sqrt{n} (\bar{x}_n - \mu) \overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2), \quad \mbox{where} \quad \bar{x}_n = \frac{1}{n} \sum_{i=1}^{n} x_i.}
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-12" class="proof"><em>Proof</em>. </span>Let us introduce the r.v. <span class="math inline">\(Y_n:= \sqrt{n}(\bar{X}_n - \mu)\)</span>. We have <span class="math inline">\(\phi_{Y_n}(u) = \left[ \mathbb{E}\left( \exp(i \frac{1}{\sqrt{n}} u (X_1 - \mu)) \right) \right]^n\)</span>. We have:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\left[ \mathbb{E}\left( \exp\left(i \frac{1}{\sqrt{n}} u (X_1 - \mu)\right) \right) \right]^n\\
&amp;=&amp; \left[ \mathbb{E}\left( 1 + i \frac{1}{\sqrt{n}} u (X_1 - \mu) - \frac{1}{2n} u^2 (X_1 - \mu)^2 + o(u^2) \right) \right]^n \\
&amp;=&amp; \left( 1 - \frac{1}{2n}u^2\sigma^2 + o(u^2)\right)^n.
\end{eqnarray*}\]</span>
Therefore <span class="math inline">\(\phi_{Y_n}(u) \underset{n \rightarrow \infty}{\rightarrow} \exp \left( - \frac{1}{2}u^2\sigma^2 \right)\)</span>, which is the characteristic function of <span class="math inline">\(\mathcal{N}(0,\sigma^2)\)</span>.</p>
</div>
</div>
</div>
<div id="GaussianVar" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Some properties of Gaussian variables<a href="append.html#GaussianVar" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="proposition">
<p><span id="prp:bandsindependent" class="proposition"><strong>Proposition 9.15  </strong></span>If <span class="math inline">\(\bv{A}\)</span> is idempotent and if <span class="math inline">\(\bv{x}\)</span> is Gaussian, <span class="math inline">\(\bv{L}\bv{x}\)</span> and <span class="math inline">\(\bv{x}&#39;\bv{A}\bv{x}\)</span> are independent if <span class="math inline">\(\bv{L}\bv{A}=\bv{0}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>If <span class="math inline">\(\bv{L}\bv{A}=\bv{0}\)</span>, then the two Gaussian vectors <span class="math inline">\(\bv{L}\bv{x}\)</span> and <span class="math inline">\(\bv{A}\bv{x}\)</span> are independent. This implies the independence of any function of <span class="math inline">\(\bv{L}\bv{x}\)</span> and any function of <span class="math inline">\(\bv{A}\bv{x}\)</span>. The results then follows from the observation that <span class="math inline">\(\bv{x}&#39;\bv{A}\bv{x}=(\bv{A}\bv{x})&#39;(\bv{A}\bv{x})\)</span>, which is a function of <span class="math inline">\(\bv{A}\bv{x}\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:update" class="proposition"><strong>Proposition 9.16  (Bayesian update in a vector of Gaussian variables) </strong></span>If
<span class="math display">\[
\left[
\begin{array}{c}
Y_1\\
Y_2
\end{array}
\right]
\sim \mathcal{N}
\left(0,
\left[\begin{array}{cc}
\Omega_{11} &amp; \Omega_{12}\\
\Omega_{21} &amp; \Omega_{22}
\end{array}\right]
\right),
\]</span>
then
<span class="math display">\[
Y_{2}|Y_{1} \sim \mathcal{N}
\left(
\Omega_{21}\Omega_{11}^{-1}Y_{1},\Omega_{22}-\Omega_{21}\Omega_{11}^{-1}\Omega_{12}
\right).
\]</span>
<span class="math display">\[
Y_{1}|Y_{2} \sim \mathcal{N}
\left(
\Omega_{12}\Omega_{22}^{-1}Y_{2},\Omega_{11}-\Omega_{12}\Omega_{22}^{-1}\Omega_{21}
\right).
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:truncated" class="proposition"><strong>Proposition 9.17  (Truncated distributions) </strong></span>If <span class="math inline">\(X\)</span> is a random variable distributed according to some p.d.f. <span class="math inline">\(f\)</span>, with c.d.f. <span class="math inline">\(F\)</span>, with infinite support. Then the p.d.f. of <span class="math inline">\(X|a \le X &lt; b\)</span> is
<span class="math display">\[
g(x) = \frac{f(x)}{F(b)-F(a)}\mathbb{I}_{\{a \le x &lt; b\}},
\]</span>
for any <span class="math inline">\(a&lt;b\)</span>.</p>
<p>In partiucular, for a Gaussian variable <span class="math inline">\(X \sim \mathcal{N}(\mu,\sigma^2)\)</span>, we have
<span class="math display">\[
f(X=x|a\le X&lt;b) = \dfrac{\dfrac{1}{\sigma}\phi\left(\dfrac{x - \mu}{\sigma}\right)}{Z}.
\]</span>
with <span class="math inline">\(Z = \Phi(\beta)-\Phi(\alpha)\)</span>, where <span class="math inline">\(\alpha = \dfrac{a - \mu}{\sigma}\)</span> and <span class="math inline">\(\beta = \dfrac{b - \mu}{\sigma}\)</span>.</p>
<p>Moreover:
<span class="math display" id="eq:Etrunc">\[\begin{eqnarray}
\mathbb{E}(X|a\le X&lt;b) &amp;=&amp; \mu - \frac{\phi\left(\beta\right)-\phi\left(\alpha\right)}{Z}\sigma. \tag{9.5}
\end{eqnarray}\]</span></p>
<p>We also have:
<span class="math display" id="eq:Vtrunc">\[\begin{eqnarray}
&amp;&amp; \mathbb{V}ar(X|a\le X&lt;b) \nonumber\\
&amp;=&amp; \sigma^2\left[
1 -  \frac{\beta\phi\left(\beta\right)-\alpha\phi\left(\alpha\right)}{Z} -  \left(\frac{\phi\left(\beta\right)-\phi\left(\alpha\right)}{Z}\right)^2 \right] \tag{9.6}
\end{eqnarray}\]</span></p>
<p>In particular, for <span class="math inline">\(b \rightarrow \infty\)</span>, we get:
<span class="math display" id="eq:Vtrunc2">\[\begin{equation}
\mathbb{V}ar(X|a &lt; X) = \sigma^2\left[1 + \alpha\lambda(-\alpha) - \lambda(-\alpha)^2 \right], \tag{9.7}
\end{equation}\]</span>
with <span class="math inline">\(\lambda(x)=\dfrac{\phi(x)}{\Phi(x)}\)</span> is called the <strong>inverse Mills ratio</strong>.</p>
</div>
<p>Consider the case where <span class="math inline">\(a \rightarrow - \infty\)</span> (i.e. the conditioning set is <span class="math inline">\(X&lt;b\)</span>) and <span class="math inline">\(\mu=0\)</span>, <span class="math inline">\(\sigma=1\)</span>. Then Eq. <a href="append.html#eq:Etrunc">(9.5)</a> gives <span class="math inline">\(\mathbb{E}(X|X&lt;b) = - \lambda(b) = - \dfrac{\phi(b)}{\Phi(b)}\)</span>, where <span class="math inline">\(\lambda\)</span> is the function computing the inverse Mills ratio.</p>
<div class="figure" style="text-align: left-aligned"><span style="display:block;" id="fig:inverseMills"></span>
<img src="EcoStat_files/figure-html/inverseMills-1.png" alt="$\mathbb{E}(X|X&lt;b)$ as a function of $b$ when $X\sim \mathcal{N}(0,1)$ (in black)." width="672" />
<p class="caption">
Figure 9.3: <span class="math inline">\(\mathbb{E}(X|X&lt;b)\)</span> as a function of <span class="math inline">\(b\)</span> when <span class="math inline">\(X\sim \mathcal{N}(0,1)\)</span> (in black).
</p>
</div>
<div class="proposition">
<p><span id="prp:pdfMultivarGaussian" class="proposition"><strong>Proposition 9.18  (p.d.f. of a multivariate Gaussian variable) </strong></span>If <span class="math inline">\(Y \sim \mathcal{N}(\mu,\Omega)\)</span> and if <span class="math inline">\(Y\)</span> is a <span class="math inline">\(n\)</span>-dimensional vector, then the density function of <span class="math inline">\(Y\)</span> is:
<span class="math display">\[
\frac{1}{(2 \pi)^{n/2}|\Omega|^{1/2}}\exp\left[-\frac{1}{2}\left(Y-\mu\right)&#39;\Omega^{-1}\left(Y-\mu\right)\right].
\]</span></p>
</div>
</div>
<div id="AppendixProof" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Proofs<a href="append.html#AppendixProof" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Proof of Proposition <a href="#prp:MLEproperties"><strong>??</strong></a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>Assumptions (i) and (ii) (in the set of Assumptions <a href="#hyp:MLEregularity"><strong>??</strong></a>) imply that <span class="math inline">\(\boldsymbol\theta_{MLE}\)</span> exists (<span class="math inline">\(=\mbox{argmax}_\theta (1/n)\log \mathcal{L}(\boldsymbol\theta;\bv{y})\)</span>).</p>
<p><span class="math inline">\((1/n)\log \mathcal{L}(\boldsymbol\theta;\bv{y})\)</span> can be interpreted as the sample mean of the r.v. <span class="math inline">\(\log f(Y_i;\boldsymbol\theta)\)</span> that are i.i.d. Therefore <span class="math inline">\((1/n)\log \mathcal{L}(\boldsymbol\theta;\bv{y})\)</span> converges to <span class="math inline">\(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\)</span> – which exists (Assumption iv).</p>
<p>Because the latter convergence is uniform (Assumption v), the solution <span class="math inline">\(\boldsymbol\theta_{MLE}\)</span> almost surely converges to the solution to the limit problem:
<span class="math display">\[
\mbox{argmax}_\theta \mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta)) = \mbox{argmax}_\theta \int_{\mathcal{Y}} \log f(y;\boldsymbol\theta)f(y;\boldsymbol\theta_0) dy.
\]</span></p>
<p>Properties of the Kullback information measure (see Prop. <a href="append.html#prp:Kullback">9.9</a>), together with the identifiability assumption (ii) implies that the solution to the limit problem is unique and equal to <span class="math inline">\(\boldsymbol\theta_0\)</span>.</p>
<p>Consider a r.v. sequence <span class="math inline">\(\boldsymbol\theta\)</span> that converges to <span class="math inline">\(\boldsymbol\theta_0\)</span>. The Taylor expansion of the score in a neighborood of <span class="math inline">\(\boldsymbol\theta_0\)</span> yields to:
<span class="math display">\[
\frac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta} = \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} + \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;}(\boldsymbol\theta - \boldsymbol\theta_0) + o_p(\boldsymbol\theta - \boldsymbol\theta_0)
\]</span></p>
<p><span class="math inline">\(\boldsymbol\theta_{MLE}\)</span> converges to <span class="math inline">\(\boldsymbol\theta_0\)</span> and satisfies the likelihood equation <span class="math inline">\(\frac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta} = \bv{0}\)</span>. Therefore:
<span class="math display">\[
\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} \approx - \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_0),
\]</span>
or equivalently:
<span class="math display">\[
\frac{1}{\sqrt{n}} \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} \approx
\left(- \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;} \right)\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_0),
\]</span></p>
<p>By the law of large numbers, we have: <span class="math inline">\(\left(- \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;} \right) \overset{}\rightarrow \frac{1}{n} \bv{I}(\boldsymbol\theta_0) = \mathcal{I}_Y(\boldsymbol\theta_0)\)</span>.</p>
<p>Besides, we have:
<span class="math display">\[\begin{eqnarray*}
\frac{1}{\sqrt{n}} \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} &amp;=&amp; \sqrt{n} \left( \frac{1}{n} \sum_i \frac{\partial \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} \right) \\
&amp;=&amp; \sqrt{n} \left( \frac{1}{n} \sum_i \left\{ \frac{\partial \log f(y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} - \mathbb{E}_{\boldsymbol\theta_0} \frac{\partial \log f(Y_i;\boldsymbol\theta_0)}{\partial \boldsymbol\theta} \right\} \right)
\end{eqnarray*}\]</span>
which converges to <span class="math inline">\(\mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0))\)</span> by the CLT.</p>
<p>Collecting the preceding results leads to (b). The fact that <span class="math inline">\(\boldsymbol\theta_{MLE}\)</span> achieves the FDCR bound proves (c).</p>
</div>
<p><strong>Proof of Proposition <a href="#prp:Walddistri"><strong>??</strong></a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>We have <span class="math inline">\(\sqrt{n}(\hat{\boldsymbol\theta}_{n} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}(\boldsymbol\theta_0)^{-1})\)</span> (Eq. <a href="#eq:normMLE">(<strong>??</strong>)</a>). A Taylor expansion around <span class="math inline">\(\boldsymbol\theta_0\)</span> yields to:
<span class="math display" id="eq:XXX">\[\begin{equation}
\sqrt{n}(h(\hat{\boldsymbol\theta}_{n}) - h(\boldsymbol\theta_{0})) \overset{d}{\rightarrow} \mathcal{N}\left(0,\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta&#39;}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})&#39;}{\partial \boldsymbol\theta}\right). \tag{9.8}
\end{equation}\]</span>
Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(h(\boldsymbol\theta_{0})=0\)</span> therefore:
<span class="math display" id="eq:lm10">\[\begin{equation}
\sqrt{n} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \mathcal{N}\left(0,\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta&#39;}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})&#39;}{\partial \boldsymbol\theta}\right). \tag{9.9}
\end{equation}\]</span>
Hence
<span class="math display">\[
\sqrt{n} \left(
\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta&#39;}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})&#39;}{\partial \boldsymbol\theta}
\right)^{-1/2} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \mathcal{N}\left(0,Id\right).
\]</span>
Taking the quadratic form, we obtain:
<span class="math display">\[
n h(\hat{\boldsymbol\theta}_{n})&#39;  \left(
\frac{\partial h(\boldsymbol\theta_{0})}{\partial \boldsymbol\theta&#39;}\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{\partial h(\boldsymbol\theta_{0})&#39;}{\partial \boldsymbol\theta}
\right)^{-1} h(\hat{\boldsymbol\theta}_{n}) \overset{d}{\rightarrow} \chi^2(r).
\]</span></p>
<p>The fact that the test has asymptotic level <span class="math inline">\(\alpha\)</span> directly stems from what precedes. <strong>Consistency of the test</strong>: Consider <span class="math inline">\(\theta_0 \in \Theta\)</span>. Because the MLE is consistent, <span class="math inline">\(h(\hat{\boldsymbol\theta}_{n})\)</span> converges to <span class="math inline">\(h(\boldsymbol\theta_0) \ne 0\)</span>. Eq. <a href="append.html#eq:XXX">(9.8)</a> is still valid. It implies that <span class="math inline">\(\xi^W_n\)</span> converges to <span class="math inline">\(+\infty\)</span> and therefore that <span class="math inline">\(\mathbb{P}_{\boldsymbol\theta}(\xi^W_n \ge \chi^2_{1-\alpha}(r)) \rightarrow 1\)</span>.</p>
</div>
<p><strong>Proof of Proposition <a href="#prp:LMdistri"><strong>??</strong></a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>Notations: “<span class="math inline">\(\approx\)</span>” means “equal up to a term that converges to 0 in probability”. We are under <span class="math inline">\(H_0\)</span>. <span class="math inline">\(\hat{\boldsymbol\theta}^0\)</span> is the constrained ML estimator; <span class="math inline">\(\hat{\boldsymbol\theta}\)</span> denotes the unconstrained one.</p>
<p>We combine the two Taylor expansion: <span class="math inline">\(h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta&#39;}(\hat{\boldsymbol\theta}_n - \boldsymbol\theta_0)\)</span> and <span class="math inline">\(h(\hat{\boldsymbol\theta}_n^0) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta&#39;}(\hat{\boldsymbol\theta}_n^0 - \boldsymbol\theta_0)\)</span> and we use <span class="math inline">\(h(\hat{\boldsymbol\theta}_n^0)=0\)</span> (by definition) to get:
<span class="math display" id="eq:lm1">\[\begin{equation}
\sqrt{n}h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta&#39;}\sqrt{n}(\hat{\boldsymbol\theta}_n - \hat{\boldsymbol\theta}^0_n). \tag{9.10}
\end{equation}\]</span>
Besides, we have (using the definition of the information matrix):
<span class="math display" id="eq:lm29">\[\begin{equation}
\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \approx
\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} - \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) \tag{9.11}
\end{equation}\]</span>
and:
<span class="math display" id="eq:lm30">\[\begin{equation}
0=\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}_n;\bv{y})}{\partial \boldsymbol\theta} \approx
\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} - \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0).\tag{9.12}
\end{equation}\]</span>
Taking the difference and multiplying by <span class="math inline">\(\mathcal{I}(\boldsymbol\theta_0)^{-1}\)</span>:
<span class="math display" id="eq:lm2">\[\begin{equation}
\sqrt{n}(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}_n^0) \approx
\mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta}
\mathcal{I}(\boldsymbol\theta_0).\tag{9.13}
\end{equation}\]</span>
Eqs. <a href="append.html#eq:lm1">(9.10)</a> and <a href="append.html#eq:lm2">(9.13)</a> yield to:
<span class="math display" id="eq:lm3">\[\begin{equation}
\sqrt{n}h(\hat{\boldsymbol\theta}_n) \approx \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\boldsymbol\theta_0)^{-1}\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta}.\tag{9.14}
\end{equation}\]</span></p>
<p>Recall that <span class="math inline">\(\hat{\boldsymbol\theta}^0_n\)</span> is the MLE of <span class="math inline">\(\boldsymbol\theta_0\)</span> under the constraint <span class="math inline">\(h(\boldsymbol\theta)=0\)</span>. The vector of Lagrange multipliers <span class="math inline">\(\hat\lambda_n\)</span> associated to this program satisfies:
<span class="math display" id="eq:multiplier">\[\begin{equation}
\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta}+ \frac{\partial h&#39;(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta}\hat\lambda_n = 0.\tag{9.15}
\end{equation}\]</span>
Substituting the latter equation in Eq. <a href="append.html#eq:lm3">(9.14)</a> gives:
<span class="math display">\[\begin{eqnarray*}
\sqrt{n}h(\hat{\boldsymbol\theta}_n) &amp;\approx&amp;
- \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\boldsymbol\theta_0)^{-1}
\frac{\partial h&#39;(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \frac{\hat\lambda_n}{\sqrt{n}} \\
&amp;\approx&amp;
- \dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\boldsymbol\theta_0)^{-1}
\frac{\partial h&#39;(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} \frac{\hat\lambda_n}{\sqrt{n}},
\end{eqnarray*}\]</span>
which yields:
<span class="math display" id="eq:lm20">\[\begin{equation}
\frac{\hat\lambda_n}{\sqrt{n}} \approx - \left(
\dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\boldsymbol\theta_0)^{-1}
\frac{\partial h&#39;(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta}
\right)^{-1}
\sqrt{n}h(\hat{\boldsymbol\theta}_n).\tag{9.16}
\end{equation}\]</span>
It follows, from Eq. <a href="append.html#eq:lm10">(9.9)</a>, that:
<span class="math display">\[
\frac{\hat\lambda_n}{\sqrt{n}} \overset{d}{\rightarrow} \mathcal{N}\left(0,\left(
\dfrac{\partial h(\boldsymbol\theta_0)}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\boldsymbol\theta_0)^{-1}
\frac{\partial h&#39;(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta}
\right)^{-1}\right).
\]</span>
Taking the quadratic form of the last equation gives:
<span class="math display">\[
\frac{1}{n}\hat\lambda_n&#39; \dfrac{\partial h(\hat{\boldsymbol\theta}^0_n)}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\hat{\boldsymbol\theta}^0_n)^{-1}
\frac{\partial h&#39;(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \hat\lambda_n \overset{d}{\rightarrow} \chi^2(r).
\]</span>
Using Eq. <a href="append.html#eq:multiplier">(9.15)</a>, it appears that the left-hand side term of the last equation is <span class="math inline">\(\xi^{LM}\)</span> as defined in Eq. <a href="#eq:xiLM">(<strong>??</strong>)</a>. Consistency: see Remark 17.3 in <span class="citation">Gouriéroux and Monfort (<a href="#ref-gourieroux_monfort_1995" role="doc-biblioref">1995</a>)</span>.</p>
</div>
<p><strong>Proof of Proposition <a href="#prp:equivLRLMW"><strong>??</strong></a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span>Let us first demonstrate the asymptotic equivalence of <span class="math inline">\(\xi^{LM}\)</span> and <span class="math inline">\(\xi^{LR}\)</span>.</p>
<p>The second-order Taylor expansions of <span class="math inline">\(\log \mathcal{L}(\hat{\boldsymbol\theta}^0_n,\bv{y})\)</span> and <span class="math inline">\(\log \mathcal{L}(\hat{\boldsymbol\theta}_n,\bv{y})\)</span> are:
<span class="math display">\[\begin{eqnarray*}
\log \mathcal{L}(\hat{\boldsymbol\theta}_n,\bv{y}) &amp;\approx&amp; \log \mathcal{L}(\boldsymbol\theta_0,\bv{y})
+ \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\bv{y})}{\partial \boldsymbol\theta&#39;}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0) \\
&amp;&amp; - \frac{n}{2} (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)&#39; \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)\\
\log \mathcal{L}(\hat{\boldsymbol\theta}^0_n,\bv{y}) &amp;\approx&amp; \log \mathcal{L}(\boldsymbol\theta_0,\bv{y})
+ \frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\bv{y})}{\partial \boldsymbol\theta&#39;}(\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) \\
&amp;&amp; - \frac{n}{2} (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)&#39; \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0).
\end{eqnarray*}\]</span>
Taking the difference, we obtain:
<span class="math display">\[\begin{eqnarray*}
\xi_n^{LR} &amp;\approx&amp; 2\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0,\bv{y})}{\partial \boldsymbol\theta&#39;}
(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n) + n (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)&#39; \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)\\
&amp;&amp; - n (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)&#39; \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0).
\end{eqnarray*}\]</span>
Using <span class="math inline">\(\dfrac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\boldsymbol\theta_0;\bv{y})}{\partial \boldsymbol\theta} \approx \mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)\)</span> (Eq. <a href="append.html#eq:lm30">(9.12)</a>), we have:
<span class="math display">\[\begin{eqnarray*}
\xi_n^{LR} &amp;\approx&amp;
2n(\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)&#39;\mathcal{I}(\boldsymbol\theta_0)
(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n)
+ n (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)&#39; \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0) \\
&amp;&amp; - n (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)&#39; \mathcal{I}(\boldsymbol\theta_0) (\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0).
\end{eqnarray*}\]</span>
In the second of the three terms in the sum, we replace <span class="math inline">\((\hat{\boldsymbol\theta}^0_n-\boldsymbol\theta_0)\)</span> by <span class="math inline">\((\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n+\hat{\boldsymbol\theta}_n-\boldsymbol\theta_0)\)</span> and we develop the associated product. This leads to:
<span class="math display" id="eq:lr10">\[\begin{equation}
\xi_n^{LR} \approx n (\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n)&#39; \mathcal{I}(\boldsymbol\theta_0)^{-1} (\hat{\boldsymbol\theta}^0_n-\hat{\boldsymbol\theta}_n). \tag{9.17}
\end{equation}\]</span>
The difference between Eqs. <a href="append.html#eq:lm29">(9.11)</a> and <a href="append.html#eq:lm30">(9.12)</a> implies:
<span class="math display">\[
\frac{1}{\sqrt{n}}\frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \approx
\mathcal{I}(\boldsymbol\theta_0)\sqrt{n}(\hat{\boldsymbol\theta}_n-\hat{\boldsymbol\theta}^0_n),
\]</span>
which, associated to Eq. @(eq:lr10), gives:
<span class="math display">\[
\xi_n^{LR} \approx \frac{1}{n} \frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\boldsymbol\theta_0)^{-1} \frac{\partial \log \mathcal{L}(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \approx \xi_n^{LM}.
\]</span>
Hence <span class="math inline">\(\xi_n^{LR}\)</span> has the same asymptotic distribution as <span class="math inline">\(\xi_n^{LM}\)</span>.</p>
<p>Let’s show that the LR test is consistent. For this, note that:
<span class="math display">\[\begin{eqnarray*}
\frac{\log \mathcal{L}(\hat{\boldsymbol\theta},\bv{y}) - \log \mathcal{L}(\hat{\boldsymbol\theta}^0,\bv{y})}{n} &amp;=&amp; \frac{1}{n} \sum_{i=1}^n[\log f(y_i;\hat{\boldsymbol\theta}_n) - \log f(y_i;\hat{\boldsymbol\theta}_n^0)]\\
&amp;\rightarrow&amp; \mathbb{E}_0[\log f(Y;\boldsymbol\theta_0) - \log f(Y;\boldsymbol\theta_\infty)],
\end{eqnarray*}\]</span>
where <span class="math inline">\(\boldsymbol\theta_\infty\)</span>, the pseudo true value, is such that <span class="math inline">\(h(\boldsymbol\theta_\infty) \ne 0\)</span> (by definition of <span class="math inline">\(H_1\)</span>). From the Kullback inequality and the asymptotic identifiability of <span class="math inline">\(\boldsymbol\theta_0\)</span>, it follows that <span class="math inline">\(\mathbb{E}_0[\log f(Y;\boldsymbol\theta_0) - \log f(Y;\boldsymbol\theta_\infty)] &gt;0\)</span>. Therefore <span class="math inline">\(\xi_n^{LR} \rightarrow + \infty\)</span> under <span class="math inline">\(H_1\)</span>.</p>
<p>Let us now demonstrate the equivalence of <span class="math inline">\(\xi^{LM} and \xi^{W}\)</span>.</p>
<p>We have (using Eq. (eq:multiplier)):
<span class="math display">\[
\xi^{LM}_n = \frac{1}{n}\hat\lambda_n&#39; \dfrac{\partial h(\hat{\boldsymbol\theta}^0_n)}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\hat{\boldsymbol\theta}^0_n)^{-1}
\frac{\partial h&#39;(\hat{\boldsymbol\theta}^0_n;\bv{y})}{\partial \boldsymbol\theta} \hat\lambda_n.
\]</span>
Since, under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(\hat{\boldsymbol\theta}_n^0\approx\hat{\boldsymbol\theta}_n \approx {\boldsymbol\theta}_0\)</span>, Eq. <a href="append.html#eq:lm20">(9.16)</a> therefore implies that:
<span class="math display">\[
\xi^{LM} \approx n h(\hat{\boldsymbol\theta}_n)&#39; \left(
\dfrac{\partial h(\hat{\boldsymbol\theta}_n)}{\partial \boldsymbol\theta&#39;} \mathcal{I}(\hat{\boldsymbol\theta}_n)^{-1}
\frac{\partial h&#39;(\hat{\boldsymbol\theta}_n;\bv{y})}{\partial \boldsymbol\theta}
\right)^{-1}
h(\hat{\boldsymbol\theta}_n) = \xi^{W},
\]</span>
which gives the result.</p>
</div>
<p><strong>Proof of Eq. <a href="#eq:TCL2">(<strong>??</strong>)</a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span>We have:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right]\\
&amp;=&amp; T\mathbb{E}\left[\left(\frac{1}{T}\sum_{t=1}^T(y_t - \mu)\right)^2\right] = \frac{1}{T} \mathbb{E}\left[\sum_{t=1}^T(y_t - \mu)^2+2\sum_{s&lt;t\le T}(y_t - \mu)(y_s - \mu)\right]\\
&amp;=&amp; \gamma_0 +\frac{2}{T}\left(\sum_{t=2}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-1} - \mu)\right]\right) +\frac{2}{T}\left(\sum_{t=3}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-2} - \mu)\right]\right) + \dots \\
&amp;&amp;+ \frac{2}{T}\left(\sum_{t=T-1}^{T}\mathbb{E}\left[(y_t - \mu)(y_{t-(T-2)} - \mu)\right]\right) + \frac{2}{T}\mathbb{E}\left[(y_t - \mu)(y_{t-(T-1)} - \mu)\right]\\
&amp;=&amp;  \gamma_0 + 2 \frac{T-1}{T}\gamma_1 + \dots + 2 \frac{1}{T}\gamma_{T-1} .
\end{eqnarray*}\]</span>
Therefore:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp; T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j \\
&amp;=&amp; - 2\frac{1}{T}\gamma_1 - 2\frac{2}{T}\gamma_2 - \dots - 2\frac{T-1}{T}\gamma_{T-1} - 2\gamma_T - 2 \gamma_{T+1} + \dots
\end{eqnarray*}\]</span>
And then:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp; \left|T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right|\\
&amp;\le&amp; 2\frac{1}{T}|\gamma_1| + 2\frac{2}{T}|\gamma_2| + \dots + 2\frac{T-1}{T}|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots
\end{eqnarray*}\]</span></p>
<p>For any <span class="math inline">\(q \le T\)</span>, we have:
<span class="math display">\[\begin{eqnarray*}
\left|T\mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right| &amp;\le&amp; 2\frac{1}{T}|\gamma_1| + 2\frac{2}{T}|\gamma_2| + \dots + 2\frac{q-1}{T}|\gamma_{q-1}| +2\frac{q}{T}|\gamma_q| +\\
&amp;&amp;2\frac{q+1}{T}|\gamma_{q+1}| + \dots  + 2\frac{T-1}{T}|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots\\
&amp;\le&amp; \frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q-1)|\gamma_{q-1}| +q|\gamma_q|\right) +\\
&amp;&amp;2|\gamma_{q+1}| + \dots  + 2|\gamma_{T-1}| + 2|\gamma_T| + 2 |\gamma_{T+1}| + \dots
\end{eqnarray*}\]</span></p>
<p>Consider <span class="math inline">\(\varepsilon &gt; 0\)</span>. The fact that the autocovariances are absolutely summable implies that there exists <span class="math inline">\(q_0\)</span> such that (Cauchy criterion, Theorem <a href="append.html#thm:cauchycritstatic">9.2</a>):
<span class="math display">\[
2|\gamma_{q_0+1}|+2|\gamma_{q_0+2}|+2|\gamma_{q_0+3}|+\dots &lt; \varepsilon/2.
\]</span>
Then, if <span class="math inline">\(T &gt; q_0\)</span>, it comes that:
<span class="math display">\[
\left|T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right|\le \frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right) + \varepsilon/2.
\]</span>
If <span class="math inline">\(T \ge 2\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right)/(\varepsilon/2)\)</span> (<span class="math inline">\(= f(q_0)\)</span>, say) then
<span class="math display">\[
\frac{2}{T}\left(|\gamma_1| + 2|\gamma_2| + \dots + (q_0-1)|\gamma_{q_0-1}| +q_0|\gamma_{q_0}|\right) \le \varepsilon/2.
\]</span>
Then, if <span class="math inline">\(T&gt;f(q_0)\)</span> and <span class="math inline">\(T&gt;q_0\)</span>, i.e. if <span class="math inline">\(T&gt;\max(f(q_0),q_0)\)</span>, we have:
<span class="math display">\[
\left|T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] - \sum_{j=-\infty}^{+\infty} \gamma_j\right|\le \varepsilon.
\]</span></p>
</div>
<p><strong>Proof of Proposition <a href="#prp:smallestMSE"><strong>??</strong></a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>Proof</em>. </span>We have:
<span class="math display" id="eq:1">\[\begin{eqnarray}
\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &amp;=&amp; \mathbb{E}\left([\color{blue}{\{y_{t+1} - \mathbb{E}(y_{t+1}|x_t)\}} + \color{red}{\{\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\}}]^2\right)\nonumber\\
&amp;=&amp;  \mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}^2\right) + \mathbb{E}\left(\color{red}{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\right)\nonumber\\
&amp;&amp; + 2\mathbb{E}\left( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\right). \tag{9.18}
\end{eqnarray}\]</span>
Let us focus on the last term. We have:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\right)\\
&amp;=&amp; \mathbb{E}( \mathbb{E}( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}\color{red}{ \underbrace{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\mbox{function of $x_t$}}}|x_t))\\
&amp;=&amp; \mathbb{E}( \color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \mathbb{E}( \color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}|x_t))\\
&amp;=&amp; \mathbb{E}( \color{red}{ [\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \color{blue}{\underbrace{[\mathbb{E}(y_{t+1}|x_t) - \mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.
\end{eqnarray*}\]</span></p>
<p>Therefore, Eq. <a href="append.html#eq:1">(9.18)</a> becomes:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\
&amp;=&amp;  \underbrace{\mathbb{E}\left(\color{blue}{[y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]}^2\right)}_{\mbox{$\ge 0$ and does not depend on $y^*_{t+1}$}} + \underbrace{\mathbb{E}\left(\color{red}{[\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\right)}_{\mbox{$\ge 0$ and depends on $y^*_{t+1}$}}.
\end{eqnarray*}\]</span>
This implies that <span class="math inline">\(\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\)</span> is always larger than <span class="math inline">\(\color{blue}{\mathbb{E}([y_{t+1} - \mathbb{E}(y_{t+1}|x_t)]^2)}\)</span>, and is therefore minimized if the second term is equal to zero, that is if <span class="math inline">\(\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\)</span>.</p>
</div>
<p><strong>Proof of Proposition <a href="#prp:estimVARGaussian"><strong>??</strong></a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>Proof</em>. </span>Using Proposition <a href="append.html#prp:pdfMultivarGaussian">9.18</a>, we obtain that, conditionally on <span class="math inline">\(x_1\)</span>, the log-likelihood is given by
<span class="math display">\[\begin{eqnarray*}
\log\mathcal{L}(Y_{T};\theta) &amp; = &amp; -(Tn/2)\log(2\pi)+(T/2)\log\left|\Omega^{-1}\right|\\
&amp;  &amp; -\frac{1}{2}\sum_{t=1}^{T}\left[\left(y_{t}-\Pi&#39;x_{t}\right)&#39;\Omega^{-1}\left(y_{t}-\Pi&#39;x_{t}\right)\right].
\end{eqnarray*}\]</span>
Let’s rewrite the last term of the log-likelihood:
<span class="math display">\[\begin{eqnarray*}
\sum_{t=1}^{T}\left[\left(y_{t}-\Pi&#39;x_{t}\right)&#39;\Omega^{-1}\left(y_{t}-\Pi&#39;x_{t}\right)\right] &amp; =\\
\sum_{t=1}^{T}\left[\left(y_{t}-\hat{\Pi}&#39;x_{t}+\hat{\Pi}&#39;x_{t}-\Pi&#39;x_{t}\right)&#39;\Omega^{-1}\left(y_{t}-\hat{\Pi}&#39;x_{t}+\hat{\Pi}&#39;x_{t}-\Pi&#39;x_{t}\right)\right] &amp; =\\
\sum_{t=1}^{T}\left[\left(\hat{\varepsilon}_{t}+(\hat{\Pi}-\Pi)&#39;x_{t}\right)&#39;\Omega^{-1}\left(\hat{\varepsilon}_{t}+(\hat{\Pi}-\Pi)&#39;x_{t}\right)\right],
\end{eqnarray*}\]</span>
where the <span class="math inline">\(j^{th}\)</span> element of the <span class="math inline">\((n\times1)\)</span> vector <span class="math inline">\(\hat{\varepsilon}_{t}\)</span> is the sample residual, for observation <span class="math inline">\(t\)</span>, from an OLS regression of <span class="math inline">\(y_{j,t}\)</span> on <span class="math inline">\(x_{t}\)</span>. Expanding the previous equation, we get:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\sum_{t=1}^{T}\left[\left(y_{t}-\Pi&#39;x_{t}\right)&#39;\Omega^{-1}\left(y_{t}-\Pi&#39;x_{t}\right)\right]  = \sum_{t=1}^{T}\hat{\varepsilon}_{t}&#39;\Omega^{-1}\hat{\varepsilon}_{t}\\
&amp;&amp;+2\sum_{t=1}^{T}\hat{\varepsilon}_{t}&#39;\Omega^{-1}(\hat{\Pi}-\Pi)&#39;x_{t}+\sum_{t=1}^{T}x&#39;_{t}(\hat{\Pi}-\Pi)\Omega^{-1}(\hat{\Pi}-\Pi)&#39;x_{t}.
\end{eqnarray*}\]</span>
Let’s apply the trace operator on the second term (that is a scalar):
<span class="math display">\[\begin{eqnarray*}
\sum_{t=1}^{T}\hat{\varepsilon}_{t}&#39;\Omega^{-1}(\hat{\Pi}-\Pi)&#39;x_{t} &amp; = &amp; Tr\left(\sum_{t=1}^{T}\hat{\varepsilon}_{t}&#39;\Omega^{-1}(\hat{\Pi}-\Pi)&#39;x_{t}\right)\\
=  Tr\left(\sum_{t=1}^{T}\Omega^{-1}(\hat{\Pi}-\Pi)&#39;x_{t}\hat{\varepsilon}_{t}&#39;\right) &amp; = &amp; Tr\left(\Omega^{-1}(\hat{\Pi}-\Pi)&#39;\sum_{t=1}^{T}x_{t}\hat{\varepsilon}_{t}&#39;\right).
\end{eqnarray*}\]</span>
Given that, by construction (property of OLS estimates), the sample residuals are orthogonal to the explanatory variables, this term is zero. Introducing <span class="math inline">\(\tilde{x}_{t}=(\hat{\Pi}-\Pi)&#39;x_{t}\)</span>, we have
<span class="math display">\[\begin{eqnarray*}
\sum_{t=1}^{T}\left[\left(y_{t}-\Pi&#39;x_{t}\right)&#39;\Omega^{-1}\left(y_{t}-\Pi&#39;x_{t}\right)\right] =\sum_{t=1}^{T}\hat{\varepsilon}_{t}&#39;\Omega^{-1}\hat{\varepsilon}_{t}+\sum_{t=1}^{T}\tilde{x}&#39;_{t}\Omega^{-1}\tilde{x}_{t}.
\end{eqnarray*}\]</span>
Since <span class="math inline">\(\Omega\)</span> is a positive definite matrix, <span class="math inline">\(\Omega^{-1}\)</span> is as well. Consequently, the smallest value that the last term can take is obtained for <span class="math inline">\(\tilde{x}_{t}=0\)</span>, i.e. when <span class="math inline">\(\Pi=\hat{\Pi}.\)</span></p>
<p>The MLE of <span class="math inline">\(\Omega\)</span> is the matrix <span class="math inline">\(\hat{\Omega}\)</span> that maximizes <span class="math inline">\(\Omega\overset{\ell}{\rightarrow}L(Y_{T};\hat{\Pi},\Omega)\)</span>. We have:
<span class="math display">\[\begin{eqnarray*}
\log\mathcal{L}(Y_{T};\hat{\Pi},\Omega) &amp; = &amp; -(Tn/2)\log(2\pi)+(T/2)\log\left|\Omega^{-1}\right| -\frac{1}{2}\sum_{t=1}^{T}\left[\hat{\varepsilon}_{t}&#39;\Omega^{-1}\hat{\varepsilon}_{t}\right].
\end{eqnarray*}\]</span></p>
<p>Matrix <span class="math inline">\(\hat{\Omega}\)</span> is a symmetric positive definite. It is easily checked that the (unrestricted) matrix that maximizes the latter expression is symmetric positive definite matrix. Indeed:
<span class="math display">\[
\frac{\partial \log\mathcal{L}(Y_{T};\hat{\Pi},\Omega)}{\partial\Omega}=\frac{T}{2}\Omega&#39;-\frac{1}{2}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}&#39;_{t}\Rightarrow\hat{\Omega}&#39;=\frac{1}{T}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}&#39;_{t},
\]</span>
which leads to the result.</p>
</div>
<p><strong>Proof of Proposition <a href="#prp:OLSVAR"><strong>??</strong></a></strong></p>
<div class="proof">
<p><span id="unlabeled-div-21" class="proof"><em>Proof</em>. </span>Let us drop the <span class="math inline">\(i\)</span> subscript. Rearranging Eq. <a href="#eq:olsar1">(<strong>??</strong>)</a>, we have:
<span class="math display">\[
\sqrt{T}(\bv{b}-\boldsymbol{\beta}) =  (X&#39;X/T)^{-1}\sqrt{T}(X&#39;\boldsymbol\varepsilon/T).
\]</span>
Let us consider the autocovariances of <span class="math inline">\(\bv{v}_t = x_t \varepsilon_t\)</span>, denoted by <span class="math inline">\(\gamma^v_j\)</span>. Using the fact that <span class="math inline">\(x_t\)</span> is a linear combination of past <span class="math inline">\(\varepsilon_t\)</span>s and that <span class="math inline">\(\varepsilon_t\)</span> is a white noise, we get that <span class="math inline">\(\mathbb{E}(\varepsilon_t x_t)=0\)</span>. Therefore
<span class="math display">\[
\gamma^v_j = \mathbb{E}(\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}&#39;).
\]</span>
If <span class="math inline">\(j&gt;0\)</span>, we have <span class="math inline">\(\mathbb{E}(\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}&#39;)=\mathbb{E}(\mathbb{E}[\varepsilon_t\varepsilon_{t-j}x_tx_{t-j}&#39;|\varepsilon_{t-j},x_t,x_{t-j}])=\)</span> <span class="math inline">\(\mathbb{E}(\varepsilon_{t-j}x_tx_{t-j}&#39;\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},x_t,x_{t-j}])=0\)</span>. Note that we have <span class="math inline">\(\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},x_t,x_{t-j}]=0\)</span> because <span class="math inline">\(\{\varepsilon_t\}\)</span> is an i.i.d. white noise sequence. If <span class="math inline">\(j=0\)</span>, we have:
<span class="math display">\[
\gamma^v_0 = \mathbb{E}(\varepsilon_t^2x_tx_{t}&#39;)= \mathbb{E}(\varepsilon_t^2) \mathbb{E}(x_tx_{t}&#39;)=\sigma^2\bv{Q}.
\]</span>
The convergence in distribution of <span class="math inline">\(\sqrt{T}(X&#39;\boldsymbol\varepsilon/T)=\sqrt{T}\frac{1}{T}\sum_{t=1}^Tv_t\)</span> results from the Central Limit Theorem for covariance-stationary processes, using the <span class="math inline">\(\gamma_j^v\)</span> computed above.</p>
</div>
</div>
<div id="additional-codes" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Additional codes<a href="append.html#additional-codes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="App:GEV" class="section level3 hasAnchor" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> Simulating GEV distributions<a href="append.html#App:GEV" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following lines of code have been used to generate Figure <a href="#fig:GEV"><strong>??</strong></a>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="append.html#cb3-1" aria-hidden="true" tabindex="-1"></a>n.sim <span class="ot">&lt;-</span> <span class="dv">4000</span></span>
<span id="cb3-2"><a href="append.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>),</span>
<span id="cb3-3"><a href="append.html#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">plt=</span><span class="fu">c</span>(.<span class="dv">2</span>,.<span class="dv">95</span>,.<span class="dv">2</span>,.<span class="dv">85</span>))</span>
<span id="cb3-4"><a href="append.html#cb3-4" aria-hidden="true" tabindex="-1"></a>all.rhos <span class="ot">&lt;-</span> <span class="fu">c</span>(.<span class="dv">3</span>,.<span class="dv">6</span>,.<span class="dv">95</span>)</span>
<span id="cb3-5"><a href="append.html#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(all.rhos)){</span>
<span id="cb3-6"><a href="append.html#cb3-6" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>all.rhos[j]</span>
<span id="cb3-7"><a href="append.html#cb3-7" aria-hidden="true" tabindex="-1"></a>  v1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n.sim)</span>
<span id="cb3-8"><a href="append.html#cb3-8" aria-hidden="true" tabindex="-1"></a>  v2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n.sim)</span>
<span id="cb3-9"><a href="append.html#cb3-9" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="fu">rep</span>(.<span class="dv">000001</span>,n.sim)</span>
<span id="cb3-10"><a href="append.html#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># solve for f(w) = w*(1 - log(w)/theta) - v2 = 0</span></span>
<span id="cb3-11"><a href="append.html#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>){</span>
<span id="cb3-12"><a href="append.html#cb3-12" aria-hidden="true" tabindex="-1"></a>    f.i <span class="ot">&lt;-</span> w <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">log</span>(w)<span class="sc">/</span>theta) <span class="sc">-</span> v2</span>
<span id="cb3-13"><a href="append.html#cb3-13" aria-hidden="true" tabindex="-1"></a>    f.prime <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">log</span>(w)<span class="sc">/</span>theta <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span>theta</span>
<span id="cb3-14"><a href="append.html#cb3-14" aria-hidden="true" tabindex="-1"></a>    w <span class="ot">&lt;-</span> w <span class="sc">-</span> f.i<span class="sc">/</span>f.prime</span>
<span id="cb3-15"><a href="append.html#cb3-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb3-16"><a href="append.html#cb3-16" aria-hidden="true" tabindex="-1"></a>  u1 <span class="ot">&lt;-</span> <span class="fu">exp</span>(v1<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>theta) <span class="sc">*</span> <span class="fu">log</span>(w))</span>
<span id="cb3-17"><a href="append.html#cb3-17" aria-hidden="true" tabindex="-1"></a>  u2 <span class="ot">&lt;-</span> <span class="fu">exp</span>((<span class="dv">1</span><span class="sc">-</span>v1)<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>theta) <span class="sc">*</span> <span class="fu">log</span>(w))</span>
<span id="cb3-18"><a href="append.html#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="append.html#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Get eps1 and eps2 using the inverse of</span></span>
<span id="cb3-20"><a href="append.html#cb3-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the Gumbel distribution&#39;s cdf:</span></span>
<span id="cb3-21"><a href="append.html#cb3-21" aria-hidden="true" tabindex="-1"></a>  eps1 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">log</span>(<span class="sc">-</span><span class="fu">log</span>(u1))</span>
<span id="cb3-22"><a href="append.html#cb3-22" aria-hidden="true" tabindex="-1"></a>  eps2 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">log</span>(<span class="sc">-</span><span class="fu">log</span>(u2))</span>
<span id="cb3-23"><a href="append.html#cb3-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(<span class="fu">cor</span>(eps1,eps2),<span class="dv">1</span><span class="sc">-</span>all.rhos[j]<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb3-24"><a href="append.html#cb3-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(eps1,eps2,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">col=</span><span class="st">&quot;#FF000044&quot;</span>,</span>
<span id="cb3-25"><a href="append.html#cb3-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">main=</span><span class="fu">paste</span>(<span class="st">&quot;rho = &quot;</span>,<span class="fu">toString</span>(all.rhos[j]),<span class="at">sep=</span><span class="st">&quot;&quot;</span>),</span>
<span id="cb3-26"><a href="append.html#cb3-26" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab=</span><span class="fu">expression</span>(epsilon[<span class="dv">1</span>]),</span>
<span id="cb3-27"><a href="append.html#cb3-27" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab=</span><span class="fu">expression</span>(epsilon[<span class="dv">2</span>]),</span>
<span id="cb3-28"><a href="append.html#cb3-28" aria-hidden="true" tabindex="-1"></a>       <span class="at">cex.lab=</span><span class="dv">2</span>,<span class="at">cex.main=</span><span class="fl">1.5</span>)</span>
<span id="cb3-29"><a href="append.html#cb3-29" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="IRFDELTA" class="section level3 hasAnchor" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Computing the covariance matrix of IRF using the delta method<a href="append.html#IRFDELTA" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="append.html#cb4-1" aria-hidden="true" tabindex="-1"></a>irf.function <span class="ot">&lt;-</span> <span class="cf">function</span>(THETA){</span>
<span id="cb4-2"><a href="append.html#cb4-2" aria-hidden="true" tabindex="-1"></a>  c <span class="ot">&lt;-</span> THETA[<span class="dv">1</span>]</span>
<span id="cb4-3"><a href="append.html#cb4-3" aria-hidden="true" tabindex="-1"></a>  phi <span class="ot">&lt;-</span> THETA[<span class="dv">2</span><span class="sc">:</span>(p<span class="sc">+</span><span class="dv">1</span>)]</span>
<span id="cb4-4"><a href="append.html#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(q<span class="sc">&gt;</span><span class="dv">0</span>){</span>
<span id="cb4-5"><a href="append.html#cb4-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,THETA[(<span class="dv">1</span><span class="sc">+</span>p<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(<span class="dv">1</span><span class="sc">+</span>p<span class="sc">+</span>q)])</span>
<span id="cb4-6"><a href="append.html#cb4-6" aria-hidden="true" tabindex="-1"></a>  }<span class="cf">else</span>{</span>
<span id="cb4-7"><a href="append.html#cb4-7" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb4-8"><a href="append.html#cb4-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb4-9"><a href="append.html#cb4-9" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">&lt;-</span> THETA[<span class="dv">1</span><span class="sc">+</span>p<span class="sc">+</span>q<span class="sc">+</span><span class="dv">1</span>]</span>
<span id="cb4-10"><a href="append.html#cb4-10" aria-hidden="true" tabindex="-1"></a>  r <span class="ot">&lt;-</span> <span class="fu">dim</span>(Matrix.of.Exog)[<span class="dv">2</span>] <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb4-11"><a href="append.html#cb4-11" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> THETA[(<span class="dv">1</span><span class="sc">+</span>p<span class="sc">+</span>q<span class="sc">+</span><span class="dv">1</span><span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(<span class="dv">1</span><span class="sc">+</span>p<span class="sc">+</span>q<span class="sc">+</span><span class="dv">1</span><span class="sc">+</span>(r<span class="sc">+</span><span class="dv">1</span>))]</span>
<span id="cb4-12"><a href="append.html#cb4-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-13"><a href="append.html#cb4-13" aria-hidden="true" tabindex="-1"></a>  irf <span class="ot">&lt;-</span> <span class="fu">sim.arma</span>(<span class="dv">0</span>,phi,beta,<span class="at">sigma=</span><span class="fu">sd</span>(Ramey<span class="sc">$</span>ED3_TC,<span class="at">na.rm=</span><span class="cn">TRUE</span>),<span class="at">T=</span><span class="dv">60</span>,</span>
<span id="cb4-14"><a href="append.html#cb4-14" aria-hidden="true" tabindex="-1"></a>                  <span class="at">y.0=</span><span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(x<span class="sc">$</span>phi)),<span class="at">nb.sim=</span><span class="dv">1</span>,<span class="at">make.IRF=</span><span class="dv">1</span>,</span>
<span id="cb4-15"><a href="append.html#cb4-15" aria-hidden="true" tabindex="-1"></a>                  <span class="at">X=</span><span class="cn">NaN</span>,<span class="at">beta=</span><span class="cn">NaN</span>)</span>
<span id="cb4-16"><a href="append.html#cb4-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(irf)</span>
<span id="cb4-17"><a href="append.html#cb4-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-18"><a href="append.html#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="append.html#cb4-19" aria-hidden="true" tabindex="-1"></a>IRF<span class="fl">.0</span> <span class="ot">&lt;-</span> <span class="dv">100</span><span class="sc">*</span><span class="fu">irf.function</span>(x<span class="sc">$</span>THETA)</span>
<span id="cb4-20"><a href="append.html#cb4-20" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> .<span class="dv">00000001</span></span>
<span id="cb4-21"><a href="append.html#cb4-21" aria-hidden="true" tabindex="-1"></a>d.IRF <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb4-22"><a href="append.html#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x<span class="sc">$</span>THETA)){</span>
<span id="cb4-23"><a href="append.html#cb4-23" aria-hidden="true" tabindex="-1"></a>  THETA.i <span class="ot">&lt;-</span> x<span class="sc">$</span>THETA</span>
<span id="cb4-24"><a href="append.html#cb4-24" aria-hidden="true" tabindex="-1"></a>  THETA.i[i] <span class="ot">&lt;-</span> THETA.i[i] <span class="sc">+</span> eps</span>
<span id="cb4-25"><a href="append.html#cb4-25" aria-hidden="true" tabindex="-1"></a>  IRF.i <span class="ot">&lt;-</span> <span class="dv">100</span><span class="sc">*</span><span class="fu">irf.function</span>(THETA.i)</span>
<span id="cb4-26"><a href="append.html#cb4-26" aria-hidden="true" tabindex="-1"></a>  d.IRF <span class="ot">&lt;-</span> <span class="fu">cbind</span>(d.IRF,</span>
<span id="cb4-27"><a href="append.html#cb4-27" aria-hidden="true" tabindex="-1"></a>                 (IRF.i <span class="sc">-</span> IRF<span class="fl">.0</span>)<span class="sc">/</span>eps</span>
<span id="cb4-28"><a href="append.html#cb4-28" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb4-29"><a href="append.html#cb4-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-30"><a href="append.html#cb4-30" aria-hidden="true" tabindex="-1"></a>mat.var.cov.IRF <span class="ot">&lt;-</span> d.IRF <span class="sc">%*%</span> x<span class="sc">$</span>I <span class="sc">%*%</span> <span class="fu">t</span>(d.IRF)</span></code></pre></div>
</div>
</div>
<div id="statistical-tables" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Statistical Tables<a href="append.html#statistical-tables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<table style="width:100%;">
<caption><span id="tab:Normal">Table 9.1: </span>Quantiles of the <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution. If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are respectively the row and column number; then the corresponding cell gives <span class="math inline">\(\mathbb{P}(0&lt;X\le a+b)\)</span>, where <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span>.</caption>
<colgroup>
<col width="5%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">0</th>
<th align="right">0.01</th>
<th align="right">0.02</th>
<th align="right">0.03</th>
<th align="right">0.04</th>
<th align="right">0.05</th>
<th align="right">0.06</th>
<th align="right">0.07</th>
<th align="right">0.08</th>
<th align="right">0.09</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="right">0.5000</td>
<td align="right">0.6179</td>
<td align="right">0.7257</td>
<td align="right">0.8159</td>
<td align="right">0.8849</td>
<td align="right">0.9332</td>
<td align="right">0.9641</td>
<td align="right">0.9821</td>
<td align="right">0.9918</td>
<td align="right">0.9965</td>
</tr>
<tr class="even">
<td align="left">0.1</td>
<td align="right">0.5040</td>
<td align="right">0.6217</td>
<td align="right">0.7291</td>
<td align="right">0.8186</td>
<td align="right">0.8869</td>
<td align="right">0.9345</td>
<td align="right">0.9649</td>
<td align="right">0.9826</td>
<td align="right">0.9920</td>
<td align="right">0.9966</td>
</tr>
<tr class="odd">
<td align="left">0.2</td>
<td align="right">0.5080</td>
<td align="right">0.6255</td>
<td align="right">0.7324</td>
<td align="right">0.8212</td>
<td align="right">0.8888</td>
<td align="right">0.9357</td>
<td align="right">0.9656</td>
<td align="right">0.9830</td>
<td align="right">0.9922</td>
<td align="right">0.9967</td>
</tr>
<tr class="even">
<td align="left">0.3</td>
<td align="right">0.5120</td>
<td align="right">0.6293</td>
<td align="right">0.7357</td>
<td align="right">0.8238</td>
<td align="right">0.8907</td>
<td align="right">0.9370</td>
<td align="right">0.9664</td>
<td align="right">0.9834</td>
<td align="right">0.9925</td>
<td align="right">0.9968</td>
</tr>
<tr class="odd">
<td align="left">0.4</td>
<td align="right">0.5160</td>
<td align="right">0.6331</td>
<td align="right">0.7389</td>
<td align="right">0.8264</td>
<td align="right">0.8925</td>
<td align="right">0.9382</td>
<td align="right">0.9671</td>
<td align="right">0.9838</td>
<td align="right">0.9927</td>
<td align="right">0.9969</td>
</tr>
<tr class="even">
<td align="left">0.5</td>
<td align="right">0.5199</td>
<td align="right">0.6368</td>
<td align="right">0.7422</td>
<td align="right">0.8289</td>
<td align="right">0.8944</td>
<td align="right">0.9394</td>
<td align="right">0.9678</td>
<td align="right">0.9842</td>
<td align="right">0.9929</td>
<td align="right">0.9970</td>
</tr>
<tr class="odd">
<td align="left">0.6</td>
<td align="right">0.5239</td>
<td align="right">0.6406</td>
<td align="right">0.7454</td>
<td align="right">0.8315</td>
<td align="right">0.8962</td>
<td align="right">0.9406</td>
<td align="right">0.9686</td>
<td align="right">0.9846</td>
<td align="right">0.9931</td>
<td align="right">0.9971</td>
</tr>
<tr class="even">
<td align="left">0.7</td>
<td align="right">0.5279</td>
<td align="right">0.6443</td>
<td align="right">0.7486</td>
<td align="right">0.8340</td>
<td align="right">0.8980</td>
<td align="right">0.9418</td>
<td align="right">0.9693</td>
<td align="right">0.9850</td>
<td align="right">0.9932</td>
<td align="right">0.9972</td>
</tr>
<tr class="odd">
<td align="left">0.8</td>
<td align="right">0.5319</td>
<td align="right">0.6480</td>
<td align="right">0.7517</td>
<td align="right">0.8365</td>
<td align="right">0.8997</td>
<td align="right">0.9429</td>
<td align="right">0.9699</td>
<td align="right">0.9854</td>
<td align="right">0.9934</td>
<td align="right">0.9973</td>
</tr>
<tr class="even">
<td align="left">0.9</td>
<td align="right">0.5359</td>
<td align="right">0.6517</td>
<td align="right">0.7549</td>
<td align="right">0.8389</td>
<td align="right">0.9015</td>
<td align="right">0.9441</td>
<td align="right">0.9706</td>
<td align="right">0.9857</td>
<td align="right">0.9936</td>
<td align="right">0.9974</td>
</tr>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.5398</td>
<td align="right">0.6554</td>
<td align="right">0.7580</td>
<td align="right">0.8413</td>
<td align="right">0.9032</td>
<td align="right">0.9452</td>
<td align="right">0.9713</td>
<td align="right">0.9861</td>
<td align="right">0.9938</td>
<td align="right">0.9974</td>
</tr>
<tr class="even">
<td align="left">1.1</td>
<td align="right">0.5438</td>
<td align="right">0.6591</td>
<td align="right">0.7611</td>
<td align="right">0.8438</td>
<td align="right">0.9049</td>
<td align="right">0.9463</td>
<td align="right">0.9719</td>
<td align="right">0.9864</td>
<td align="right">0.9940</td>
<td align="right">0.9975</td>
</tr>
<tr class="odd">
<td align="left">1.2</td>
<td align="right">0.5478</td>
<td align="right">0.6628</td>
<td align="right">0.7642</td>
<td align="right">0.8461</td>
<td align="right">0.9066</td>
<td align="right">0.9474</td>
<td align="right">0.9726</td>
<td align="right">0.9868</td>
<td align="right">0.9941</td>
<td align="right">0.9976</td>
</tr>
<tr class="even">
<td align="left">1.3</td>
<td align="right">0.5517</td>
<td align="right">0.6664</td>
<td align="right">0.7673</td>
<td align="right">0.8485</td>
<td align="right">0.9082</td>
<td align="right">0.9484</td>
<td align="right">0.9732</td>
<td align="right">0.9871</td>
<td align="right">0.9943</td>
<td align="right">0.9977</td>
</tr>
<tr class="odd">
<td align="left">1.4</td>
<td align="right">0.5557</td>
<td align="right">0.6700</td>
<td align="right">0.7704</td>
<td align="right">0.8508</td>
<td align="right">0.9099</td>
<td align="right">0.9495</td>
<td align="right">0.9738</td>
<td align="right">0.9875</td>
<td align="right">0.9945</td>
<td align="right">0.9977</td>
</tr>
<tr class="even">
<td align="left">1.5</td>
<td align="right">0.5596</td>
<td align="right">0.6736</td>
<td align="right">0.7734</td>
<td align="right">0.8531</td>
<td align="right">0.9115</td>
<td align="right">0.9505</td>
<td align="right">0.9744</td>
<td align="right">0.9878</td>
<td align="right">0.9946</td>
<td align="right">0.9978</td>
</tr>
<tr class="odd">
<td align="left">1.6</td>
<td align="right">0.5636</td>
<td align="right">0.6772</td>
<td align="right">0.7764</td>
<td align="right">0.8554</td>
<td align="right">0.9131</td>
<td align="right">0.9515</td>
<td align="right">0.9750</td>
<td align="right">0.9881</td>
<td align="right">0.9948</td>
<td align="right">0.9979</td>
</tr>
<tr class="even">
<td align="left">1.7</td>
<td align="right">0.5675</td>
<td align="right">0.6808</td>
<td align="right">0.7794</td>
<td align="right">0.8577</td>
<td align="right">0.9147</td>
<td align="right">0.9525</td>
<td align="right">0.9756</td>
<td align="right">0.9884</td>
<td align="right">0.9949</td>
<td align="right">0.9979</td>
</tr>
<tr class="odd">
<td align="left">1.8</td>
<td align="right">0.5714</td>
<td align="right">0.6844</td>
<td align="right">0.7823</td>
<td align="right">0.8599</td>
<td align="right">0.9162</td>
<td align="right">0.9535</td>
<td align="right">0.9761</td>
<td align="right">0.9887</td>
<td align="right">0.9951</td>
<td align="right">0.9980</td>
</tr>
<tr class="even">
<td align="left">1.9</td>
<td align="right">0.5753</td>
<td align="right">0.6879</td>
<td align="right">0.7852</td>
<td align="right">0.8621</td>
<td align="right">0.9177</td>
<td align="right">0.9545</td>
<td align="right">0.9767</td>
<td align="right">0.9890</td>
<td align="right">0.9952</td>
<td align="right">0.9981</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="right">0.5793</td>
<td align="right">0.6915</td>
<td align="right">0.7881</td>
<td align="right">0.8643</td>
<td align="right">0.9192</td>
<td align="right">0.9554</td>
<td align="right">0.9772</td>
<td align="right">0.9893</td>
<td align="right">0.9953</td>
<td align="right">0.9981</td>
</tr>
<tr class="even">
<td align="left">2.1</td>
<td align="right">0.5832</td>
<td align="right">0.6950</td>
<td align="right">0.7910</td>
<td align="right">0.8665</td>
<td align="right">0.9207</td>
<td align="right">0.9564</td>
<td align="right">0.9778</td>
<td align="right">0.9896</td>
<td align="right">0.9955</td>
<td align="right">0.9982</td>
</tr>
<tr class="odd">
<td align="left">2.2</td>
<td align="right">0.5871</td>
<td align="right">0.6985</td>
<td align="right">0.7939</td>
<td align="right">0.8686</td>
<td align="right">0.9222</td>
<td align="right">0.9573</td>
<td align="right">0.9783</td>
<td align="right">0.9898</td>
<td align="right">0.9956</td>
<td align="right">0.9982</td>
</tr>
<tr class="even">
<td align="left">2.3</td>
<td align="right">0.5910</td>
<td align="right">0.7019</td>
<td align="right">0.7967</td>
<td align="right">0.8708</td>
<td align="right">0.9236</td>
<td align="right">0.9582</td>
<td align="right">0.9788</td>
<td align="right">0.9901</td>
<td align="right">0.9957</td>
<td align="right">0.9983</td>
</tr>
<tr class="odd">
<td align="left">2.4</td>
<td align="right">0.5948</td>
<td align="right">0.7054</td>
<td align="right">0.7995</td>
<td align="right">0.8729</td>
<td align="right">0.9251</td>
<td align="right">0.9591</td>
<td align="right">0.9793</td>
<td align="right">0.9904</td>
<td align="right">0.9959</td>
<td align="right">0.9984</td>
</tr>
<tr class="even">
<td align="left">2.5</td>
<td align="right">0.5987</td>
<td align="right">0.7088</td>
<td align="right">0.8023</td>
<td align="right">0.8749</td>
<td align="right">0.9265</td>
<td align="right">0.9599</td>
<td align="right">0.9798</td>
<td align="right">0.9906</td>
<td align="right">0.9960</td>
<td align="right">0.9984</td>
</tr>
<tr class="odd">
<td align="left">2.6</td>
<td align="right">0.6026</td>
<td align="right">0.7123</td>
<td align="right">0.8051</td>
<td align="right">0.8770</td>
<td align="right">0.9279</td>
<td align="right">0.9608</td>
<td align="right">0.9803</td>
<td align="right">0.9909</td>
<td align="right">0.9961</td>
<td align="right">0.9985</td>
</tr>
<tr class="even">
<td align="left">2.7</td>
<td align="right">0.6064</td>
<td align="right">0.7157</td>
<td align="right">0.8078</td>
<td align="right">0.8790</td>
<td align="right">0.9292</td>
<td align="right">0.9616</td>
<td align="right">0.9808</td>
<td align="right">0.9911</td>
<td align="right">0.9962</td>
<td align="right">0.9985</td>
</tr>
<tr class="odd">
<td align="left">2.8</td>
<td align="right">0.6103</td>
<td align="right">0.7190</td>
<td align="right">0.8106</td>
<td align="right">0.8810</td>
<td align="right">0.9306</td>
<td align="right">0.9625</td>
<td align="right">0.9812</td>
<td align="right">0.9913</td>
<td align="right">0.9963</td>
<td align="right">0.9986</td>
</tr>
<tr class="even">
<td align="left">2.9</td>
<td align="right">0.6141</td>
<td align="right">0.7224</td>
<td align="right">0.8133</td>
<td align="right">0.8830</td>
<td align="right">0.9319</td>
<td align="right">0.9633</td>
<td align="right">0.9817</td>
<td align="right">0.9916</td>
<td align="right">0.9964</td>
<td align="right">0.9986</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:Student">Table 9.2: </span>Quantiles of the Student-<span class="math inline">\(t\)</span> distribution. The rows correspond to different degrees of freedom (<span class="math inline">\(\nu\)</span>, say); the columns correspond to different probabilities (<span class="math inline">\(z\)</span>, say). The cell gives <span class="math inline">\(q\)</span> that is s.t. <span class="math inline">\(\mathbb{P}(-q&lt;X&lt;q)=z\)</span>, with <span class="math inline">\(X \sim t(\nu)\)</span>.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">0.05</th>
<th align="right">0.1</th>
<th align="right">0.75</th>
<th align="right">0.9</th>
<th align="right">0.95</th>
<th align="right">0.975</th>
<th align="right">0.99</th>
<th align="right">0.999</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.079</td>
<td align="right">0.158</td>
<td align="right">2.414</td>
<td align="right">6.314</td>
<td align="right">12.706</td>
<td align="right">25.452</td>
<td align="right">63.657</td>
<td align="right">636.619</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="right">0.071</td>
<td align="right">0.142</td>
<td align="right">1.604</td>
<td align="right">2.920</td>
<td align="right">4.303</td>
<td align="right">6.205</td>
<td align="right">9.925</td>
<td align="right">31.599</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="right">0.068</td>
<td align="right">0.137</td>
<td align="right">1.423</td>
<td align="right">2.353</td>
<td align="right">3.182</td>
<td align="right">4.177</td>
<td align="right">5.841</td>
<td align="right">12.924</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="right">0.067</td>
<td align="right">0.134</td>
<td align="right">1.344</td>
<td align="right">2.132</td>
<td align="right">2.776</td>
<td align="right">3.495</td>
<td align="right">4.604</td>
<td align="right">8.610</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="right">0.066</td>
<td align="right">0.132</td>
<td align="right">1.301</td>
<td align="right">2.015</td>
<td align="right">2.571</td>
<td align="right">3.163</td>
<td align="right">4.032</td>
<td align="right">6.869</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="right">0.065</td>
<td align="right">0.131</td>
<td align="right">1.273</td>
<td align="right">1.943</td>
<td align="right">2.447</td>
<td align="right">2.969</td>
<td align="right">3.707</td>
<td align="right">5.959</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="right">0.065</td>
<td align="right">0.130</td>
<td align="right">1.254</td>
<td align="right">1.895</td>
<td align="right">2.365</td>
<td align="right">2.841</td>
<td align="right">3.499</td>
<td align="right">5.408</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="right">0.065</td>
<td align="right">0.130</td>
<td align="right">1.240</td>
<td align="right">1.860</td>
<td align="right">2.306</td>
<td align="right">2.752</td>
<td align="right">3.355</td>
<td align="right">5.041</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="right">0.064</td>
<td align="right">0.129</td>
<td align="right">1.230</td>
<td align="right">1.833</td>
<td align="right">2.262</td>
<td align="right">2.685</td>
<td align="right">3.250</td>
<td align="right">4.781</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="right">0.064</td>
<td align="right">0.129</td>
<td align="right">1.221</td>
<td align="right">1.812</td>
<td align="right">2.228</td>
<td align="right">2.634</td>
<td align="right">3.169</td>
<td align="right">4.587</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">0.063</td>
<td align="right">0.127</td>
<td align="right">1.185</td>
<td align="right">1.725</td>
<td align="right">2.086</td>
<td align="right">2.423</td>
<td align="right">2.845</td>
<td align="right">3.850</td>
</tr>
<tr class="even">
<td align="left">30</td>
<td align="right">0.063</td>
<td align="right">0.127</td>
<td align="right">1.173</td>
<td align="right">1.697</td>
<td align="right">2.042</td>
<td align="right">2.360</td>
<td align="right">2.750</td>
<td align="right">3.646</td>
</tr>
<tr class="odd">
<td align="left">40</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.167</td>
<td align="right">1.684</td>
<td align="right">2.021</td>
<td align="right">2.329</td>
<td align="right">2.704</td>
<td align="right">3.551</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.164</td>
<td align="right">1.676</td>
<td align="right">2.009</td>
<td align="right">2.311</td>
<td align="right">2.678</td>
<td align="right">3.496</td>
</tr>
<tr class="odd">
<td align="left">60</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.162</td>
<td align="right">1.671</td>
<td align="right">2.000</td>
<td align="right">2.299</td>
<td align="right">2.660</td>
<td align="right">3.460</td>
</tr>
<tr class="even">
<td align="left">70</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.160</td>
<td align="right">1.667</td>
<td align="right">1.994</td>
<td align="right">2.291</td>
<td align="right">2.648</td>
<td align="right">3.435</td>
</tr>
<tr class="odd">
<td align="left">80</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.159</td>
<td align="right">1.664</td>
<td align="right">1.990</td>
<td align="right">2.284</td>
<td align="right">2.639</td>
<td align="right">3.416</td>
</tr>
<tr class="even">
<td align="left">90</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.158</td>
<td align="right">1.662</td>
<td align="right">1.987</td>
<td align="right">2.280</td>
<td align="right">2.632</td>
<td align="right">3.402</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.157</td>
<td align="right">1.660</td>
<td align="right">1.984</td>
<td align="right">2.276</td>
<td align="right">2.626</td>
<td align="right">3.390</td>
</tr>
<tr class="even">
<td align="left">200</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.154</td>
<td align="right">1.653</td>
<td align="right">1.972</td>
<td align="right">2.258</td>
<td align="right">2.601</td>
<td align="right">3.340</td>
</tr>
<tr class="odd">
<td align="left">500</td>
<td align="right">0.063</td>
<td align="right">0.126</td>
<td align="right">1.152</td>
<td align="right">1.648</td>
<td align="right">1.965</td>
<td align="right">2.248</td>
<td align="right">2.586</td>
<td align="right">3.310</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:Chi2">Table 9.3: </span>Quantiles of the <span class="math inline">\(\chi^2\)</span> distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities.</caption>
<colgroup>
<col width="5%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">0.05</th>
<th align="right">0.1</th>
<th align="right">0.75</th>
<th align="right">0.9</th>
<th align="right">0.95</th>
<th align="right">0.975</th>
<th align="right">0.99</th>
<th align="right">0.999</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.004</td>
<td align="right">0.016</td>
<td align="right">1.323</td>
<td align="right">2.706</td>
<td align="right">3.841</td>
<td align="right">5.024</td>
<td align="right">6.635</td>
<td align="right">10.828</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="right">0.103</td>
<td align="right">0.211</td>
<td align="right">2.773</td>
<td align="right">4.605</td>
<td align="right">5.991</td>
<td align="right">7.378</td>
<td align="right">9.210</td>
<td align="right">13.816</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="right">0.352</td>
<td align="right">0.584</td>
<td align="right">4.108</td>
<td align="right">6.251</td>
<td align="right">7.815</td>
<td align="right">9.348</td>
<td align="right">11.345</td>
<td align="right">16.266</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="right">0.711</td>
<td align="right">1.064</td>
<td align="right">5.385</td>
<td align="right">7.779</td>
<td align="right">9.488</td>
<td align="right">11.143</td>
<td align="right">13.277</td>
<td align="right">18.467</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="right">1.145</td>
<td align="right">1.610</td>
<td align="right">6.626</td>
<td align="right">9.236</td>
<td align="right">11.070</td>
<td align="right">12.833</td>
<td align="right">15.086</td>
<td align="right">20.515</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="right">1.635</td>
<td align="right">2.204</td>
<td align="right">7.841</td>
<td align="right">10.645</td>
<td align="right">12.592</td>
<td align="right">14.449</td>
<td align="right">16.812</td>
<td align="right">22.458</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="right">2.167</td>
<td align="right">2.833</td>
<td align="right">9.037</td>
<td align="right">12.017</td>
<td align="right">14.067</td>
<td align="right">16.013</td>
<td align="right">18.475</td>
<td align="right">24.322</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="right">2.733</td>
<td align="right">3.490</td>
<td align="right">10.219</td>
<td align="right">13.362</td>
<td align="right">15.507</td>
<td align="right">17.535</td>
<td align="right">20.090</td>
<td align="right">26.124</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="right">3.325</td>
<td align="right">4.168</td>
<td align="right">11.389</td>
<td align="right">14.684</td>
<td align="right">16.919</td>
<td align="right">19.023</td>
<td align="right">21.666</td>
<td align="right">27.877</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="right">3.940</td>
<td align="right">4.865</td>
<td align="right">12.549</td>
<td align="right">15.987</td>
<td align="right">18.307</td>
<td align="right">20.483</td>
<td align="right">23.209</td>
<td align="right">29.588</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">10.851</td>
<td align="right">12.443</td>
<td align="right">23.828</td>
<td align="right">28.412</td>
<td align="right">31.410</td>
<td align="right">34.170</td>
<td align="right">37.566</td>
<td align="right">45.315</td>
</tr>
<tr class="even">
<td align="left">30</td>
<td align="right">18.493</td>
<td align="right">20.599</td>
<td align="right">34.800</td>
<td align="right">40.256</td>
<td align="right">43.773</td>
<td align="right">46.979</td>
<td align="right">50.892</td>
<td align="right">59.703</td>
</tr>
<tr class="odd">
<td align="left">40</td>
<td align="right">26.509</td>
<td align="right">29.051</td>
<td align="right">45.616</td>
<td align="right">51.805</td>
<td align="right">55.758</td>
<td align="right">59.342</td>
<td align="right">63.691</td>
<td align="right">73.402</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">34.764</td>
<td align="right">37.689</td>
<td align="right">56.334</td>
<td align="right">63.167</td>
<td align="right">67.505</td>
<td align="right">71.420</td>
<td align="right">76.154</td>
<td align="right">86.661</td>
</tr>
<tr class="odd">
<td align="left">60</td>
<td align="right">43.188</td>
<td align="right">46.459</td>
<td align="right">66.981</td>
<td align="right">74.397</td>
<td align="right">79.082</td>
<td align="right">83.298</td>
<td align="right">88.379</td>
<td align="right">99.607</td>
</tr>
<tr class="even">
<td align="left">70</td>
<td align="right">51.739</td>
<td align="right">55.329</td>
<td align="right">77.577</td>
<td align="right">85.527</td>
<td align="right">90.531</td>
<td align="right">95.023</td>
<td align="right">100.425</td>
<td align="right">112.317</td>
</tr>
<tr class="odd">
<td align="left">80</td>
<td align="right">60.391</td>
<td align="right">64.278</td>
<td align="right">88.130</td>
<td align="right">96.578</td>
<td align="right">101.879</td>
<td align="right">106.629</td>
<td align="right">112.329</td>
<td align="right">124.839</td>
</tr>
<tr class="even">
<td align="left">90</td>
<td align="right">69.126</td>
<td align="right">73.291</td>
<td align="right">98.650</td>
<td align="right">107.565</td>
<td align="right">113.145</td>
<td align="right">118.136</td>
<td align="right">124.116</td>
<td align="right">137.208</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">77.929</td>
<td align="right">82.358</td>
<td align="right">109.141</td>
<td align="right">118.498</td>
<td align="right">124.342</td>
<td align="right">129.561</td>
<td align="right">135.807</td>
<td align="right">149.449</td>
</tr>
<tr class="even">
<td align="left">200</td>
<td align="right">168.279</td>
<td align="right">174.835</td>
<td align="right">213.102</td>
<td align="right">226.021</td>
<td align="right">233.994</td>
<td align="right">241.058</td>
<td align="right">249.445</td>
<td align="right">267.541</td>
</tr>
<tr class="odd">
<td align="left">500</td>
<td align="right">449.147</td>
<td align="right">459.926</td>
<td align="right">520.950</td>
<td align="right">540.930</td>
<td align="right">553.127</td>
<td align="right">563.852</td>
<td align="right">576.493</td>
<td align="right">603.446</td>
</tr>
</tbody>
</table>
<table style="width:100%;">
<caption><span id="tab:Fstat">Table 9.4: </span>Quantiles of the <span class="math inline">\(\mathcal{F}\)</span> distribution. The columns and rows correspond to different degrees of freedom (resp. <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>). The different panels correspond to different probabilities (<span class="math inline">\(\alpha\)</span>) The corresponding cell gives <span class="math inline">\(z\)</span> that is s.t. <span class="math inline">\(\mathbb{P}(X \le z)=\alpha\)</span>, with <span class="math inline">\(X \sim \mathcal{F}(n_1,n_2)\)</span>.</caption>
<colgroup>
<col width="15%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">alpha = 0.9</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="right">4.060</td>
<td align="right">3.780</td>
<td align="right">3.619</td>
<td align="right">3.520</td>
<td align="right">3.453</td>
<td align="right">3.405</td>
<td align="right">3.368</td>
<td align="right">3.339</td>
<td align="right">3.316</td>
<td align="right">3.297</td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="right">3.285</td>
<td align="right">2.924</td>
<td align="right">2.728</td>
<td align="right">2.605</td>
<td align="right">2.522</td>
<td align="right">2.461</td>
<td align="right">2.414</td>
<td align="right">2.377</td>
<td align="right">2.347</td>
<td align="right">2.323</td>
</tr>
<tr class="even">
<td align="left">15</td>
<td align="right">3.073</td>
<td align="right">2.695</td>
<td align="right">2.490</td>
<td align="right">2.361</td>
<td align="right">2.273</td>
<td align="right">2.208</td>
<td align="right">2.158</td>
<td align="right">2.119</td>
<td align="right">2.086</td>
<td align="right">2.059</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">2.975</td>
<td align="right">2.589</td>
<td align="right">2.380</td>
<td align="right">2.249</td>
<td align="right">2.158</td>
<td align="right">2.091</td>
<td align="right">2.040</td>
<td align="right">1.999</td>
<td align="right">1.965</td>
<td align="right">1.937</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">2.809</td>
<td align="right">2.412</td>
<td align="right">2.197</td>
<td align="right">2.061</td>
<td align="right">1.966</td>
<td align="right">1.895</td>
<td align="right">1.840</td>
<td align="right">1.796</td>
<td align="right">1.760</td>
<td align="right">1.729</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">2.756</td>
<td align="right">2.356</td>
<td align="right">2.139</td>
<td align="right">2.002</td>
<td align="right">1.906</td>
<td align="right">1.834</td>
<td align="right">1.778</td>
<td align="right">1.732</td>
<td align="right">1.695</td>
<td align="right">1.663</td>
</tr>
<tr class="even">
<td align="left">500</td>
<td align="right">2.716</td>
<td align="right">2.313</td>
<td align="right">2.095</td>
<td align="right">1.956</td>
<td align="right">1.859</td>
<td align="right">1.786</td>
<td align="right">1.729</td>
<td align="right">1.683</td>
<td align="right">1.644</td>
<td align="right">1.612</td>
</tr>
<tr class="odd">
<td align="left">alpha = 0.95</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="right">6.608</td>
<td align="right">5.786</td>
<td align="right">5.409</td>
<td align="right">5.192</td>
<td align="right">5.050</td>
<td align="right">4.950</td>
<td align="right">4.876</td>
<td align="right">4.818</td>
<td align="right">4.772</td>
<td align="right">4.735</td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="right">4.965</td>
<td align="right">4.103</td>
<td align="right">3.708</td>
<td align="right">3.478</td>
<td align="right">3.326</td>
<td align="right">3.217</td>
<td align="right">3.135</td>
<td align="right">3.072</td>
<td align="right">3.020</td>
<td align="right">2.978</td>
</tr>
<tr class="even">
<td align="left">15</td>
<td align="right">4.543</td>
<td align="right">3.682</td>
<td align="right">3.287</td>
<td align="right">3.056</td>
<td align="right">2.901</td>
<td align="right">2.790</td>
<td align="right">2.707</td>
<td align="right">2.641</td>
<td align="right">2.588</td>
<td align="right">2.544</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">4.351</td>
<td align="right">3.493</td>
<td align="right">3.098</td>
<td align="right">2.866</td>
<td align="right">2.711</td>
<td align="right">2.599</td>
<td align="right">2.514</td>
<td align="right">2.447</td>
<td align="right">2.393</td>
<td align="right">2.348</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">4.034</td>
<td align="right">3.183</td>
<td align="right">2.790</td>
<td align="right">2.557</td>
<td align="right">2.400</td>
<td align="right">2.286</td>
<td align="right">2.199</td>
<td align="right">2.130</td>
<td align="right">2.073</td>
<td align="right">2.026</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">3.936</td>
<td align="right">3.087</td>
<td align="right">2.696</td>
<td align="right">2.463</td>
<td align="right">2.305</td>
<td align="right">2.191</td>
<td align="right">2.103</td>
<td align="right">2.032</td>
<td align="right">1.975</td>
<td align="right">1.927</td>
</tr>
<tr class="even">
<td align="left">500</td>
<td align="right">3.860</td>
<td align="right">3.014</td>
<td align="right">2.623</td>
<td align="right">2.390</td>
<td align="right">2.232</td>
<td align="right">2.117</td>
<td align="right">2.028</td>
<td align="right">1.957</td>
<td align="right">1.899</td>
<td align="right">1.850</td>
</tr>
<tr class="odd">
<td align="left">alpha = 0.99</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="right">16.258</td>
<td align="right">13.274</td>
<td align="right">12.060</td>
<td align="right">11.392</td>
<td align="right">10.967</td>
<td align="right">10.672</td>
<td align="right">10.456</td>
<td align="right">10.289</td>
<td align="right">10.158</td>
<td align="right">10.051</td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="right">10.044</td>
<td align="right">7.559</td>
<td align="right">6.552</td>
<td align="right">5.994</td>
<td align="right">5.636</td>
<td align="right">5.386</td>
<td align="right">5.200</td>
<td align="right">5.057</td>
<td align="right">4.942</td>
<td align="right">4.849</td>
</tr>
<tr class="even">
<td align="left">15</td>
<td align="right">8.683</td>
<td align="right">6.359</td>
<td align="right">5.417</td>
<td align="right">4.893</td>
<td align="right">4.556</td>
<td align="right">4.318</td>
<td align="right">4.142</td>
<td align="right">4.004</td>
<td align="right">3.895</td>
<td align="right">3.805</td>
</tr>
<tr class="odd">
<td align="left">20</td>
<td align="right">8.096</td>
<td align="right">5.849</td>
<td align="right">4.938</td>
<td align="right">4.431</td>
<td align="right">4.103</td>
<td align="right">3.871</td>
<td align="right">3.699</td>
<td align="right">3.564</td>
<td align="right">3.457</td>
<td align="right">3.368</td>
</tr>
<tr class="even">
<td align="left">50</td>
<td align="right">7.171</td>
<td align="right">5.057</td>
<td align="right">4.199</td>
<td align="right">3.720</td>
<td align="right">3.408</td>
<td align="right">3.186</td>
<td align="right">3.020</td>
<td align="right">2.890</td>
<td align="right">2.785</td>
<td align="right">2.698</td>
</tr>
<tr class="odd">
<td align="left">100</td>
<td align="right">6.895</td>
<td align="right">4.824</td>
<td align="right">3.984</td>
<td align="right">3.513</td>
<td align="right">3.206</td>
<td align="right">2.988</td>
<td align="right">2.823</td>
<td align="right">2.694</td>
<td align="right">2.590</td>
<td align="right">2.503</td>
</tr>
<tr class="even">
<td align="left">500</td>
<td align="right">6.686</td>
<td align="right">4.648</td>
<td align="right">3.821</td>
<td align="right">3.357</td>
<td align="right">3.054</td>
<td align="right">2.838</td>
<td align="right">2.675</td>
<td align="right">2.547</td>
<td align="right">2.443</td>
<td align="right">2.356</td>
</tr>
</tbody>
</table>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gourieroux_monfort_1995" class="csl-entry">
Gouriéroux, Christian, and Alain Monfort. 1995. <em>Statistics and Econometric Models</em>. Vol. 1. Themes in Modern Econometrics. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511751967">https://doi.org/10.1017/CBO9780511751967</a>.
</div>
<div id="ref-Litterman_Scheinkman_1991" class="csl-entry">
Litterman, Robert, and Jose Scheinkman. 1991. <span>“<span>Common Factors Affecting Bond Returns</span>.”</span> <em>Journal of Fixed Income</em>, no. 1: 54–61. <a href="https://www.math.nyu.edu/~avellane/Litterman1991.pdf">https://www.math.nyu.edu/~avellane/Litterman1991.pdf</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="TS.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-Appendix.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["EcoStat.pdf", "EcoStat.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
