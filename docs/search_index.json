[["append.html", "Chapter 9 Appendix 9.1 Principal component analysis (PCA) 9.2 Linear algebra: definitions and results 9.3 Statistical analysis: definitions and results 9.4 Some properties of Gaussian variables 9.5 Proofs 9.6 Additional codes 9.7 Statistical Tables", " Chapter 9 Appendix 9.1 Principal component analysis (PCA) Principal component analysis (PCA) is a classical and easy-to-use statistical method to reduce the dimension of large datasets containing variables that are linearly driven by a relatively small number of factors. This approach is widely used in data analysis and image compression. Suppose that we have \\(T\\) observations of a \\(n\\)-dimensional random vector \\(x\\), denoted by \\(x_{1},x_{2},\\ldots,x_{T}\\). We suppose that each component of \\(x\\) is of mean zero. Let denote with \\(X\\) the matrix given by \\(\\left[\\begin{array}{cccc} x_{1} &amp; x_{2} &amp; \\ldots &amp; x_{T}\\end{array}\\right]&#39;\\). Denote the \\(j^{th}\\) column of \\(X\\) by \\(X_{j}\\). We want to find the linear combination of the \\(x_{i}\\)’s (\\(x.u\\)), with \\(\\left\\Vert u\\right\\Vert =1\\), with “maximum variance.” That is, we want to solve: \\[\\begin{equation} \\begin{array}{clll} \\underset{u}{\\arg\\max} &amp; u&#39;X&#39;Xu. \\\\ \\mbox{s.t. } &amp; \\left| u\\right| =1 \\end{array}\\tag{9.1} \\end{equation}\\] Since \\(X&#39;X\\) is a positive definite matrix, it admits the following decomposition: \\[\\begin{eqnarray*} X&#39;X &amp; = &amp; PDP&#39;\\\\ &amp; = &amp; P\\left[\\begin{array}{ccc} \\lambda_{1}\\\\ &amp; \\ddots\\\\ &amp; &amp; \\lambda_{n} \\end{array}\\right]P&#39;, \\end{eqnarray*}\\] where \\(P\\) is an orthogonal matrix whose columns are the eigenvectors of \\(X&#39;X\\). We can order the eigenvalues such that \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X&#39;X\\) is positive definite, all these eigenvalues are positive.) Since \\(P\\) is orthogonal, we have \\(u&#39;X&#39;Xu=u&#39;PDP&#39;u=y&#39;Dy\\) where \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, we have \\(y_{i}^{2}\\leq 1\\) for any \\(i\\leq n\\). As a consequence: \\[ y&#39;Dy=\\sum_{i=1}^{n}y_{i}^{2}\\lambda_{i}\\leq\\lambda_{1}\\sum_{i=1}^{n}y_{i}^{2}=\\lambda_{1}. \\] It is easily seen that the maximum is reached for \\(y=\\left[1,0,\\cdots,0\\right]&#39;\\). Therefore, the maximum of the optimization program (Eq. (9.1)) is obtained for \\(u=P\\left[1,0,\\cdots,0\\right]&#39;\\). That is, \\(u\\) is the eigenvector of \\(X&#39;X\\) that is associated with its larger eigenvalue (first column of \\(P\\)). Let us denote with \\(F\\) the vector that is given by the matrix product \\(XP\\) (note that its last column is equal to \\(Xu\\)). The columns of \\(F\\), denoted by \\(F_{j}\\), are called factors. We have: \\[ F&#39;F=P&#39;X&#39;XP=D. \\] Therefore, in particular, the \\(F_{j}\\)’s are orthogonal. Since \\(X=FP&#39;\\), the \\(X_{j}\\)’s are linear combinations of the factors. Let us then denote with \\(\\hat{X}_{i,j}\\) the part of \\(X_{i}\\) that is explained by factor \\(F_{j}\\), we have: \\[\\begin{eqnarray*} \\hat{X}_{i,j} &amp; = &amp; p_{ij}F_{j}\\\\ X_{i} &amp; = &amp; \\sum_{j}\\hat{X}_{i,j}=\\sum_{j}p_{ij}F_{j}. \\end{eqnarray*}\\] Consider the share of variance that is explained –through the \\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))– by the first factor \\(F_{1}\\): \\[\\begin{eqnarray*} \\frac{\\sum_{i}\\hat{X}_{i,1}\\hat{X}&#39;_{i,1}}{\\sum_{i}X_{i}X&#39;_{i}} &amp; = &amp; \\frac{\\sum_{i}p_{i1}F_{1}F&#39;_{1}p_{i1}}{tr(X&#39;X)} = \\frac{\\sum_{i}p_{i1}^{2}\\lambda_{1}}{tr(X&#39;X)} = \\frac{\\lambda_{1}}{\\sum_{i}\\lambda_{i}}. \\end{eqnarray*}\\] Intuitively, if the first eigenvalue is large, it means that the first factor embed a large share of the fluctutaions of the \\(n\\) \\(X_{i}\\)’s. Let us illustrate PCA on the term structure of yields. The term strucutre of yields (or yield curve) is know to be driven by only a small number of factors (e.g., Litterman and Scheinkman (1991)). One can typically employ PCA to recover such factors. The data used in the example below are taken from the Fred database (tickers: “DGS6MO”,“DGS1”, …). The second plot shows the factor loardings, that indicate that the first factor is a level factor (loadings = black line), the second factor is a slope factor (loadings = blue line), the third factor is a curvature factor (loadings = red line). To run a PCA, one simply has to apply function prcomp to a matrix of data: library(AEC) USyields &lt;- USyields[complete.cases(USyields),] yds &lt;- USyields[c(&quot;Y1&quot;,&quot;Y2&quot;,&quot;Y3&quot;,&quot;Y5&quot;,&quot;Y7&quot;,&quot;Y10&quot;,&quot;Y20&quot;,&quot;Y30&quot;)] PCA.yds &lt;- prcomp(yds,center=TRUE,scale. = TRUE) Let us know visualize some results. The first plot of Figure 9.1 shows the share of total variance explained by the different principal components (PCs). The second plot shows the facotr loadings. The two bottom plots show how yields (in black) are fitted by linear combinations of the first two PCs only. par(mfrow=c(2,2)) par(plt=c(.1,.95,.2,.8)) barplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2), main=&quot;Share of variance expl. by PC&#39;s&quot;) axis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x)) nb.PC &lt;- 2 plot(-PCA.yds$rotation[,1],type=&quot;l&quot;,lwd=2,ylim=c(-1,1), main=&quot;Factor loadings (1st 3 PCs)&quot;,xaxt=&quot;n&quot;,xlab=&quot;&quot;) axis(1, at=1:dim(yds)[2], labels=colnames(yds)) lines(PCA.yds$rotation[,2],type=&quot;l&quot;,lwd=2,col=&quot;blue&quot;) lines(PCA.yds$rotation[,3],type=&quot;l&quot;,lwd=2,col=&quot;red&quot;) Y1.hat &lt;- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[&quot;Y1&quot;,1:2] Y1.hat &lt;- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat plot(USyields$date,USyields$Y1,type=&quot;l&quot;,lwd=2, main=&quot;Fit of 1-year yields (2 PCs)&quot;, ylab=&quot;Obs (black) / Fitted by 2PCs (dashed blue)&quot;) lines(USyields$date,Y1.hat,col=&quot;blue&quot;,lty=2,lwd=2) Y10.hat &lt;- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[&quot;Y10&quot;,1:2] Y10.hat &lt;- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat plot(USyields$date,USyields$Y10,type=&quot;l&quot;,lwd=2, main=&quot;Fit of 10-year yields (2 PCs)&quot;, ylab=&quot;Obs (black) / Fitted by 2PCs (dashed blue)&quot;) lines(USyields$date,Y10.hat,col=&quot;blue&quot;,lty=2,lwd=2) Figure 9.1: Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities. 9.2 Linear algebra: definitions and results Definition 9.1 (Eigenvalues) The eigenvalues of of a matrix \\(M\\) are the numbers \\(\\lambda\\) for which: \\[ |M - \\lambda I| = 0, \\] where \\(| \\bullet |\\) is the determinant operator. Proposition 9.1 (Properties of the determinant) We have: \\(|MN|=|M|\\times|N|\\). \\(|M^{-1}|=|M|^{-1}\\). If \\(M\\) admits the diagonal representation \\(M=TDT^{-1}\\), where \\(D\\) is a diagonal matrix whose diagonal entries are \\(\\{\\lambda_i\\}_{i=1,\\dots,n}\\), then: \\[ |M - \\lambda I |=\\prod_{i=1}^n (\\lambda_i - \\lambda). \\] Definition 9.2 (Moore-Penrose inverse) If \\(M \\in \\mathbb{R}^{m \\times n}\\), then its Moore-Penrose pseudo inverse (exists and) is the unique matrix \\(M^* \\in \\mathbb{R}^{n \\times m}\\) that satisfies: \\(M M^* M = M\\) \\(M^* M M^* = M^*\\) \\((M M^*)&#39;=M M^*\\) .iv \\((M^* M)&#39;=M^* M\\). Proposition 9.2 (Properties of the Moore-Penrose inverse) If \\(M\\) is invertible then \\(M^* = M^{-1}\\). The pseudo-inverse of a zero matrix is its transpose. * The pseudo-inverse of the pseudo-inverse is the original matrix. Definition 9.3 (Idempotent matrix) Matrix \\(M\\) is idempotent if \\(M^2=M\\). If \\(M\\) is a symmetric idempotent matrix, then \\(M&#39;M=M\\). Proposition 9.3 (Roots of an idempotent matrix) The eigenvalues of an idempotent matrix are either 1 or 0. Proof. If \\(\\lambda\\) is an eigenvalue of an idempotent matrix \\(M\\) then \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either all element of \\(Mx\\) are zero, in which case \\(\\lambda=0\\) or at least one element of \\(Mx\\) is nonzero, in which case \\(\\lambda=1\\). Proposition 9.4 (Idempotent matrix and chi-square distribution) The rank of a symmetric idempotent matrix is equal to its trace. Proof. The result follows from Prop. 9.3, combined with the fact that the rank of a symmetric matrix is equal to the number of its nonzero eigenvalues. Proposition 9.5 (Constrained least squares) The solution of the following optimisation problem: \\[\\begin{eqnarray*} \\underset{\\boldsymbol\\beta}{\\min} &amp;&amp; || \\bv{y} - \\bv{X}\\boldsymbol\\beta ||^2 \\\\ &amp;&amp; \\mbox{subject to } \\bv{R}\\boldsymbol\\beta = \\bv{q} \\end{eqnarray*}\\] is given by: \\[ \\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\bv{X}&#39;\\bv{X})^{-1} \\bv{R}&#39;\\{\\bv{R}(\\bv{X}&#39;\\bv{X})^{-1}\\bv{R}&#39;\\}^{-1}(\\bv{R}\\boldsymbol\\beta_0 - \\bv{q}),} \\] where \\(\\boldsymbol\\beta_0=(\\bv{X}&#39;\\bv{X})^{-1}\\bv{X}&#39;\\bv{y}\\). Proof. See for instance Jackman, 2007. Proposition 9.6 (Inverse of a partitioned matrix) We have: \\[\\begin{eqnarray*} &amp;&amp;\\left[ \\begin{array}{cc} \\bv{A}_{11} &amp; \\bv{A}_{12} \\\\ \\bv{A}_{21} &amp; \\bv{A}_{22} \\end{array}\\right]^{-1} = \\\\ &amp;&amp;\\left[ \\begin{array}{cc} (\\bv{A}_{11} - \\bv{A}_{12}\\bv{A}_{22}^{-1}\\bv{A}_{21})^{-1} &amp; - \\bv{A}_{11}^{-1}\\bv{A}_{12}(\\bv{A}_{22} - \\bv{A}_{21}\\bv{A}_{11}^{-1}\\bv{A}_{12})^{-1} \\\\ -(\\bv{A}_{22} - \\bv{A}_{21}\\bv{A}_{11}^{-1}\\bv{A}_{12})^{-1}\\bv{A}_{21}\\bv{A}_{11}^{-1} &amp; (\\bv{A}_{22} - \\bv{A}_{21}\\bv{A}_{11}^{-1}\\bv{A}_{12})^{-1} \\end{array} \\right]. \\end{eqnarray*}\\] Definition 9.4 (Matrix derivatives) Consider a fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). Its first-order derivative is: \\[ \\frac{\\partial f}{\\partial \\bv{b}}(\\bv{b}) = \\left[\\begin{array}{c} \\frac{\\partial f}{\\partial b_1}(\\bv{b})\\\\ \\vdots\\\\ \\frac{\\partial f}{\\partial b_K}(\\bv{b}) \\end{array} \\right]. \\] We use the notation: \\[ \\frac{\\partial f}{\\partial \\bv{b}&#39;}(\\bv{b}) = \\left(\\frac{\\partial f}{\\partial \\bv{b}}(\\bv{b})\\right)&#39;. \\] Proposition 9.7 We have: If \\(f(\\bv{b}) = A&#39; \\bv{b}\\) where \\(A\\) is a \\(K \\times 1\\) vector then \\(\\frac{\\partial f}{\\partial \\bv{b}}(\\bv{b}) = A\\). If \\(f(\\bv{b}) = \\bv{b}&#39;A\\bv{b}\\) where \\(A\\) is a \\(K \\times K\\) matrix, then \\(\\frac{\\partial f}{\\partial \\bv{b}}(\\bv{b}) = 2A\\bv{b}\\). Proposition 9.8 (Square and absolute summability) We have: \\[ \\underbrace{\\sum_{i=0}^{\\infty}|\\theta_i| &lt; + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{i=0}^{\\infty} \\theta_i^2 &lt; + \\infty}_{\\mbox{Square summability}}. \\] Proof. See Appendix 3.A in Hamilton. Idea: Absolute summability implies that there exist \\(N\\) such that, for \\(j&gt;N\\), \\(|\\theta_j| &lt; 1\\) (deduced from Cauchy criterion, Theorem 9.2 and therefore \\(\\theta_j^2 &lt; |\\theta_j|\\). 9.3 Statistical analysis: definitions and results 9.3.1 Moments and statistics Definition 9.5 (Partial correlation) The partial correlation between \\(y\\) and \\(z\\), controlling for some variables \\(\\bv{X}\\) is the sample correlation between \\(y^*\\) and \\(z^*\\), where the latter two variables are the residuals in regressions of \\(y\\) on \\(\\bv{X}\\) and of \\(z\\) on \\(\\bv{X}\\), respectively. This correlation is denoted by \\(r_{yz}^\\bv{X}\\). By definition, we have: \\[\\begin{equation} r_{yz}^\\bv{X} = \\frac{\\bv{z^*}&#39;\\bv{y^*}}{\\sqrt{(\\bv{z^*}&#39;\\bv{z^*})(\\bv{y^*}&#39;\\bv{y^*})}}.\\tag{9.2} \\end{equation}\\] Definition 9.6 (Skewness and kurtosis) Let \\(Y\\) be a random variable whose fourth moment exists. The expectation of \\(Y\\) is denoted by \\(\\mu\\). The skewness of \\(Y\\) is given by: \\[ \\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}. \\] The kurtosis of \\(Y\\) is given by: \\[ \\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}. \\] Theorem 9.1 (Cauchy-Schwarz inequality) We have: \\[ |\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)} \\] and, if \\(X \\ne =\\) and \\(Y \\ne 0\\), the equality holds iff \\(X\\) and \\(Y\\) are the same up to an affine transformation. Proof. If \\(\\mathbb{V}ar(X)=0\\), this is trivial. If this is not the case, then let’s define \\(Z\\) as \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). It is easily seen that \\(\\mathbb{C}ov(X,Z)=0\\). Then, the variance of \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) is equal to the sum of the variance of \\(Z\\) and of the variance of \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), that is: \\[ \\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X). \\] The equality holds iff \\(\\mathbb{V}ar(Z)=0\\), i.e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\). Definition 9.7 (Asymptotic level) An asymptotic test with critical region \\(\\Omega_n\\) has an asymptotic level equal to \\(\\alpha\\) if: \\[ \\underset{\\theta \\in \\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\in \\Omega_n) = \\alpha, \\] where \\(S_n\\) is the test statistic and \\(\\Theta\\) is such that the null hypothesis \\(H_0\\) is equivalent to \\(\\theta \\in \\Theta\\). Definition 9.8 (Asymptotically consistent test) An asymptotic test with critical region \\(\\Omega_n\\) is consistent if: \\[ \\forall \\theta \\in \\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\in \\Omega_n) \\rightarrow 1, \\] where \\(S_n\\) is the test statistic and \\(\\Theta^c\\) is such that the null hypothesis \\(H_0\\) is equivalent to \\(\\theta \\notin \\Theta^c\\). Definition 9.9 (Kullback discrepancy) Given two p.d.f. \\(f\\) and \\(f^*\\), the Kullback discrepancy is defined by: \\[ I(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy. \\] Proposition 9.9 (Properties of the Kullback discrepancy) We have: \\(I(f,f^*) \\ge 0\\) \\(I(f,f^*) = 0\\) iff \\(f \\equiv f^*\\). Proof. \\(x \\rightarrow -\\log(x)\\) is a convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves (i)). Since \\(x \\rightarrow -\\log(x)\\) is strictly convex, equality in (i) holds if and only if \\(f(Y)/f^*(Y)\\) is constant (proves (ii)). Definition 9.10 (Characteristic function) For any real-valued random variable \\(X\\), the characteristic function is defined by: \\[ \\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)]. \\] 9.3.2 Standard distributions Definition 9.11 (F distribution) Consider \\(n=n_1+n_2\\) i.i.d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). If the r.v. \\(F\\) is defined by: \\[ F = \\frac{\\sum_{i=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1} \\] then \\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 9.4 for quantiles.) Definition 9.12 (Student-t distribution) \\(Z\\) follows a Student-t (or \\(t\\)) distribution with \\(\\nu\\) degrees of freedom (d.f.) if: \\[ Z = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{i=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim i.i.d. \\mathcal{N}(0,1). \\] We have \\(\\mathbb{E}(Z)=0\\), and \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) if \\(\\nu&gt;2\\). (See Table 9.2 for quantiles.) Definition 9.13 (Chi-square distribution) \\(Z\\) follows a \\(\\chi^2\\) distribution with \\(\\nu\\) d.f. if \\(Z = \\sum_{i=1}^{\\nu}X_i^2\\) where \\(X_i \\sim i.i.d. \\mathcal{N}(0,1)\\). We have \\(\\mathbb{E}(Z)=\\nu\\). (See Table 9.3 for quantiles.) Definition 9.14 (Cauchy distribution) The probability distribution function of the Cauchy distribution defined by a location parameter \\(\\mu\\) and a scale parameter \\(\\gamma\\) is: \\[ f(x) = \\frac{1}{\\pi \\gamma \\left(1 + \\left[\\frac{x-\\mu}{\\gamma}\\right]^2\\right)}. \\] The mean and variance of this distribution are undefined. Figure 9.2: Pdf of the Cauchy distribution (\\(\\mu=0\\), \\(\\gamma=1\\)). Proposition 9.10 (Inner product of a multivariate Gaussian variable) Let \\(X\\) be a \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). We have: \\[ X&#39; \\Sigma^{-1}X \\sim \\chi^2(n). \\] Proof. Because \\(\\Sigma\\) is a symmetrical definite positive matrix, it admits the spectral decomposition \\(PDP&#39;\\) where \\(P\\) is an orthogonal matrix (i.e. \\(PP&#39;=Id\\)) and D is a diagonal matrix with non-negative entries. Denoting by \\(\\sqrt{D^{-1}}\\) the diagonal matrix whose diagonal entries are the inverse of those of \\(D\\), it is easily checked that the covariance matrix of \\(Y:=\\sqrt{D^{-1}}P&#39;X\\) is \\(Id\\). Therefore \\(Y\\) is a vector of uncorrelated Gaussian variables. The properties of Gaussian variables imply that the components of \\(Y\\) are then also independent. Hence \\(Y&#39;Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\). It remains to note that \\(Y&#39;Y=X&#39;PD^{-1}P&#39;X=X&#39;\\mathbb{V}ar(X)^{-1}X\\) to conclude. Definition 9.15 (Generalized Extreme Value (GEV) distribution) The vector of disturbances \\(\\boldsymbol\\varepsilon=[\\varepsilon_{1,1},\\dots,\\varepsilon_{1,K_1},\\dots,\\varepsilon_{J,1},\\dots,\\varepsilon_{J,K_J}]&#39;\\) follows the Generalized Extreme Value (GEV) distribution if its c.d.f. is: \\[ F(\\boldsymbol\\varepsilon,\\boldsymbol\\rho) = \\exp(-G(e^{-\\varepsilon_{1,1}},\\dots,e^{-\\varepsilon_{J,K_J}};\\boldsymbol\\rho)) \\] with \\[\\begin{eqnarray*} G(\\bv{Y};\\boldsymbol\\rho) &amp;\\equiv&amp; G(Y_{1,1},\\dots,Y_{1,K_1},\\dots,Y_{J,1},\\dots,Y_{J,K_J};\\boldsymbol\\rho) \\\\ &amp;=&amp; \\sum_{j=1}^J\\left(\\sum_{k=1}^{K_j} Y_{jk}^{1/\\rho_j} \\right)^{\\rho_j} \\end{eqnarray*}\\] 9.3.3 Stochastic convergences Proposition 9.11 (Chebychev's inequality) If \\(\\mathbb{E}(|X|^r)\\) is finite for some \\(r&gt;0\\) then: \\[ \\forall \\varepsilon &gt; 0, \\quad \\mathbb{P}(|X - c|&gt;\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}. \\] In particular, for \\(r=2\\): \\[ \\forall \\varepsilon &gt; 0, \\quad \\mathbb{P}(|X - c|&gt;\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}. \\] Proof. Remark that \\(\\varepsilon^r \\mathbb{I}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) and take the expectation of both sides. Definition 9.16 (Convergence in probability) The random variable sequence \\(x_n\\) converges in probability to a constant \\(c\\) if \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|&gt;\\varepsilon) = 0\\). It is denoted as: \\(\\mbox{plim } x_n = c\\). Definition 9.17 (Convergence in the Lr norm) \\(x_n\\) converges in the \\(r\\)-th mean (or in the \\(L^r\\)-norm) towards \\(x\\), if \\(\\mathbb{E}(|x_n|^r)\\) and \\(\\mathbb{E}(|x|^r)\\) exist and if \\[ \\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0. \\] It is denoted as: \\(x_n \\overset{L^r}{\\rightarrow} c\\). For \\(r=2\\), this convergence is called mean square convergence. Definition 9.18 (Almost sure convergence) The random variable sequence \\(x_n\\) converges almost surely to \\(c\\) if \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\). It is denoted as: \\(x_n \\overset{a.s.}{\\rightarrow} c\\). Definition 9.19 (Convergence in distribution) \\(x_n\\) is said to converge in distribution (or in law) to \\(x\\) if \\[ \\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s) \\] for all \\(s\\) at which \\(F_X\\) –the cumulative distribution of \\(X\\)– is continuous. It is denoted as: \\(x_n \\overset{d}{\\rightarrow} x\\). Proposition 9.12 (Rules for limiting distributions (Slutsky)) We have: Slutsky’s theorem: If \\(x_n \\overset{d}{\\rightarrow} x\\) and \\(y_n \\overset{p}{\\rightarrow} c\\) then \\[\\begin{eqnarray*} x_n y_n &amp;\\overset{d}{\\rightarrow}&amp; x c \\\\ x_n + y_n &amp;\\overset{d}{\\rightarrow}&amp; x + c \\\\ x_n/y_n &amp;\\overset{d}{\\rightarrow}&amp; x / c \\quad (\\mbox{if }c \\ne 0) \\end{eqnarray*}\\] Continuous mapping theorem: If \\(x_n \\overset{d}{\\rightarrow} x\\) and \\(g\\) is a continuous function then \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\) Proposition 9.13 (Implications of stochastic convergences) We have: \\[\\begin{align*} &amp;\\boxed{\\overset{L^s}{\\rightarrow}}&amp; &amp;\\underset{1 \\le r \\le s}{\\Rightarrow}&amp; &amp;\\boxed{\\overset{L^r}{\\rightarrow}}&amp;\\\\ &amp;&amp; &amp;&amp; &amp;\\Downarrow&amp;\\\\ &amp;\\boxed{\\overset{a.s.}{\\rightarrow}}&amp; &amp;\\Rightarrow&amp; &amp;\\boxed{\\overset{p}{\\rightarrow}}&amp; \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}. \\end{align*}\\] Proof. (of the fact that \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume that \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting by \\(F\\) and \\(F_n\\) the c.d.f. of \\(X\\) and \\(X_n\\), respectively: \\[\\begin{eqnarray*} F_n(x) &amp;=&amp; \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X &gt; x+\\varepsilon)\\\\ &amp;\\le&amp; F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|&gt;\\varepsilon).\\tag{9.3} \\end{eqnarray*}\\] Besides, \\[\\begin{eqnarray*} F(x-\\varepsilon) &amp;=&amp; \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n &gt; x)\\\\ &amp;\\le&amp; F_n(x) + \\mathbb{P}(|X_n - X|&gt;\\varepsilon), \\end{eqnarray*}\\] which implies: \\[\\begin{equation} F(x-\\varepsilon) - \\mathbb{P}(|X_n - X|&gt;\\varepsilon) \\le F_n(x).\\tag{9.4} \\end{equation}\\] Eqs. (9.3) and (9.4) imply: \\[ F(x-\\varepsilon) - \\mathbb{P}(|X_n - X|&gt;\\varepsilon) \\le F_n(x) \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|&gt;\\varepsilon). \\] Taking limits as \\(n \\rightarrow \\infty\\) yields \\[ F(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x) \\le F(x+\\varepsilon). \\] The result is then obtained by taking limits as \\(\\varepsilon \\rightarrow 0\\) (if \\(F\\) is continuous at \\(x\\)). Proposition 9.14 (Convergence in distribution to a constant) If \\(X_n\\) converges in distribution to a constant \\(c\\), then \\(X_n\\) converges in probability to \\(c\\). Proof. If \\(\\varepsilon&gt;0\\), we have \\(\\mathbb{P}(X_n &lt; c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) i.e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) and \\(\\mathbb{P}(X_n &lt; c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n &lt; c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\), which gives the result. Example 9.1 (Convergence in probability but not $L^r$) Let \\(\\{x_n\\}_{n \\in \\mathbb{N}}\\) be a series of random variables defined by: \\[ x_n = n u_n, \\] where \\(u_n\\) are independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\). We have \\(x_n \\overset{p}{\\rightarrow} 0\\) but \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) because \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\). Theorem 9.2 (Cauchy criterion (non-stochastic case)) We have that \\(\\sum_{i=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, for any \\(\\eta &gt; 0\\), there exists an integer \\(N\\) such that, for all \\(M\\ge N\\), \\[ \\left|\\sum_{i=N+1}^{M} a_i\\right| &lt; \\eta. \\] Theorem 9.3 (Cauchy criterion (stochastic case)) We have that \\(\\sum_{i=0}^{T} \\theta_i \\varepsilon_{t-i}\\) converges in mean square (\\(T \\rightarrow \\infty\\)) to a random variable iff, for any \\(\\eta &gt; 0\\), there exists an integer \\(N\\) such that, for all \\(M\\ge N\\), \\[ \\mathbb{E}\\left[\\left(\\sum_{i=N+1}^{M} \\theta_i \\varepsilon_{t-i}\\right)^2\\right] &lt; \\eta. \\] 9.3.4 Central limit theorem Theorem 9.4 (Law of large numbers) The sample mean is a consistent estimator of the population mean. Proof. Let’s denote by \\(\\phi_{X_i}\\) the characteristic function of a r.v. \\(X_i\\). If the mean of \\(X_i\\) is \\(\\mu\\) then the Talyor expansion of the characteristic function is: \\[ \\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u). \\] The properties of the characteristic function (see Def. 9.10) imply that: \\[ \\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{i=1}^{n} \\left(1 + i\\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}. \\] The facts that (a) \\(e^{iu\\mu}\\) is the characteristic function of the constant \\(\\mu\\) and (b) that a characteristic function uniquely characterises a distribution imply that the sample mean converges in distribution to the constant \\(\\mu\\), which further implies that it converges in probability to \\(\\mu\\). Theorem 9.5 (Lindberg-Levy Central limit theorem, CLT) If \\(x_n\\) is an i.i.d. sequence of random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\) (\\(\\in ]0,+\\infty[\\)), then: \\[ \\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{where} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i.} \\] Proof. Let us introduce the r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). We have \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(i \\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). We have: \\[\\begin{eqnarray*} &amp;&amp;\\left[ \\mathbb{E}\\left( \\exp\\left(i \\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n\\\\ &amp;=&amp; \\left[ \\mathbb{E}\\left( 1 + i \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\ &amp;=&amp; \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n. \\end{eqnarray*}\\] Therefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), which is the characteristic function of \\(\\mathcal{N}(0,\\sigma^2)\\). 9.4 Some properties of Gaussian variables Proposition 9.15 If \\(\\bv{A}\\) is idempotent and if \\(\\bv{x}\\) is Gaussian, \\(\\bv{L}\\bv{x}\\) and \\(\\bv{x}&#39;\\bv{A}\\bv{x}\\) are independent if \\(\\bv{L}\\bv{A}=\\bv{0}\\). Proof. If \\(\\bv{L}\\bv{A}=\\bv{0}\\), then the two Gaussian vectors \\(\\bv{L}\\bv{x}\\) and \\(\\bv{A}\\bv{x}\\) are independent. This implies the independence of any function of \\(\\bv{L}\\bv{x}\\) and any function of \\(\\bv{A}\\bv{x}\\). The results then follows from the observation that \\(\\bv{x}&#39;\\bv{A}\\bv{x}=(\\bv{A}\\bv{x})&#39;(\\bv{A}\\bv{x})\\), which is a function of \\(\\bv{A}\\bv{x}\\). Proposition 9.16 (Bayesian update in a vector of Gaussian variables) If \\[ \\left[ \\begin{array}{c} Y_1\\\\ Y_2 \\end{array} \\right] \\sim \\mathcal{N} \\left(0, \\left[\\begin{array}{cc} \\Omega_{11} &amp; \\Omega_{12}\\\\ \\Omega_{21} &amp; \\Omega_{22} \\end{array}\\right] \\right), \\] then \\[ Y_{2}|Y_{1} \\sim \\mathcal{N} \\left( \\Omega_{21}\\Omega_{11}^{-1}Y_{1},\\Omega_{22}-\\Omega_{21}\\Omega_{11}^{-1}\\Omega_{12} \\right). \\] \\[ Y_{1}|Y_{2} \\sim \\mathcal{N} \\left( \\Omega_{12}\\Omega_{22}^{-1}Y_{2},\\Omega_{11}-\\Omega_{12}\\Omega_{22}^{-1}\\Omega_{21} \\right). \\] Proposition 9.17 (Truncated distributions) If \\(X\\) is a random variable distributed according to some p.d.f. \\(f\\), with c.d.f. \\(F\\), with infinite support. Then the p.d.f. of \\(X|a \\le X &lt; b\\) is \\[ g(x) = \\frac{f(x)}{F(b)-F(a)}\\mathbb{I}_{\\{a \\le x &lt; b\\}}, \\] for any \\(a&lt;b\\). In partiucular, for a Gaussian variable \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), we have \\[ f(X=x|a\\le X&lt;b) = \\dfrac{\\dfrac{1}{\\sigma}\\phi\\left(\\dfrac{x - \\mu}{\\sigma}\\right)}{Z}. \\] with \\(Z = \\Phi(\\beta)-\\Phi(\\alpha)\\), where \\(\\alpha = \\dfrac{a - \\mu}{\\sigma}\\) and \\(\\beta = \\dfrac{b - \\mu}{\\sigma}\\). Moreover: \\[\\begin{eqnarray} \\mathbb{E}(X|a\\le X&lt;b) &amp;=&amp; \\mu - \\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\sigma. \\tag{9.5} \\end{eqnarray}\\] We also have: \\[\\begin{eqnarray} &amp;&amp; \\mathbb{V}ar(X|a\\le X&lt;b) \\nonumber\\\\ &amp;=&amp; \\sigma^2\\left[ 1 - \\frac{\\beta\\phi\\left(\\beta\\right)-\\alpha\\phi\\left(\\alpha\\right)}{Z} - \\left(\\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\right)^2 \\right] \\tag{9.6} \\end{eqnarray}\\] In particular, for \\(b \\rightarrow \\infty\\), we get: \\[\\begin{equation} \\mathbb{V}ar(X|a &lt; X) = \\sigma^2\\left[1 + \\alpha\\lambda(-\\alpha) - \\lambda(-\\alpha)^2 \\right], \\tag{9.7} \\end{equation}\\] with \\(\\lambda(x)=\\dfrac{\\phi(x)}{\\Phi(x)}\\) is called the inverse Mills ratio. Consider the case where \\(a \\rightarrow - \\infty\\) (i.e. the conditioning set is \\(X&lt;b\\)) and \\(\\mu=0\\), \\(\\sigma=1\\). Then Eq. (9.5) gives \\(\\mathbb{E}(X|X&lt;b) = - \\lambda(b) = - \\dfrac{\\phi(b)}{\\Phi(b)}\\), where \\(\\lambda\\) is the function computing the inverse Mills ratio. Figure 9.3: \\(\\mathbb{E}(X|X&lt;b)\\) as a function of \\(b\\) when \\(X\\sim \\mathcal{N}(0,1)\\) (in black). Proposition 9.18 (p.d.f. of a multivariate Gaussian variable) If \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) and if \\(Y\\) is a \\(n\\)-dimensional vector, then the density function of \\(Y\\) is: \\[ \\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)&#39;\\Omega^{-1}\\left(Y-\\mu\\right)\\right]. \\] 9.5 Proofs Proof of Proposition ?? Proof. Assumptions (i) and (ii) (in the set of Assumptions ??) imply that \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y})\\)). \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y})\\) can be interpreted as the sample mean of the r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) that are i.i.d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y})\\) converges to \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – which exists (Assumption iv). Because the latter convergence is uniform (Assumption v), the solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges to the solution to the limit problem: \\[ \\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy. \\] Properties of the Kullback information measure (see Prop. 9.9), together with the identifiability assumption (ii) implies that the solution to the limit problem is unique and equal to \\(\\boldsymbol\\theta_0\\). Consider a r.v. sequence \\(\\boldsymbol\\theta\\) that converges to \\(\\boldsymbol\\theta_0\\). The Taylor expansion of the score in a neighborood of \\(\\boldsymbol\\theta_0\\) yields to: \\[ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0) \\] \\(\\boldsymbol\\theta_{MLE}\\) converges to \\(\\boldsymbol\\theta_0\\) and satisfies the likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y})}{\\partial \\boldsymbol\\theta} = \\bv{0}\\). Therefore: \\[ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0), \\] or equivalently: \\[ \\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} \\approx \\left(- \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0), \\] By the law of large numbers, we have: \\(\\left(- \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\bv{I}(\\boldsymbol\\theta_0) = \\mathcal{I}_Y(\\boldsymbol\\theta_0)\\). Besides, we have: \\[\\begin{eqnarray*} \\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} &amp;=&amp; \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\ &amp;=&amp; \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right) \\end{eqnarray*}\\] which converges to \\(\\mathcal{N}(0,\\mathcal{I}_Y(\\boldsymbol\\theta_0))\\) by the CLT. Collecting the preceding results leads to (b). The fact that \\(\\boldsymbol\\theta_{MLE}\\) achieves the FDCR bound proves (c). Proof of Proposition ?? Proof. We have \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{I}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (??)). A Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields to: \\[\\begin{equation} \\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta&#39;}\\mathcal{I}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})&#39;}{\\partial \\boldsymbol\\theta}\\right). \\tag{9.8} \\end{equation}\\] Under \\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore: \\[\\begin{equation} \\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta&#39;}\\mathcal{I}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})&#39;}{\\partial \\boldsymbol\\theta}\\right). \\tag{9.9} \\end{equation}\\] Hence \\[ \\sqrt{n} \\left( \\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta&#39;}\\mathcal{I}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})&#39;}{\\partial \\boldsymbol\\theta} \\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right). \\] Taking the quadratic form, we obtain: \\[ n h(\\hat{\\boldsymbol\\theta}_{n})&#39; \\left( \\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta&#39;}\\mathcal{I}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})&#39;}{\\partial \\boldsymbol\\theta} \\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r). \\] The fact that the test has asymptotic level \\(\\alpha\\) directly stems from what precedes. Consistency of the test: Consider \\(\\theta_0 \\in \\Theta\\). Because the MLE is consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges to \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (9.8) is still valid. It implies that \\(\\xi^W_n\\) converges to \\(+\\infty\\) and therefore that \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\). Proof of Proposition ?? Proof. Notations: “\\(\\approx\\)” means “equal up to a term that converges to 0 in probability”. We are under \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) is the constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes the unconstrained one. We combine the two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta&#39;}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) and \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta&#39;}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) and we use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (by definition) to get: \\[\\begin{equation} \\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta&#39;}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{9.10} \\end{equation}\\] Besides, we have (using the definition of the information matrix): \\[\\begin{equation} \\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\approx \\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{I}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{9.11} \\end{equation}\\] and: \\[\\begin{equation} 0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\approx \\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{I}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{9.12} \\end{equation}\\] Taking the difference and multiplying by \\(\\mathcal{I}(\\boldsymbol\\theta_0)^{-1}\\): \\[\\begin{equation} \\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx \\mathcal{I}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\mathcal{I}(\\boldsymbol\\theta_0).\\tag{9.13} \\end{equation}\\] Eqs. (9.10) and (9.13) yield to: \\[\\begin{equation} \\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta}.\\tag{9.14} \\end{equation}\\] Recall that \\(\\hat{\\boldsymbol\\theta}^0_n\\) is the MLE of \\(\\boldsymbol\\theta_0\\) under the constraint \\(h(\\boldsymbol\\theta)=0\\). The vector of Lagrange multipliers \\(\\hat\\lambda_n\\) associated to this program satisfies: \\[\\begin{equation} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h&#39;(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{9.15} \\end{equation}\\] Substituting the latter equation in Eq. (9.14) gives: \\[\\begin{eqnarray*} \\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) &amp;\\approx&amp; - \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial h&#39;(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\\\ &amp;\\approx&amp; - \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial h&#39;(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}}, \\end{eqnarray*}\\] which yields: \\[\\begin{equation} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left( \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial h&#39;(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} \\right)^{-1} \\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{9.16} \\end{equation}\\] It follows, from Eq. (9.9), that: \\[ \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left( \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial h&#39;(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} \\right)^{-1}\\right). \\] Taking the quadratic form of the last equation gives: \\[ \\frac{1}{n}\\hat\\lambda_n&#39; \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\hat{\\boldsymbol\\theta}^0_n)^{-1} \\frac{\\partial h&#39;(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r). \\] Using Eq. (9.15), it appears that the left-hand side term of the last equation is \\(\\xi^{LM}\\) as defined in Eq. (??). Consistency: see Remark 17.3 in Gouriéroux and Monfort (1995). Proof of Proposition ?? Proof. Let us first demonstrate the asymptotic equivalence of \\(\\xi^{LM}\\) and \\(\\xi^{LR}\\). The second-order Taylor expansions of \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\bv{y})\\) and \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\bv{y})\\) are: \\[\\begin{eqnarray*} \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\bv{y}) &amp;\\approx&amp; \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\bv{y}) + \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\bv{y})}{\\partial \\boldsymbol\\theta&#39;}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0) \\\\ &amp;&amp; - \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)&#39; \\mathcal{I}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\ \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\bv{y}) &amp;\\approx&amp; \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\bv{y}) + \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\bv{y})}{\\partial \\boldsymbol\\theta&#39;}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\\\ &amp;&amp; - \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)&#39; \\mathcal{I}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0). \\end{eqnarray*}\\] Taking the difference, we obtain: \\[\\begin{eqnarray*} \\xi_n^{LR} &amp;\\approx&amp; 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\bv{y})}{\\partial \\boldsymbol\\theta&#39;} (\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)&#39; \\mathcal{I}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\\\ &amp;&amp; - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)&#39; \\mathcal{I}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0). \\end{eqnarray*}\\] Using \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\bv{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{I}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (9.12)), we have: \\[\\begin{eqnarray*} \\xi_n^{LR} &amp;\\approx&amp; 2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)&#39;\\mathcal{I}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)&#39; \\mathcal{I}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\\\ &amp;&amp; - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)&#39; \\mathcal{I}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0). \\end{eqnarray*}\\] In the second of the three terms in the sum, we replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) by \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) and we develop the associated product. This leads to: \\[\\begin{equation} \\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)&#39; \\mathcal{I}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{9.17} \\end{equation}\\] The difference between Eqs. (9.11) and (9.12) implies: \\[ \\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{I}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n), \\] which, associated to Eq. @(eq:lr10), gives: \\[ \\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}. \\] Hence \\(\\xi_n^{LR}\\) has the same asymptotic distribution as \\(\\xi_n^{LM}\\). Let’s show that the LR test is consistent. For this, note that: \\[\\begin{eqnarray*} \\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\bv{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\bv{y})}{n} &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)]\\\\ &amp;\\rightarrow&amp; \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)], \\end{eqnarray*}\\] where \\(\\boldsymbol\\theta_\\infty\\), the pseudo true value, is such that \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (by definition of \\(H_1\\)). From the Kullback inequality and the asymptotic identifiability of \\(\\boldsymbol\\theta_0\\), it follows that \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] &gt;0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) under \\(H_1\\). Let us now demonstrate the equivalence of \\(\\xi^{LM} and \\xi^{W}\\). We have (using Eq. (eq:multiplier)): \\[ \\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n&#39; \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\hat{\\boldsymbol\\theta}^0_n)^{-1} \\frac{\\partial h&#39;(\\hat{\\boldsymbol\\theta}^0_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n. \\] Since, under \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (9.16) therefore implies that: \\[ \\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)&#39; \\left( \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta&#39;} \\mathcal{I}(\\hat{\\boldsymbol\\theta}_n)^{-1} \\frac{\\partial h&#39;(\\hat{\\boldsymbol\\theta}_n;\\bv{y})}{\\partial \\boldsymbol\\theta} \\right)^{-1} h(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W}, \\] which gives the result. Proof of Eq. (??) Proof. We have: \\[\\begin{eqnarray*} &amp;&amp;T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\ &amp;=&amp; T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s&lt;t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\ &amp;=&amp; \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\ &amp;&amp;+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\ &amp;=&amp; \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} . \\end{eqnarray*}\\] Therefore: \\[\\begin{eqnarray*} &amp;&amp; T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\\\ &amp;=&amp; - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots \\end{eqnarray*}\\] And then: \\[\\begin{eqnarray*} &amp;&amp; \\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\\\ &amp;\\le&amp; 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots \\end{eqnarray*}\\] For any \\(q \\le T\\), we have: \\[\\begin{eqnarray*} \\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &amp;\\le&amp; 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\ &amp;&amp;2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\ &amp;\\le&amp; \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\ &amp;&amp;2|\\gamma_{q+1}| + \\dots + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots \\end{eqnarray*}\\] Consider \\(\\varepsilon &gt; 0\\). The fact that the autocovariances are absolutely summable implies that there exists \\(q_0\\) such that (Cauchy criterion, Theorem 9.2): \\[ 2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots &lt; \\varepsilon/2. \\] Then, if \\(T &gt; q_0\\), it comes that: \\[ \\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2. \\] If \\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) then \\[ \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2. \\] Then, if \\(T&gt;f(q_0)\\) and \\(T&gt;q_0\\), i.e. if \\(T&gt;\\max(f(q_0),q_0)\\), we have: \\[ \\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon. \\] Proof of Proposition ?? Proof. We have: \\[\\begin{eqnarray} \\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &amp;=&amp; \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t) - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\ &amp;=&amp; \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t) - y^*_{t+1}]}^2\\right)\\nonumber\\\\ &amp;&amp; + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t) - y^*_{t+1}]}\\right). \\tag{9.18} \\end{eqnarray}\\] Let us focus on the last term. We have: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t) - y^*_{t+1}]}\\right)\\\\ &amp;=&amp; \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - y^*_{t+1}]}_{\\mbox{function of $x_t$}}}|x_t))\\\\ &amp;=&amp; \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t) - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\ &amp;=&amp; \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t) - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0. \\end{eqnarray*}\\] Therefore, Eq. (9.18) becomes: \\[\\begin{eqnarray*} &amp;&amp;\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\ &amp;=&amp; \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ and does not depend on $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t) - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ and depends on $y^*_{t+1}$}}. \\end{eqnarray*}\\] This implies that \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) is always larger than \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), and is therefore minimized if the second term is equal to zero, that is if \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\). Proof of Proposition ?? Proof. Using Proposition 9.18, we obtain that, conditionally on \\(x_1\\), the log-likelihood is given by \\[\\begin{eqnarray*} \\log\\mathcal{L}(Y_{T};\\theta) &amp; = &amp; -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\ &amp; &amp; -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\Pi&#39;x_{t}\\right)\\right]. \\end{eqnarray*}\\] Let’s rewrite the last term of the log-likelihood: \\[\\begin{eqnarray*} \\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\Pi&#39;x_{t}\\right)\\right] &amp; =\\\\ \\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}&#39;x_{t}+\\hat{\\Pi}&#39;x_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}&#39;x_{t}+\\hat{\\Pi}&#39;x_{t}-\\Pi&#39;x_{t}\\right)\\right] &amp; =\\\\ \\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\right)\\right], \\end{eqnarray*}\\] where the \\(j^{th}\\) element of the \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) is the sample residual, for observation \\(t\\), from an OLS regression of \\(y_{j,t}\\) on \\(x_{t}\\). Expanding the previous equation, we get: \\[\\begin{eqnarray*} &amp;&amp;\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\Pi&#39;x_{t}\\right)\\right] = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\ &amp;&amp;+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t}+\\sum_{t=1}^{T}x&#39;_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t}. \\end{eqnarray*}\\] Let’s apply the trace operator on the second term (that is a scalar): \\[\\begin{eqnarray*} \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t} &amp; = &amp; Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\right)\\\\ = Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\hat{\\varepsilon}_{t}&#39;\\right) &amp; = &amp; Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)&#39;\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}&#39;\\right). \\end{eqnarray*}\\] Given that, by construction (property of OLS estimates), the sample residuals are orthogonal to the explanatory variables, this term is zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)&#39;x_{t}\\), we have \\[\\begin{eqnarray*} \\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi&#39;x_{t}\\right)&#39;\\Omega^{-1}\\left(y_{t}-\\Pi&#39;x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}&#39;_{t}\\Omega^{-1}\\tilde{x}_{t}. \\end{eqnarray*}\\] Since \\(\\Omega\\) is a positive definite matrix, \\(\\Omega^{-1}\\) is as well. Consequently, the smallest value that the last term can take is obtained for \\(\\tilde{x}_{t}=0\\), i.e. when \\(\\Pi=\\hat{\\Pi}.\\) The MLE of \\(\\Omega\\) is the matrix \\(\\hat{\\Omega}\\) that maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). We have: \\[\\begin{eqnarray*} \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) &amp; = &amp; -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}&#39;\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right]. \\end{eqnarray*}\\] Matrix \\(\\hat{\\Omega}\\) is a symmetric positive definite. It is easily checked that the (unrestricted) matrix that maximizes the latter expression is symmetric positive definite matrix. Indeed: \\[ \\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega&#39;-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}&#39;_{t}\\Rightarrow\\hat{\\Omega}&#39;=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}&#39;_{t}, \\] which leads to the result. Proof of Proposition ?? Proof. Let us drop the \\(i\\) subscript. Rearranging Eq. (??), we have: \\[ \\sqrt{T}(\\bv{b}-\\boldsymbol{\\beta}) = (X&#39;X/T)^{-1}\\sqrt{T}(X&#39;\\boldsymbol\\varepsilon/T). \\] Let us consider the autocovariances of \\(\\bv{v}_t = x_t \\varepsilon_t\\), denoted by \\(\\gamma^v_j\\). Using the fact that \\(x_t\\) is a linear combination of past \\(\\varepsilon_t\\)s and that \\(\\varepsilon_t\\) is a white noise, we get that \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore \\[ \\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}&#39;). \\] If \\(j&gt;0\\), we have \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}&#39;)=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}&#39;|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}&#39;\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note that we have \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) because \\(\\{\\varepsilon_t\\}\\) is an i.i.d. white noise sequence. If \\(j=0\\), we have: \\[ \\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}&#39;)= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}&#39;)=\\sigma^2\\bv{Q}. \\] The convergence in distribution of \\(\\sqrt{T}(X&#39;\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results from the Central Limit Theorem for covariance-stationary processes, using the \\(\\gamma_j^v\\) computed above. 9.6 Additional codes 9.6.1 Simulating GEV distributions The following lines of code have been used to generate Figure ??. n.sim &lt;- 4000 par(mfrow=c(1,3), plt=c(.2,.95,.2,.85)) all.rhos &lt;- c(.3,.6,.95) for(j in 1:length(all.rhos)){ theta &lt;- 1/all.rhos[j] v1 &lt;- runif(n.sim) v2 &lt;- runif(n.sim) w &lt;- rep(.000001,n.sim) # solve for f(w) = w*(1 - log(w)/theta) - v2 = 0 for(i in 1:20){ f.i &lt;- w * (1 - log(w)/theta) - v2 f.prime &lt;- 1 - log(w)/theta - 1/theta w &lt;- w - f.i/f.prime } u1 &lt;- exp(v1^(1/theta) * log(w)) u2 &lt;- exp((1-v1)^(1/theta) * log(w)) # Get eps1 and eps2 using the inverse of # the Gumbel distribution&#39;s cdf: eps1 &lt;- -log(-log(u1)) eps2 &lt;- -log(-log(u2)) cbind(cor(eps1,eps2),1-all.rhos[j]^2) plot(eps1,eps2,pch=19,col=&quot;#FF000044&quot;, main=paste(&quot;rho = &quot;,toString(all.rhos[j]),sep=&quot;&quot;), xlab=expression(epsilon[1]), ylab=expression(epsilon[2]), cex.lab=2,cex.main=1.5) } 9.6.2 Computing the covariance matrix of IRF using the delta method irf.function &lt;- function(THETA){ c &lt;- THETA[1] phi &lt;- THETA[2:(p+1)] if(q&gt;0){ theta &lt;- c(1,THETA[(1+p+1):(1+p+q)]) }else{ theta &lt;- 1 } sigma &lt;- THETA[1+p+q+1] r &lt;- dim(Matrix.of.Exog)[2] - 1 beta &lt;- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))] irf &lt;- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60, y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1, X=NaN,beta=NaN) return(irf) } IRF.0 &lt;- 100*irf.function(x$THETA) eps &lt;- .00000001 d.IRF &lt;- NULL for(i in 1:length(x$THETA)){ THETA.i &lt;- x$THETA THETA.i[i] &lt;- THETA.i[i] + eps IRF.i &lt;- 100*irf.function(THETA.i) d.IRF &lt;- cbind(d.IRF, (IRF.i - IRF.0)/eps ) } mat.var.cov.IRF &lt;- d.IRF %*% x$I %*% t(d.IRF) 9.7 Statistical Tables Table 9.1: Quantiles of the \\(\\mathcal{N}(0,1)\\) distribution. If \\(a\\) and \\(b\\) are respectively the row and column number; then the corresponding cell gives \\(\\mathbb{P}(0&lt;X\\le a+b)\\), where \\(X \\sim \\mathcal{N}(0,1)\\). 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.5000 0.6179 0.7257 0.8159 0.8849 0.9332 0.9641 0.9821 0.9918 0.9965 0.1 0.5040 0.6217 0.7291 0.8186 0.8869 0.9345 0.9649 0.9826 0.9920 0.9966 0.2 0.5080 0.6255 0.7324 0.8212 0.8888 0.9357 0.9656 0.9830 0.9922 0.9967 0.3 0.5120 0.6293 0.7357 0.8238 0.8907 0.9370 0.9664 0.9834 0.9925 0.9968 0.4 0.5160 0.6331 0.7389 0.8264 0.8925 0.9382 0.9671 0.9838 0.9927 0.9969 0.5 0.5199 0.6368 0.7422 0.8289 0.8944 0.9394 0.9678 0.9842 0.9929 0.9970 0.6 0.5239 0.6406 0.7454 0.8315 0.8962 0.9406 0.9686 0.9846 0.9931 0.9971 0.7 0.5279 0.6443 0.7486 0.8340 0.8980 0.9418 0.9693 0.9850 0.9932 0.9972 0.8 0.5319 0.6480 0.7517 0.8365 0.8997 0.9429 0.9699 0.9854 0.9934 0.9973 0.9 0.5359 0.6517 0.7549 0.8389 0.9015 0.9441 0.9706 0.9857 0.9936 0.9974 1 0.5398 0.6554 0.7580 0.8413 0.9032 0.9452 0.9713 0.9861 0.9938 0.9974 1.1 0.5438 0.6591 0.7611 0.8438 0.9049 0.9463 0.9719 0.9864 0.9940 0.9975 1.2 0.5478 0.6628 0.7642 0.8461 0.9066 0.9474 0.9726 0.9868 0.9941 0.9976 1.3 0.5517 0.6664 0.7673 0.8485 0.9082 0.9484 0.9732 0.9871 0.9943 0.9977 1.4 0.5557 0.6700 0.7704 0.8508 0.9099 0.9495 0.9738 0.9875 0.9945 0.9977 1.5 0.5596 0.6736 0.7734 0.8531 0.9115 0.9505 0.9744 0.9878 0.9946 0.9978 1.6 0.5636 0.6772 0.7764 0.8554 0.9131 0.9515 0.9750 0.9881 0.9948 0.9979 1.7 0.5675 0.6808 0.7794 0.8577 0.9147 0.9525 0.9756 0.9884 0.9949 0.9979 1.8 0.5714 0.6844 0.7823 0.8599 0.9162 0.9535 0.9761 0.9887 0.9951 0.9980 1.9 0.5753 0.6879 0.7852 0.8621 0.9177 0.9545 0.9767 0.9890 0.9952 0.9981 2 0.5793 0.6915 0.7881 0.8643 0.9192 0.9554 0.9772 0.9893 0.9953 0.9981 2.1 0.5832 0.6950 0.7910 0.8665 0.9207 0.9564 0.9778 0.9896 0.9955 0.9982 2.2 0.5871 0.6985 0.7939 0.8686 0.9222 0.9573 0.9783 0.9898 0.9956 0.9982 2.3 0.5910 0.7019 0.7967 0.8708 0.9236 0.9582 0.9788 0.9901 0.9957 0.9983 2.4 0.5948 0.7054 0.7995 0.8729 0.9251 0.9591 0.9793 0.9904 0.9959 0.9984 2.5 0.5987 0.7088 0.8023 0.8749 0.9265 0.9599 0.9798 0.9906 0.9960 0.9984 2.6 0.6026 0.7123 0.8051 0.8770 0.9279 0.9608 0.9803 0.9909 0.9961 0.9985 2.7 0.6064 0.7157 0.8078 0.8790 0.9292 0.9616 0.9808 0.9911 0.9962 0.9985 2.8 0.6103 0.7190 0.8106 0.8810 0.9306 0.9625 0.9812 0.9913 0.9963 0.9986 2.9 0.6141 0.7224 0.8133 0.8830 0.9319 0.9633 0.9817 0.9916 0.9964 0.9986 Table 9.2: Quantiles of the Student-\\(t\\) distribution. The rows correspond to different degrees of freedom (\\(\\nu\\), say); the columns correspond to different probabilities (\\(z\\), say). The cell gives \\(q\\) that is s.t. \\(\\mathbb{P}(-q&lt;X&lt;q)=z\\), with \\(X \\sim t(\\nu)\\). 0.05 0.1 0.75 0.9 0.95 0.975 0.99 0.999 1 0.079 0.158 2.414 6.314 12.706 25.452 63.657 636.619 2 0.071 0.142 1.604 2.920 4.303 6.205 9.925 31.599 3 0.068 0.137 1.423 2.353 3.182 4.177 5.841 12.924 4 0.067 0.134 1.344 2.132 2.776 3.495 4.604 8.610 5 0.066 0.132 1.301 2.015 2.571 3.163 4.032 6.869 6 0.065 0.131 1.273 1.943 2.447 2.969 3.707 5.959 7 0.065 0.130 1.254 1.895 2.365 2.841 3.499 5.408 8 0.065 0.130 1.240 1.860 2.306 2.752 3.355 5.041 9 0.064 0.129 1.230 1.833 2.262 2.685 3.250 4.781 10 0.064 0.129 1.221 1.812 2.228 2.634 3.169 4.587 20 0.063 0.127 1.185 1.725 2.086 2.423 2.845 3.850 30 0.063 0.127 1.173 1.697 2.042 2.360 2.750 3.646 40 0.063 0.126 1.167 1.684 2.021 2.329 2.704 3.551 50 0.063 0.126 1.164 1.676 2.009 2.311 2.678 3.496 60 0.063 0.126 1.162 1.671 2.000 2.299 2.660 3.460 70 0.063 0.126 1.160 1.667 1.994 2.291 2.648 3.435 80 0.063 0.126 1.159 1.664 1.990 2.284 2.639 3.416 90 0.063 0.126 1.158 1.662 1.987 2.280 2.632 3.402 100 0.063 0.126 1.157 1.660 1.984 2.276 2.626 3.390 200 0.063 0.126 1.154 1.653 1.972 2.258 2.601 3.340 500 0.063 0.126 1.152 1.648 1.965 2.248 2.586 3.310 Table 9.3: Quantiles of the \\(\\chi^2\\) distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities. 0.05 0.1 0.75 0.9 0.95 0.975 0.99 0.999 1 0.004 0.016 1.323 2.706 3.841 5.024 6.635 10.828 2 0.103 0.211 2.773 4.605 5.991 7.378 9.210 13.816 3 0.352 0.584 4.108 6.251 7.815 9.348 11.345 16.266 4 0.711 1.064 5.385 7.779 9.488 11.143 13.277 18.467 5 1.145 1.610 6.626 9.236 11.070 12.833 15.086 20.515 6 1.635 2.204 7.841 10.645 12.592 14.449 16.812 22.458 7 2.167 2.833 9.037 12.017 14.067 16.013 18.475 24.322 8 2.733 3.490 10.219 13.362 15.507 17.535 20.090 26.124 9 3.325 4.168 11.389 14.684 16.919 19.023 21.666 27.877 10 3.940 4.865 12.549 15.987 18.307 20.483 23.209 29.588 20 10.851 12.443 23.828 28.412 31.410 34.170 37.566 45.315 30 18.493 20.599 34.800 40.256 43.773 46.979 50.892 59.703 40 26.509 29.051 45.616 51.805 55.758 59.342 63.691 73.402 50 34.764 37.689 56.334 63.167 67.505 71.420 76.154 86.661 60 43.188 46.459 66.981 74.397 79.082 83.298 88.379 99.607 70 51.739 55.329 77.577 85.527 90.531 95.023 100.425 112.317 80 60.391 64.278 88.130 96.578 101.879 106.629 112.329 124.839 90 69.126 73.291 98.650 107.565 113.145 118.136 124.116 137.208 100 77.929 82.358 109.141 118.498 124.342 129.561 135.807 149.449 200 168.279 174.835 213.102 226.021 233.994 241.058 249.445 267.541 500 449.147 459.926 520.950 540.930 553.127 563.852 576.493 603.446 Table 9.4: Quantiles of the \\(\\mathcal{F}\\) distribution. The columns and rows correspond to different degrees of freedom (resp. \\(n_1\\) and \\(n_2\\)). The different panels correspond to different probabilities (\\(\\alpha\\)) The corresponding cell gives \\(z\\) that is s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), with \\(X \\sim \\mathcal{F}(n_1,n_2)\\). 1 2 3 4 5 6 7 8 9 10 alpha = 0.9 5 4.060 3.780 3.619 3.520 3.453 3.405 3.368 3.339 3.316 3.297 10 3.285 2.924 2.728 2.605 2.522 2.461 2.414 2.377 2.347 2.323 15 3.073 2.695 2.490 2.361 2.273 2.208 2.158 2.119 2.086 2.059 20 2.975 2.589 2.380 2.249 2.158 2.091 2.040 1.999 1.965 1.937 50 2.809 2.412 2.197 2.061 1.966 1.895 1.840 1.796 1.760 1.729 100 2.756 2.356 2.139 2.002 1.906 1.834 1.778 1.732 1.695 1.663 500 2.716 2.313 2.095 1.956 1.859 1.786 1.729 1.683 1.644 1.612 alpha = 0.95 5 6.608 5.786 5.409 5.192 5.050 4.950 4.876 4.818 4.772 4.735 10 4.965 4.103 3.708 3.478 3.326 3.217 3.135 3.072 3.020 2.978 15 4.543 3.682 3.287 3.056 2.901 2.790 2.707 2.641 2.588 2.544 20 4.351 3.493 3.098 2.866 2.711 2.599 2.514 2.447 2.393 2.348 50 4.034 3.183 2.790 2.557 2.400 2.286 2.199 2.130 2.073 2.026 100 3.936 3.087 2.696 2.463 2.305 2.191 2.103 2.032 1.975 1.927 500 3.860 3.014 2.623 2.390 2.232 2.117 2.028 1.957 1.899 1.850 alpha = 0.99 5 16.258 13.274 12.060 11.392 10.967 10.672 10.456 10.289 10.158 10.051 10 10.044 7.559 6.552 5.994 5.636 5.386 5.200 5.057 4.942 4.849 15 8.683 6.359 5.417 4.893 4.556 4.318 4.142 4.004 3.895 3.805 20 8.096 5.849 4.938 4.431 4.103 3.871 3.699 3.564 3.457 3.368 50 7.171 5.057 4.199 3.720 3.408 3.186 3.020 2.890 2.785 2.698 100 6.895 4.824 3.984 3.513 3.206 2.988 2.823 2.694 2.590 2.503 500 6.686 4.648 3.821 3.357 3.054 2.838 2.675 2.547 2.443 2.356 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
