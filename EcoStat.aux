\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{apalike}
\HyPL@Entry{0<</S/D>>}
\newlabel{intro}{{}{7}{Econometrics and statistics}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Basic statistical results}{9}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Basics}{{1}{9}{Basic statistical results}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Cumulative and probability density funtions (c.d.f. and p.d.f.)}{9}{section.1.1}\protected@file@percent }
\newlabel{cumulative-and-probability-density-funtions-c.d.f.-and-p.d.f.}{{1.1}{9}{Cumulative and probability density funtions (c.d.f. and p.d.f.)}{section.1.1}{}}
\newlabel{def:cdf}{{1.1}{9}{Cumulative distribution function (c.d.f.)}{definition.1.1}{}}
\newlabel{def:pdf}{{1.2}{9}{Probability distribution function (p.d.f.)}{definition.1.2}{}}
\newlabel{eq:flim}{{1.1}{9}{Cumulative and probability density funtions (c.d.f. and p.d.f.)}{equation.1.1.1}{}}
\newlabel{def:jointcdf}{{1.3}{10}{Joint cumulative distribution function (c.d.f.)}{definition.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The volume between the horizontal plane ($z=0$) and the surface is equal to $F_{XY}(0.5,1)=\symbb {P}(X<0.5,Y<1)$.}}{10}{figure.1.1}\protected@file@percent }
\newlabel{fig:essai3d2}{{1.1}{10}{The volume between the horizontal plane ($z=0$) and the surface is equal to $F_{XY}(0.5,1)=\mathbb {P}(X<0.5,Y<1)$}{figure.1.1}{}}
\newlabel{def:jointpdf}{{1.4}{10}{Joint probability density function (p.d.f.)}{definition.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Assume that the basis of the black column is defined by those points whose $x$-coordinates are between $x$ and $x+\mitvarepsilon $ and $y$-coordinates are between $y$ and $y+\mitvarepsilon $. Then the volume of the black column is equal to $\symbb {P}(x < X \le x+\mitvarepsilon ,y < Y \le y+\mitvarepsilon )$, which is approximately equal to $f_{XY}(x,y)\mitvarepsilon ^2$ if $\mitvarepsilon $ is small.}}{11}{figure.1.2}\protected@file@percent }
\newlabel{fig:essai3d}{{1.2}{11}{Assume that the basis of the black column is defined by those points whose $x$-coordinates are between $x$ and $x+\varepsilon $ and $y$-coordinates are between $y$ and $y+\varepsilon $. Then the volume of the black column is equal to $\mathbb {P}(x < X \le x+\varepsilon ,y < Y \le y+\varepsilon )$, which is approximately equal to $f_{XY}(x,y)\varepsilon ^2$ if $\varepsilon $ is small}{figure.1.2}{}}
\newlabel{def:condcdf}{{1.5}{11}{Conditional probability distribution function}{definition.1.5}{}}
\newlabel{prp:condcdf}{{1.1}{12}{Conditional probability distribution function}{proposition.1.1}{}}
\newlabel{def:independent}{{1.6}{12}{Independent random variables}{definition.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Law of iterated expectations}{13}{section.1.2}\protected@file@percent }
\newlabel{law-of-iterated-expectations}{{1.2}{13}{Law of iterated expectations}{section.1.2}{}}
\newlabel{prp:lawiteratedexpect}{{1.2}{13}{Law of iterated expectations}{proposition.1.2}{}}
\newlabel{exm:mixture}{{1.1}{13}{Mixture of Gaussian distributions}{example.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Example of pdfs of mixtures of Gaussian distribututions.}}{14}{figure.1.3}\protected@file@percent }
\newlabel{fig:mixtureG}{{1.3}{14}{Example of pdfs of mixtures of Gaussian distribututions}{figure.1.3}{}}
\newlabel{exm:Buffon}{{1.2}{14}{Buffon (1733)'s needles}{example.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Schematic representation of the problem.}}{15}{figure.1.4}\protected@file@percent }
\newlabel{fig:Buffon}{{1.4}{15}{Schematic representation of the problem}{figure.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Law of total variance}{15}{section.1.3}\protected@file@percent }
\newlabel{law-of-total-variance}{{1.3}{15}{Law of total variance}{section.1.3}{}}
\newlabel{prp:lawtotalvariance}{{1.3}{15}{Law of total variance}{proposition.1.3}{}}
\newlabel{exm:mixture2}{{1.3}{15}{Mixture of Gaussian distributions (cont'd)}{example.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}About consistent estimators}{16}{section.1.4}\protected@file@percent }
\newlabel{about-consistent-estimators}{{1.4}{16}{About consistent estimators}{section.1.4}{}}
\newlabel{exm:NonConsist}{{1.4}{16}{Example of non-convergent estimator}{example.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Simulation of $\bar {X}_n$ when $X_i \sim i.i.d. \mbox  {Cauchy}$.}}{17}{figure.1.5}\protected@file@percent }
\newlabel{fig:figCauchy}{{1.5}{17}{Simulation of $\bar {X}_n$ when $X_i \sim i.i.d. \mbox {Cauchy}$}{figure.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Central Limit Theorem}{19}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{TCL}{{2}{19}{Central Limit Theorem}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Law of large numbers}{19}{section.2.1}\protected@file@percent }
\newlabel{law-of-large-numbers}{{2.1}{19}{Law of large numbers}{section.2.1}{}}
\newlabel{def:smplpop}{{2.1}{19}{Sample and Population}{definition.2.1}{}}
\newlabel{thm:LLN}{{2.1}{20}{Law of large numbers}{theorem.2.1}{}}
\newlabel{exm:doctorVisits}{{2.1}{20}{}{example.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Distribution of the number of doctor visits. Source: Swiss Household Panel.}}{21}{figure.2.1}\protected@file@percent }
\newlabel{fig:incomeDistri}{{2.1}{21}{Distribution of the number of doctor visits. Source: Swiss Household Panel}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Central Limit Theorem (CLT)}{22}{section.2.2}\protected@file@percent }
\newlabel{central-limit-theorem-clt}{{2.2}{22}{Central Limit Theorem (CLT)}{section.2.2}{}}
\newlabel{thm:LindbergLevyCLT}{{2.2}{22}{Lindberg-Levy Central limit theorem, CLT}{theorem.2.2}{}}
\newlabel{exr:ExoBuffon}{{2.1}{23}{}{exercise.2.1}{}}
\newlabel{exr:ExoPileFace}{{2.2}{23}{}{exercise.2.2}{}}
\newlabel{thm:MCLT}{{2.3}{23}{Multivariate Central limit theorem, CLT}{theorem.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Comparison of sample means}{23}{section.2.3}\protected@file@percent }
\newlabel{comparison-of-sample-means}{{2.3}{23}{Comparison of sample means}{section.2.3}{}}
\newlabel{exm:comparSmplMeans}{{2.2}{24}{Comparison of sample means}{example.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Distribution of yearly gross incomes for Swiss residents under the age of 35. Source: SHP. Vertical dashed lines indicates the sample mean.}}{25}{figure.2.2}\protected@file@percent }
\newlabel{fig:RcompMeans}{{2.2}{25}{Distribution of yearly gross incomes for Swiss residents under the age of 35. Source: SHP. Vertical dashed lines indicates the sample mean}{figure.2.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Statistical tests}{27}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Tests}{{3}{27}{Statistical tests}{chapter.3}{}}
\newlabel{exm:FPFN}{{3.1}{28}{Early Warning Signals}{example.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Size and power of a test}{29}{section.3.1}\protected@file@percent }
\newlabel{size-and-power-of-a-test}{{3.1}{29}{Size and power of a test}{section.3.1}{}}
\newlabel{def:sizepower}{{3.1}{29}{Size and Power of a test}{definition.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The different types of statistical tests}{29}{section.3.2}\protected@file@percent }
\newlabel{the-different-types-of-statistical-tests}{{3.2}{29}{The different types of statistical tests}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Two-sided test. Under $H_0$, $S \sim t(5)$. $\mitalpha $ is the size of the test.}}{30}{figure.3.1}\protected@file@percent }
\newlabel{fig:Illusttest1}{{3.1}{30}{Two-sided test. Under $H_0$, $S \sim t(5)$. $\alpha $ is the size of the test}{figure.3.1}{}}
\newlabel{exm:Factory}{{3.2}{30}{A practical illustration of size and power}{example.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Two-sided test. Under $H_0$, $S \sim t(5)$. $\mitalpha $ is the size of the test.}}{31}{figure.3.2}\protected@file@percent }
\newlabel{fig:Illusttest2}{{3.2}{31}{Two-sided test. Under $H_0$, $S \sim t(5)$. $\alpha $ is the size of the test}{figure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces One-sided test. Under $H_0$, $S \sim \mitchi ^2(5)$. $\mitalpha $ is the size of the test.}}{32}{figure.3.3}\protected@file@percent }
\newlabel{fig:IllusTestOneSided1}{{3.3}{32}{One-sided test. Under $H_0$, $S \sim \chi ^2(5)$. $\alpha $ is the size of the test}{figure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces One-sided test. Under $H_0$, $S \sim \mitchi ^2(5)$. $\mitalpha $ is the size of the test.}}{33}{figure.3.4}\protected@file@percent }
\newlabel{fig:IllusTestOneSided2}{{3.4}{33}{One-sided test. Under $H_0$, $S \sim \chi ^2(5)$. $\alpha $ is the size of the test}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Factory example.}}{35}{figure.3.5}\protected@file@percent }
\newlabel{fig:FactoryR}{{3.5}{35}{Factory example}{figure.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Asymptotic properties of statistical tests}{36}{section.3.3}\protected@file@percent }
\newlabel{asymptotic-properties-of-statistical-tests}{{3.3}{36}{Asymptotic properties of statistical tests}{section.3.3}{}}
\newlabel{def:asmyptlevel}{{3.2}{36}{Asymptotic level}{definition.3.2}{}}
\newlabel{exm:FactAsymptlevel}{{3.3}{36}{The factory example}{example.3.3}{}}
\newlabel{def:asmyptconsisttest}{{3.3}{37}{Asymptotically consistent test}{definition.3.3}{}}
\newlabel{exm:FactAsymptConsist}{{3.4}{37}{The factory example}{example.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Example: Normality tests}{37}{section.3.4}\protected@file@percent }
\newlabel{example-normality-tests}{{3.4}{37}{Example: Normality tests}{section.3.4}{}}
\newlabel{prp:normSkewKurt}{{3.1}{38}{Skewness and kurtosis of the normal distribution}{proposition.3.1}{}}
\newlabel{prp:conssitCentralMoments}{{3.2}{38}{Consistency of central sample moments}{proposition.3.2}{}}
\newlabel{prp:Asymptg3Normal}{{3.3}{39}{Asymptotic distribution of 3rd-order sample central moment of a normal distribution}{proposition.3.3}{}}
\newlabel{prp:Asymptg4Normal}{{3.4}{39}{Asymptotic distribution of 4th-order sample central moment of a normal distribution}{proposition.3.4}{}}
\newlabel{prp:Asymptg3g4Normal}{{3.5}{39}{Joint asymptotic distribution of 3rd and 4th-order sample central moments of a normal distribution}{proposition.3.5}{}}
\newlabel{prp:JB}{{3.6}{39}{Jarque-Bera asympt. distri}{proposition.3.6}{}}
\newlabel{exm:JB}{{3.5}{39}{Consistency of the Jarque-Bera normality test}{example.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Distribution of the JB test statistic under $H_0$ (normality).}}{41}{figure.3.6}\protected@file@percent }
\newlabel{fig:JBTest2}{{3.6}{41}{Distribution of the JB test statistic under $H_0$ (normality)}{figure.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Distribution of the JB test statistic when the $y_i$'s are drawn from a uniform distribution (hence $H_0$ is not satisfied).}}{42}{figure.3.7}\protected@file@percent }
\newlabel{fig:JBTest3}{{3.7}{42}{Distribution of the JB test statistic when the $y_i$'s are drawn from a uniform distribution (hence $H_0$ is not satisfied)}{figure.3.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Linear Regressions}{43}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ChapterLS}{{4}{43}{Linear Regressions}{chapter.4}{}}
\newlabel{def:essai}{{4.1}{43}{}{definition.4.1}{}}
\newlabel{eq:linearspecif}{{4.1}{43}{}{equation.4.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Hypotheses}{43}{section.4.1}\protected@file@percent }
\newlabel{linearHyp}{{4.1}{43}{Hypotheses}{section.4.1}{}}
\newlabel{hyp:fullrank}{{4.1}{44}{Full rank}{hypothesis.4.1}{}}
\newlabel{hyp:exogeneity}{{4.2}{44}{Conditional mean-zero assumption}{hypothesis.4.2}{}}
\newlabel{prp:implicationExog}{{4.1}{44}{}{proposition.4.1}{}}
\newlabel{hyp:homoskedasticity}{{4.3}{44}{Homoskedasticity}{hypothesis.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Homoskedasticity vs heteroskedasticity. See text for the exact specifications.}}{46}{figure.4.1}\protected@file@percent }
\newlabel{fig:heteroskedasticity}{{4.1}{46}{Homoskedasticity vs heteroskedasticity. See text for the exact specifications}{figure.4.1}{}}
\newlabel{hyp:noncorrelResid}{{4.4}{46}{Uncorrelated errors}{hypothesis.4.4}{}}
\newlabel{prp:Sigma}{{4.2}{46}{}{proposition.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Income versus age. Data are from the Swiss Household Panel. The sample is restricted to persons that have completed at least 19 years of study. The figure shows that the dispersion of yearly income increases with age.}}{47}{figure.4.2}\protected@file@percent }
\newlabel{fig:exmpSalarayPhDSHP}{{4.2}{47}{Income versus age. Data are from the Swiss Household Panel. The sample is restricted to persons that have completed at least 19 years of study. The figure shows that the dispersion of yearly income increases with age}{figure.4.2}{}}
\newlabel{hyp:normality}{{4.5}{47}{Normal distribution}{hypothesis.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Least square estimation}{47}{section.4.2}\protected@file@percent }
\newlabel{LSquares}{{4.2}{47}{Least square estimation}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Derivation of the OLS formula}{47}{subsection.4.2.1}\protected@file@percent }
\newlabel{derivation-of-the-ols-formula}{{4.2.1}{47}{Derivation of the OLS formula}{subsection.4.2.1}{}}
\newlabel{eq:OLSFOC}{{4.3}{48}{Derivation of the OLS formula}{equation.4.2.3}{}}
\newlabel{eq:Mres}{{4.4}{48}{Derivation of the OLS formula}{equation.4.2.4}{}}
\newlabel{eq:Proj}{{4.5}{48}{Derivation of the OLS formula}{equation.4.2.5}{}}
\newlabel{exm:bivar}{{4.1}{49}{Bivariate case}{example.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Properties of the OLS estimate (small sample)}{50}{subsection.4.2.2}\protected@file@percent }
\newlabel{properties-of-the-ols-estimate-small-sample}{{4.2.2}{50}{Properties of the OLS estimate (small sample)}{subsection.4.2.2}{}}
\newlabel{prp:propOLS}{{4.3}{50}{Properties of the OLS estimator}{proposition.4.3}{}}
\newlabel{thm:GaussMarkov}{{4.1}{51}{Gauss-Markov Theorem}{theorem.4.1}{}}
\newlabel{thm:FW}{{4.2}{51}{Frisch-Waugh Theorem}{theorem.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Goodness of fit}{53}{subsection.4.2.3}\protected@file@percent }
\newlabel{goodness-of-fit}{{4.2.3}{53}{Goodness of fit}{subsection.4.2.3}{}}
\citation{Greene2003Econometric}
\newlabel{eq:RR2}{{4.6}{54}{Goodness of fit}{equation.4.2.6}{}}
\newlabel{prp:chgeR2}{{4.4}{54}{Change in SSR when a variable is added}{proposition.4.4}{}}
\newlabel{eq:uu}{{4.7}{54}{Change in SSR when a variable is added}{equation.4.2.7}{}}
\newlabel{eq:uuu}{{4.8}{55}{Goodness of fit}{equation.4.2.8}{}}
\newlabel{prp:chgeInR2}{{4.5}{55}{Change in the coefficient of determination when a variable is added}{proposition.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces This figure illustrates the monotonous increase in the $R^2$ as a function of the number of explanatory variables. In the true model, there is no explanatory variables, i.e., $y_i = \mitvarepsilon _i$. We then take (independent) regressors and regress $y$ on the latter, progressively increasing the set of regressors.}}{56}{figure.4.3}\protected@file@percent }
\newlabel{fig:R2issue}{{4.3}{56}{This figure illustrates the monotonous increase in the $R^2$ as a function of the number of explanatory variables. In the true model, there is no explanatory variables, i.e., $y_i = \varepsilon _i$. We then take (independent) regressors and regress $y$ on the latter, progressively increasing the set of regressors}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Inference and confidence intervals (in small sample)}{57}{subsection.4.2.4}\protected@file@percent }
\newlabel{inference-and-confidence-intervals-in-small-sample}{{4.2.4}{57}{Inference and confidence intervals (in small sample)}{subsection.4.2.4}{}}
\newlabel{eq:distriBcondi}{{4.9}{57}{Inference and confidence intervals (in small sample)}{equation.4.2.9}{}}
\newlabel{prp:expects2}{{4.6}{57}{}{proposition.4.6}{}}
\newlabel{eq:s2}{{4.10}{57}{}{equation.4.2.10}{}}
\newlabel{prp:s2distri}{{4.7}{58}{}{proposition.4.7}{}}
\newlabel{prp:indeps2b}{{4.8}{58}{}{proposition.4.8}{}}
\newlabel{eq:resultstudentt}{{4.11}{58}{Inference and confidence intervals (in small sample)}{equation.4.2.11}{}}
\newlabel{eq:resultstudentt2}{{4.12}{59}{Inference and confidence intervals (in small sample)}{equation.4.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The higher the degree of freedom, the closer the distribution of \(t(\mitnu )\) gets to the normal distribution. (Convergence in distribution.)}}{60}{figure.4.4}\protected@file@percent }
\newlabel{fig:chartStudent}{{4.4}{60}{The higher the degree of freedom, the closer the distribution of \(t(\nu )\) gets to the normal distribution. (Convergence in distribution.)}{figure.4.4}{}}
\newlabel{exm:SHP0001}{{4.2}{61}{Education and income}{example.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Testing a set of linear restrictions}{63}{subsection.4.2.5}\protected@file@percent }
\newlabel{Ftest}{{4.2.5}{63}{Testing a set of linear restrictions}{subsection.4.2.5}{}}
\newlabel{eq:restrictions}{{4.13}{63}{Testing a set of linear restrictions}{equation.4.2.13}{}}
\newlabel{eq:H0Ftest}{{4.15}{63}{Testing a set of linear restrictions}{equation.4.2.15}{}}
\newlabel{eq:W1}{{4.16}{63}{Testing a set of linear restrictions}{equation.4.2.16}{}}
\newlabel{prp:Ftest1}{{4.9}{63}{}{proposition.4.9}{}}
\newlabel{eq:defFstatistics}{{4.17}{63}{}{equation.4.2.17}{}}
\newlabel{prp:Ftest}{{4.10}{64}{}{proposition.4.10}{}}
\newlabel{eq:defFstatistics2}{{4.18}{64}{}{equation.4.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Large Sample Properties}{65}{subsection.4.2.6}\protected@file@percent }
\newlabel{largeSample}{{4.2.6}{65}{Large Sample Properties}{subsection.4.2.6}{}}
\newlabel{prp:asymptOLS}{{4.11}{65}{}{proposition.4.11}{}}
\newlabel{eq:Qasympt}{{4.19}{65}{}{equation.4.2.19}{}}
\newlabel{eq:convgceOLS}{{4.20}{65}{}{equation.4.2.20}{}}
\newlabel{eq:sXX}{{4.21}{66}{Large Sample Properties}{equation.4.2.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Common pitfalls in linear regressions}{66}{section.4.3}\protected@file@percent }
\newlabel{CommonPitfalls}{{4.3}{66}{Common pitfalls in linear regressions}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Multicollinearity}{66}{subsection.4.3.1}\protected@file@percent }
\newlabel{multicollinearity}{{4.3.1}{66}{Multicollinearity}{subsection.4.3.1}{}}
\newlabel{eq:multicollin}{{4.22}{66}{Multicollinearity}{equation.4.3.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Omitted variables}{67}{subsection.4.3.2}\protected@file@percent }
\newlabel{Omitted}{{4.3.2}{67}{Omitted variables}{subsection.4.3.2}{}}
\newlabel{exm:CASchools}{{4.3}{67}{}{example.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Irrelevant variable}{68}{subsection.4.3.3}\protected@file@percent }
\newlabel{irrelevant}{{4.3.3}{68}{Irrelevant variable}{subsection.4.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Instrumental Variables}{69}{section.4.4}\protected@file@percent }
\newlabel{IV}{{4.4}{69}{Instrumental Variables}{section.4.4}{}}
\newlabel{eq:modelIV}{{4.23}{69}{Instrumental Variables}{equation.4.4.23}{}}
\newlabel{eq:exmIV}{{4.24}{69}{Instrumental Variables}{equation.4.4.24}{}}
\newlabel{def:instruments}{{4.2}{69}{Instrumental variables}{definition.4.2}{}}
\newlabel{prp:IV}{{4.12}{70}{Asymptotic distribution of the IV estimator}{proposition.4.12}{}}
\citation{stock_yogo_2005}
\citation{Andrews_Stock_Sun_2019}
\citation{Durbin_1954}
\citation{Wu_1973}
\citation{Hausman_1978}
\newlabel{eq:IV}{{4.25}{71}{Instrumental Variables}{equation.4.4.25}{}}
\newlabel{exm:priceElasticity}{{4.4}{71}{Estimation of price elasticity}{example.4.4}{}}
\citation{Stock_Watson_2003}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces This figure illustrates the situation prevailing when estimating a price-elasticity (and the price is endogenous).}}{73}{figure.4.5}\protected@file@percent }
\newlabel{fig:figureIV}{{4.5}{73}{This figure illustrates the situation prevailing when estimating a price-elasticity (and the price is endogenous)}{figure.4.5}{}}
\citation{Stock_Watson_2003}
\citation{DEE20041697}
\newlabel{exm:IVCollegeDistance}{{4.5}{75}{Education and wage}{example.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}General Regression Model (GRM) and robust covariance matrices}{76}{section.4.5}\protected@file@percent }
\newlabel{general-regression-model-grm-and-robust-covariance-matrices}{{4.5}{76}{General Regression Model (GRM) and robust covariance matrices}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Presentation of the General Regression Model (GRM)}{77}{subsection.4.5.1}\protected@file@percent }
\newlabel{presentation-of-the-general-regression-model-grm}{{4.5.1}{77}{Presentation of the General Regression Model (GRM)}{subsection.4.5.1}{}}
\newlabel{eq:assumGLS2}{{4.26}{77}{Presentation of the General Regression Model (GRM)}{equation.4.5.26}{}}
\newlabel{def:GRM}{{4.3}{77}{General Regression Model (GRM)}{definition.4.3}{}}
\newlabel{eq:heteroskedasticity}{{4.27}{77}{Presentation of the General Regression Model (GRM)}{equation.4.5.27}{}}
\newlabel{eq:autocorrelation}{{4.28}{77}{Presentation of the General Regression Model (GRM)}{equation.4.5.28}{}}
\newlabel{exm:autocorrelaaa}{{4.6}{77}{Auto-regressive processes}{example.4.6}{}}
\newlabel{eq:usual}{{4.29}{78}{Auto-regressive processes}{equation.4.5.29}{}}
\newlabel{eq:usual2}{{4.30}{78}{Auto-regressive processes}{equation.4.5.30}{}}
\newlabel{eq:SigmaAutocorrel}{{4.31}{78}{Auto-regressive processes}{equation.4.5.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Generalized Least Squares}{78}{subsection.4.5.2}\protected@file@percent }
\newlabel{GLS}{{4.5.2}{78}{Generalized Least Squares}{subsection.4.5.2}{}}
\newlabel{eq:betaGLS}{{4.32}{78}{Generalized Least Squares}{equation.4.5.32}{}}
\citation{Cochrane_Orcutt_1949}
\newlabel{exm:autocorrelaaa2}{{4.7}{79}{GLS in the auto-correlation case}{example.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Asymptotic properties of the OLS estimator in the GRM framework}{79}{subsection.4.5.3}\protected@file@percent }
\newlabel{asymptotic-properties-of-the-ols-estimator-in-the-grm-framework}{{4.5.3}{79}{Asymptotic properties of the OLS estimator in the GRM framework}{subsection.4.5.3}{}}
\newlabel{eq:xsx}{{4.33}{79}{Asymptotic properties of the OLS estimator in the GRM framework}{equation.4.5.33}{}}
\newlabel{prp:XXX}{{4.13}{79}{Consistency of the OLS estimator in the GRM framework}{proposition.4.13}{}}
\citation{White_1980}
\citation{Newey_West_1987}
\newlabel{prp:AsymptGRM}{{4.14}{80}{Asymptotic distribution of the OLS estimator in the GRM framework}{proposition.4.14}{}}
\newlabel{prp:AsymptIVGRM}{{4.15}{80}{Asymptotic distribution of the IV estimator in the GRM framework}{proposition.4.15}{}}
\newlabel{eq:GeneralXSigmaX}{{4.34}{80}{Asymptotic properties of the OLS estimator in the GRM framework}{equation.4.5.34}{}}
\citation{White_1980}
\citation{MacKinnon_White_1985}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}HAC-robust covariance matrices}{81}{subsection.4.5.4}\protected@file@percent }
\newlabel{HAC}{{4.5.4}{81}{HAC-robust covariance matrices}{subsection.4.5.4}{}}
\newlabel{exm:HCheteroskedasticity}{{4.8}{81}{Heteroskedasticity}{example.4.8}{}}
\newlabel{eq:white}{{4.35}{81}{Heteroskedasticity}{equation.4.5.35}{}}
\newlabel{eq:White}{{4.36}{81}{Heteroskedasticity}{equation.4.5.36}{}}
\newlabel{eq:WhiteHC1}{{4.37}{81}{Heteroskedasticity}{equation.4.5.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Situation of heteroskedasticity. The model is $y_i = x_i + \mitvarepsilon _i, \hskip 1em\relax \mitvarepsilon _i \sim \symcal {N}(0,x_i^2)$, where the $x_i$'s are i.i.d. $t(6)$.}}{82}{figure.4.6}\protected@file@percent }
\newlabel{fig:simulHeterosk}{{4.6}{82}{Situation of heteroskedasticity. The model is $y_i = x_i + \varepsilon _i, \quad \varepsilon _i \sim \mathcal {N}(0,x_i^2)$, where the $x_i$'s are i.i.d. $t(6)$}{figure.4.6}{}}
\citation{JST_2017}
\citation{Newey_West_1987}
\newlabel{exm:HCheterAC}{{4.9}{84}{Heteroskedasticity and Autocorrelation (HAC)}{example.4.9}{}}
\newlabel{eq:NW}{{4.39}{84}{Heteroskedasticity and Autocorrelation (HAC)}{equation.4.5.38}{}}
\newlabel{eq:simul11}{{4.39}{84}{Heteroskedasticity and Autocorrelation (HAC)}{equation.4.5.39}{}}
\newlabel{eq:simul22}{{4.40}{84}{Heteroskedasticity and Autocorrelation (HAC)}{equation.4.5.40}{}}
\citation{Durbin_Watson_1950}
\citation{Durbin_Watson_1951}
\newlabel{exm:DurbinWats}{{4.10}{86}{Durbin-Watson test}{example.4.10}{}}
\citation{MACKINNON2022}
\citation{Cameron_Miller_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Cluster-robust covariance matrices}{87}{subsection.4.5.5}\protected@file@percent }
\newlabel{Clusters}{{4.5.5}{87}{Cluster-robust covariance matrices}{subsection.4.5.5}{}}
\newlabel{eq:BBB}{{4.41}{87}{Cluster-robust covariance matrices}{equation.4.5.41}{}}
\newlabel{eq:cluster1}{{4.42}{88}{Cluster-robust covariance matrices}{equation.4.5.42}{}}
\newlabel{eq:definiS}{{4.43}{88}{Cluster-robust covariance matrices}{equation.4.5.43}{}}
\newlabel{hyp:cluster}{{4.6}{88}{Clusters}{hypothesis.4.6}{}}
\newlabel{eq:cluster2}{{4.44}{88}{Cluster-robust covariance matrices}{equation.4.5.44}{}}
\citation{White_1980}
\citation{MACKINNON2022}
\newlabel{eq:AsymptCL}{{4.45}{89}{Cluster-robust covariance matrices}{equation.4.5.45}{}}
\citation{MACKINNON2022}
\newlabel{hyp:twowaycluster}{{4.7}{90}{Two-way clusters}{hypothesis.4.7}{}}
\newlabel{prp:twoWayCov}{{4.16}{90}{Covariance of scores in the two-way-cluster setup}{proposition.4.16}{}}
\citation{MACKINNON2022}
\citation{Tibshirani_2011}
\citation{James2013}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Shrinkage methods}{91}{section.4.6}\protected@file@percent }
\newlabel{shrinkage-methods}{{4.6}{91}{Shrinkage methods}{section.4.6}{}}
\newlabel{eq:minLasso}{{4.47}{92}{Shrinkage methods}{equation.4.6.47}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Panel regressions}{99}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Panel}{{5}{99}{Panel regressions}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Specification and notations}{99}{section.5.1}\protected@file@percent }
\newlabel{specification-and-notations}{{5.1}{99}{Specification and notations}{section.5.1}{}}
\newlabel{eq:panel1}{{5.1}{99}{Specification and notations}{equation.5.1.1}{}}
\citation{Stock_Watson_2003}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The data are the same for both panels. On Panel (b), blue dots are for $t=1$, red dots are for $t=2$. The lines relate the dots associated to the same entity $i$.}}{100}{figure.5.1}\protected@file@percent }
\newlabel{fig:simulPanel}{{5.1}{100}{The data are the same for both panels. On Panel (b), blue dots are for $t=1$, red dots are for $t=2$. The lines relate the dots associated to the same entity $i$}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Cigarette consumption versus real price in the CigarettesSW panel dataset.}}{101}{figure.5.2}\protected@file@percent }
\newlabel{fig:cigarettes}{{5.2}{101}{Cigarette consumption versus real price in the CigarettesSW panel dataset}{figure.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Three standard cases}{102}{section.5.2}\protected@file@percent }
\newlabel{three-standard-cases}{{5.2}{102}{Three standard cases}{section.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Estimation of Fixed-Effects Models}{103}{section.5.3}\protected@file@percent }
\newlabel{FixedEffect}{{5.3}{103}{Estimation of Fixed-Effects Models}{section.5.3}{}}
\newlabel{hyp:FE}{{5.1}{103}{Fixed-effect model}{hypothesis.5.1}{}}
\newlabel{eq:LSDV}{{5.2}{103}{Estimation of Fixed-Effects Models}{equation.5.3.2}{}}
\newlabel{eq:bfixedeffects11}{{5.3}{104}{Estimation of Fixed-Effects Models}{equation.5.3.3}{}}
\newlabel{eq:bfixedeffects}{{5.5}{104}{Estimation of Fixed-Effects Models}{equation.5.3.5}{}}
\newlabel{eq:a}{{5.6}{104}{Estimation of Fixed-Effects Models}{equation.5.3.6}{}}
\newlabel{eq:LSDV2}{{5.7}{105}{Estimation of Fixed-Effects Models}{equation.5.3.7}{}}
\citation{JST_2017}
\newlabel{exm:JSTPanel}{{5.1}{106}{Housing prices and interest rates}{example.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Estimation of random effects models}{108}{section.5.4}\protected@file@percent }
\newlabel{RandomEffect}{{5.4}{108}{Estimation of random effects models}{section.5.4}{}}
\newlabel{eq:OLSRUM}{{5.8}{109}{Estimation of random effects models}{equation.5.4.8}{}}
\citation{Hausman_1978}
\newlabel{exm:airbnb}{{5.2}{111}{Spatial data}{example.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Airbnb prices for the Zurich area, 22 June 2017. The size of the circles is proportional to the prices. White lines delineate the 12 districts of the city.}}{112}{figure.5.3}\protected@file@percent }
\newlabel{fig:airbnb}{{5.3}{112}{Airbnb prices for the Zurich area, 22 June 2017. The size of the circles is proportional to the prices. White lines delineate the 12 districts of the city}{figure.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Regression residuals. The sizes of the circles are proportional to the absolute values of the residuals, the color indicates the sign (blue for negative).}}{114}{figure.5.4}\protected@file@percent }
\newlabel{fig:airbnb3}{{5.4}{114}{Regression residuals. The sizes of the circles are proportional to the absolute values of the residuals, the color indicates the sign (blue for negative)}{figure.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Dynamic Panel Regressions}{114}{section.5.5}\protected@file@percent }
\newlabel{DynPanel}{{5.5}{114}{Dynamic Panel Regressions}{section.5.5}{}}
\newlabel{eq:paneldyn}{{5.9}{114}{Dynamic Panel Regressions}{equation.5.5.9}{}}
\citation{Anderson_Hsiao_1982}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces illustration of the bias pertianing to the LSDV estimation approach in the presence of auto-correlation of the depend variable.}}{116}{figure.5.5}\protected@file@percent }
\newlabel{fig:dynpanel1}{{5.5}{116}{illustration of the bias pertianing to the LSDV estimation approach in the presence of auto-correlation of the depend variable}{figure.5.5}{}}
\newlabel{eq:paneldynFisrtDiff}{{5.10}{116}{Dynamic Panel Regressions}{equation.5.5.10}{}}
\citation{Arellano_Bond_1991}
\citation{Fritsch_et_al_2019}
\citation{Arellano_Bond_1991}
\citation{Arellano_Bond_1991}
\citation{Abadie_Cattaneo_2018}
\citation{angrist_mostly_2008}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Introduction to program evaluation}{122}{section.5.6}\protected@file@percent }
\newlabel{introduction-to-program-evaluation}{{5.6}{122}{Introduction to program evaluation}{section.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Presentation of the problem}{122}{subsection.5.6.1}\protected@file@percent }
\newlabel{presentation-of-the-problem}{{5.6.1}{122}{Presentation of the problem}{subsection.5.6.1}{}}
\newlabel{eq:EY1a}{{5.11}{123}{Presentation of the problem}{equation.5.6.11}{}}
\newlabel{eq:EY0a}{{5.12}{123}{Presentation of the problem}{equation.5.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Randomized controlled trials (RCTs)}{123}{subsection.5.6.2}\protected@file@percent }
\newlabel{randomized-controlled-trials-rcts}{{5.6.2}{123}{Randomized controlled trials (RCTs)}{subsection.5.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Difference-in-Difference (DiD) approach}{123}{subsection.5.6.3}\protected@file@percent }
\newlabel{difference-in-difference-did-approach}{{5.6.3}{123}{Difference-in-Difference (DiD) approach}{subsection.5.6.3}{}}
\citation{Meyer_Viscusi_Durbin_1995}
\newlabel{eq:DiD}{{5.13}{124}{Difference-in-Difference (DiD) approach}{equation.5.6.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.4}Application of the DiD approach}{124}{subsection.5.6.4}\protected@file@percent }
\newlabel{application-of-the-did-approach}{{5.6.4}{124}{Application of the DiD approach}{subsection.5.6.4}{}}
\citation{Meyer_Viscusi_Durbin_1995}
\citation{Meyer_Viscusi_Durbin_1995}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Source: Abadie et al., (1998).}}{125}{figure.5.6}\protected@file@percent }
\newlabel{fig:figAbadie}{{5.6}{125}{Source: Abadie et al., (1998)}{figure.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Source: Meyer et al., (1995).}}{126}{figure.5.7}\protected@file@percent }
\newlabel{fig:figMeyer}{{5.7}{126}{Source: Meyer et al., (1995)}{figure.5.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Estimation Methods}{129}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{estimation-methods}{{6}{129}{Estimation Methods}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Generalized Method of Moments (GMM)}{129}{section.6.1}\protected@file@percent }
\newlabel{secGMM}{{6.1}{129}{Generalized Method of Moments (GMM)}{section.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Definition of the GMM estimator}{129}{subsection.6.1.1}\protected@file@percent }
\newlabel{definition-of-the-gmm-estimator}{{6.1.1}{129}{Definition of the GMM estimator}{subsection.6.1.1}{}}
\newlabel{def:GMM}{{6.1}{130}{}{definition.6.1}{}}
\newlabel{eq:consistGMM}{{6.1}{130}{Definition of the GMM estimator}{equation.6.1.1}{}}
\citation{Newey_West_1987}
\citation{Sargan_1958}
\citation{Hansen_1982}
\newlabel{eq:Shat}{{6.2}{131}{Definition of the GMM estimator}{equation.6.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Asymptotic distribution of the GMM estimator}{131}{subsection.6.1.2}\protected@file@percent }
\newlabel{asymptotic-distribution-of-the-gmm-estimator}{{6.1.2}{131}{Asymptotic distribution of the GMM estimator}{subsection.6.1.2}{}}
\newlabel{eq:asymptGMM}{{6.3}{131}{Asymptotic distribution of the GMM estimator}{equation.6.1.3}{}}
\newlabel{eq:VGMM}{{6.4}{131}{Asymptotic distribution of the GMM estimator}{equation.6.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Testing hypotheses in the GMM framework}{132}{subsection.6.1.3}\protected@file@percent }
\newlabel{overidentif}{{6.1.3}{132}{Testing hypotheses in the GMM framework}{subsection.6.1.3}{}}
\newlabel{eq:HansenSargan}{{6.5}{132}{Testing hypotheses in the GMM framework}{equation.6.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Example: Estimation of the Stochastic Discount Factor (s.d.f.)}{132}{subsection.6.1.4}\protected@file@percent }
\newlabel{example-estimation-of-the-stochastic-discount-factor-s.d.f.}{{6.1.4}{132}{Example: Estimation of the Stochastic Discount Factor (s.d.f.)}{subsection.6.1.4}{}}
\newlabel{eq:sdf}{{6.6}{133}{Example: Estimation of the Stochastic Discount Factor (s.d.f.)}{equation.6.1.6}{}}
\newlabel{eq:momF}{{6.7}{133}{Example: Estimation of the Stochastic Discount Factor (s.d.f.)}{equation.6.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Maximum Likelihood Estimation}{136}{section.6.2}\protected@file@percent }
\newlabel{secMLE}{{6.2}{136}{Maximum Likelihood Estimation}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Intuition}{136}{subsection.6.2.1}\protected@file@percent }
\newlabel{intuition}{{6.2.1}{136}{Intuition}{subsection.6.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations}}{137}{figure.6.1}\protected@file@percent }
\newlabel{fig:MLE1}{{6.1}{137}{The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations}{figure.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Proba. that $y_i-\mitvarepsilon \le Y_i < y_i+\mitvarepsilon $, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function.}}{138}{figure.6.2}\protected@file@percent }
\newlabel{fig:MLE2}{{6.2}{138}{Proba. that $y_i-\varepsilon \le Y_i < y_i+\varepsilon $, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function}{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.}}{139}{figure.6.3}\protected@file@percent }
\newlabel{fig:MLE3}{{6.3}{139}{Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function}{figure.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Definition and properties}{139}{subsection.6.2.2}\protected@file@percent }
\newlabel{definition-and-properties}{{6.2.2}{139}{Definition and properties}{subsection.6.2.2}{}}
\newlabel{def:likelihood}{{6.2}{139}{Likelihood function}{definition.6.2}{}}
\newlabel{exm:normal}{{6.1}{139}{Gaussian distribution}{example.6.1}{}}
\newlabel{def:score}{{6.3}{140}{Score}{definition.6.3}{}}
\newlabel{prp:score}{{6.1}{140}{Score expectation}{proposition.6.1}{}}
\newlabel{def:Fisher}{{6.4}{140}{Fisher information matrix}{definition.6.4}{}}
\newlabel{prp:Fisher}{{6.2}{140}{}{proposition.6.2}{}}
\newlabel{prp:additiv}{{6.3}{141}{Additive property of the Information matrix}{proposition.6.3}{}}
\newlabel{thm:FDCR}{{6.1}{141}{Frechet-Darmois-Cramer-Rao bound}{theorem.6.1}{}}
\newlabel{def:identif}{{6.5}{141}{Identifiability}{definition.6.5}{}}
\newlabel{def:MLEest}{{6.6}{141}{Maximum Likelihood Estimator (MLE)}{definition.6.6}{}}
\newlabel{eq:MLEestimator}{{6.8}{141}{Maximum Likelihood Estimator (MLE)}{equation.6.2.8}{}}
\newlabel{def:likFunction}{{6.7}{141}{Likelihood equation}{definition.6.7}{}}
\newlabel{hyp:MLEregularity}{{6.1}{142}{Regularity assumptions}{hypothesis.6.1}{}}
\newlabel{prp:MLEproperties}{{6.4}{142}{Properties of MLE}{proposition.6.4}{}}
\newlabel{eq:normMLE}{{6.10}{142}{Properties of MLE}{equation.6.2.10}{}}
\newlabel{eq:III1}{{6.11}{143}{Definition and properties}{equation.6.2.11}{}}
\newlabel{eq:I2}{{6.12}{143}{Definition and properties}{equation.6.2.11}{}}
\newlabel{eq:III3}{{6.13}{143}{Definition and properties}{equation.6.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}To sum up -- MLE in practice}{143}{subsection.6.2.3}\protected@file@percent }
\newlabel{to-sum-up-mle-in-practice}{{6.2.3}{143}{To sum up -- MLE in practice}{subsection.6.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Example: MLE estimation of a mixture of Gaussian distribution}{144}{subsection.6.2.4}\protected@file@percent }
\newlabel{example-mle-estimation-of-a-mixture-of-gaussian-distribution}{{6.2.4}{144}{Example: MLE estimation of a mixture of Gaussian distribution}{subsection.6.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Time series of SMI weekly returns (source: Yahoo Finance).}}{145}{figure.6.4}\protected@file@percent }
\newlabel{fig:smiData}{{6.4}{145}{Time series of SMI weekly returns (source: Yahoo Finance)}{figure.6.4}{}}
\newlabel{eq:DeltaMethod}{{6.15}{146}{Example: MLE estimation of a mixture of Gaussian distribution}{equation.6.2.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Comparison of different estimates of the distribution of returns.}}{148}{figure.6.5}\protected@file@percent }
\newlabel{fig:smidistri}{{6.5}{148}{Comparison of different estimates of the distribution of returns}{figure.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}Test procedures}{149}{subsection.6.2.5}\protected@file@percent }
\newlabel{TestMLE}{{6.2.5}{149}{Test procedures}{subsection.6.2.5}{}}
\newlabel{prp:Walddistri}{{6.5}{149}{Asymptotic distribution of the Wald statistic}{proposition.6.5}{}}
\newlabel{eq:varinWald}{{6.17}{150}{Asymptotic distribution of the Wald statistic}{equation.6.2.17}{}}
\newlabel{prp:LMdistri}{{6.6}{150}{Asymptotic distribution of the LM test statistic}{proposition.6.6}{}}
\newlabel{eq:xiLM}{{6.18}{150}{Asymptotic distribution of the LM test statistic}{equation.6.2.18}{}}
\newlabel{def:LR}{{6.8}{151}{Likelihood Ratio test statistics}{definition.6.8}{}}
\newlabel{prp:equivLRLMW}{{6.7}{151}{Asymptotic equivalence of LR, LM, and Wald tests}{proposition.6.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Bayesian approach}{151}{section.6.3}\protected@file@percent }
\newlabel{bayesian-approach}{{6.3}{151}{Bayesian approach}{section.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Introduction}{151}{subsection.6.3.1}\protected@file@percent }
\newlabel{introduction}{{6.3.1}{151}{Introduction}{subsection.6.3.1}{}}
\newlabel{eq:post1}{{6.19}{152}{Introduction}{equation.6.3.19}{}}
\newlabel{eq:post2}{{6.21}{153}{Introduction}{equation.6.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Monte-Carlo Markov Chains}{153}{subsection.6.3.2}\protected@file@percent }
\newlabel{monte-carlo-markov-chains}{{6.3.2}{153}{Monte-Carlo Markov Chains}{subsection.6.3.2}{}}
\newlabel{def:MC}{{6.9}{153}{Markov Chain}{definition.6.9}{}}
\newlabel{eq:alphaXXX}{{6.22}{154}{Monte-Carlo Markov Chains}{equation.6.3.22}{}}
\newlabel{eq:symmQ}{{6.23}{154}{Monte-Carlo Markov Chains}{equation.6.3.23}{}}
\newlabel{eq:hypoQ}{{6.24}{154}{Monte-Carlo Markov Chains}{equation.6.3.24}{}}
\citation{Roberts_Gelman_Gilks_1997}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Example: AR(1) specification}{156}{subsection.6.3.3}\protected@file@percent }
\newlabel{example-ar1-specification}{{6.3.3}{156}{Example: AR(1) specification}{subsection.6.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces The upper line of plot compares prior (black) and posterior (red) distributions. The vertical dashed blue lines indicate the true values of the parameters. The second row of plots show the sequence of $\boldsymbol  \mittheta _i$'s generated by the MCMC algorithm. These sequences are the ones used to produce the posterior distributions (red lines) in the upper plots.}}{160}{figure.6.6}\protected@file@percent }
\newlabel{fig:MCMC8}{{6.6}{160}{The upper line of plot compares prior (black) and posterior (red) distributions. The vertical dashed blue lines indicate the true values of the parameters. The second row of plots show the sequence of $\boldsymbol \theta _i$'s generated by the MCMC algorithm. These sequences are the ones used to produce the posterior distributions (red lines) in the upper plots}{figure.6.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Microeconometrics}{161}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{microeconometrics}{{7}{161}{Microeconometrics}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Binary-choice models}{161}{section.7.1}\protected@file@percent }
\newlabel{binary-choice-models}{{7.1}{161}{Binary-choice models}{section.7.1}{}}
\newlabel{eq:binaryBenroulli}{{7.1}{162}{Binary-choice models}{equation.7.1.1}{}}
\newlabel{eq:genericBinary}{{7.2}{162}{Binary-choice models}{equation.7.1.2}{}}
\gdef \LT@i {\LT@entry 
    {1}{84.43176pt}\LT@entry 
    {1}{142.69913pt}\LT@entry 
    {1}{162.86911pt}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Fitting a binary variable with a linear model (Linear Probability Model, LPM). The model is $\symbb {P}(y_i=1|x_i)=\mitPhi (0.5+2x_i)$, where $\mitPhi $ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\symcal {N}(0,1)$.}}{163}{figure.7.1}\protected@file@percent }
\newlabel{fig:LPM}{{7.1}{163}{Fitting a binary variable with a linear model (Linear Probability Model, LPM). The model is $\mathbb {P}(y_i=1|x_i)=\Phi (0.5+2x_i)$, where $\Phi $ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\mathcal {N}(0,1)$}{figure.7.1}{}}
\newlabel{tab:foo}{{7.1}{163}{Binary-choice models}{table.7.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{ This table provides examples of function \(g\), s.t. \(\symbb {P}(y_i=1|\mathbf {x}_i;\boldsymbol  heta) = g(\boldsymbol  \mittheta '\mathbf {x}_i)\). The LPM case (last row) is given for comparison but, again, it does not satisfy \(g(\boldsymbol  \mittheta '\mathbf {x}_i) \in [0,1]\) for any value of \(\boldsymbol  \mittheta '\mathbf {x}_i\).}}{163}{table.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Probit, Logit, and Log-log functions.}}{163}{figure.7.2}\protected@file@percent }
\newlabel{fig:ProbLogit}{{7.2}{163}{Probit, Logit, and Log-log functions}{figure.7.2}{}}
\newlabel{eq:probit}{{7.3}{163}{Binary-choice models}{equation.7.1.3}{}}
\newlabel{eq:logit}{{7.4}{164}{Binary-choice models}{equation.7.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The model is $\symbb {P}(y_i=1|x_i)=\mitPhi (0.5+2x_i)$, where $\mitPhi $ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\symcal {N}(0,1)$. Crosses give the model-implied probabilities of having $y_i=1$ (conditional on $x_i$).}}{164}{figure.7.3}\protected@file@percent }
\newlabel{fig:LPM2}{{7.3}{164}{The model is $\mathbb {P}(y_i=1|x_i)=\Phi (0.5+2x_i)$, where $\Phi $ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\mathcal {N}(0,1)$. Crosses give the model-implied probabilities of having $y_i=1$ (conditional on $x_i$)}{figure.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Interpretation in terms of latent variable, and utility-based models}{164}{subsection.7.1.1}\protected@file@percent }
\newlabel{latent}{{7.1.1}{164}{Interpretation in terms of latent variable, and utility-based models}{subsection.7.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Distribution of $y_i^*$ conditional on $\mathbf {x}_i$.}}{165}{figure.7.4}\protected@file@percent }
\newlabel{fig:Latent}{{7.4}{165}{Distribution of $y_i^*$ conditional on $\mathbf {x}_i$}{figure.7.4}{}}
\citation{Nakosteen_Zimmer_1980}
\newlabel{eq:utility}{{7.5}{166}{Interpretation in terms of latent variable, and utility-based models}{equation.7.1.5}{}}
\newlabel{exm:migration}{{7.1}{166}{Migration and income}{example.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Alternative-Varying Regressors}{166}{subsection.7.1.2}\protected@file@percent }
\newlabel{Avregressors}{{7.1.2}{166}{Alternative-Varying Regressors}{subsection.7.1.2}{}}
\citation{Cameron_Trivedi_2005}
\newlabel{eq:utility2}{{7.6}{167}{Alternative-Varying Regressors}{equation.7.1.6}{}}
\newlabel{eq:utility3}{{7.7}{167}{Alternative-Varying Regressors}{equation.7.1.7}{}}
\newlabel{exm:FishingTable}{{7.2}{167}{Fishing-mode dataset}{example.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Estimation}{168}{subsection.7.1.3}\protected@file@percent }
\newlabel{estimation}{{7.1.3}{168}{Estimation}{subsection.7.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}Marginal effects}{169}{subsection.7.1.4}\protected@file@percent }
\newlabel{marginalFX}{{7.1.4}{169}{Marginal effects}{subsection.7.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.5}Goodness of fit}{170}{subsection.7.1.5}\protected@file@percent }
\newlabel{goodness-of-fit-1}{{7.1.5}{170}{Goodness of fit}{subsection.7.1.5}{}}
\newlabel{exm:creditProbit}{{7.3}{170}{Credit and defaults (Lending-club dataset)}{example.7.3}{}}
\citation{Cameron_Trivedi_2005}
\newlabel{exm:Fisch142}{{7.4}{174}{Replicating Table 14.2 of Cameron and Trivedi (2005)}{example.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.6}Predictions and ROC curves}{176}{subsection.7.1.6}\protected@file@percent }
\newlabel{predictions-and-roc-curves}{{7.1.6}{176}{Predictions and ROC curves}{subsection.7.1.6}{}}
\newlabel{exm:FishingROC}{{7.5}{176}{ROC with the fishing-mode dataset}{example.7.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Application of the ROC methodology on the fishing-mode dataset.}}{177}{figure.7.5}\protected@file@percent }
\newlabel{fig:fishing3}{{7.5}{177}{Application of the ROC methodology on the fishing-mode dataset}{figure.7.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Multiple Choice Models}{177}{section.7.2}\protected@file@percent }
\newlabel{multiple-choice-models}{{7.2}{177}{Multiple Choice Models}{section.7.2}{}}
\newlabel{eq:generalMultiNom}{{7.8}{178}{Multiple Choice Models}{equation.7.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Ordered case}{178}{subsection.7.2.1}\protected@file@percent }
\newlabel{ordered-case}{{7.2.1}{178}{Ordered case}{subsection.7.2.1}{}}
\newlabel{eq:Pordered}{{7.9}{178}{Ordered case}{equation.7.2.9}{}}
\newlabel{eq:multipleLogLik}{{7.10}{179}{Ordered case}{equation.7.2.10}{}}
\newlabel{exm:orderedCredit}{{7.6}{179}{Predicting credit ratings (Lending-club dataset)}{example.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}General multinomial logit model}{181}{subsection.7.2.2}\protected@file@percent }
\newlabel{MNL}{{7.2.2}{181}{General multinomial logit model}{subsection.7.2.2}{}}
\newlabel{eq:GeneralMNL}{{7.11}{181}{General multinomial logit model}{equation.7.2.11}{}}
\newlabel{eq:xorganiz}{{7.12}{181}{General multinomial logit model}{equation.7.2.12}{}}
\citation{Cameron_Trivedi_2005}
\citation{Cameron_Trivedi_2005}
\newlabel{eq:thetaOrganiz}{{7.13}{182}{General multinomial logit model}{equation.7.2.13}{}}
\newlabel{eq:thetaOrganizCL}{{7.14}{182}{General multinomial logit model}{equation.7.2.14}{}}
\newlabel{eq:thetaOrganizML}{{7.15}{182}{General multinomial logit model}{equation.7.2.15}{}}
\newlabel{eq:thetaOrganizCL}{{7.16}{182}{General multinomial logit model}{equation.7.2.16}{}}
\newlabel{exm:FishingGeneralLogit}{{7.7}{182}{CL and MNL with the fishing-mode dataset}{example.7.7}{}}
\newlabel{eq:multipleLogLik}{{7.17}{184}{General multinomial logit model}{equation.7.2.17}{}}
\newlabel{def:Gumbel}{{7.1}{185}{Gumbel distribution}{definition.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces C.d.f. of the Gumbel distribution ($F(x)=\exp (-\exp (-x))$).}}{186}{figure.7.6}\protected@file@percent }
\newlabel{fig:Gumbel}{{7.6}{186}{C.d.f. of the Gumbel distribution ($F(x)=\exp (-\exp (-x))$)}{figure.7.6}{}}
\newlabel{prp:Weibull}{{7.1}{186}{Weibull}{proposition.7.1}{}}
\newlabel{eq:condiProba}{{7.18}{187}{General multinomial logit model}{equation.7.2.18}{}}
\newlabel{exm:redbluebus}{{7.8}{188}{Red-blue bus and IIA}{example.7.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Nested logits}{188}{subsection.7.2.3}\protected@file@percent }
\newlabel{nested-logits}{{7.2.3}{188}{Nested logits}{subsection.7.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces GEV simulations.}}{189}{figure.7.7}\protected@file@percent }
\newlabel{fig:GEV}{{7.7}{189}{GEV simulations}{figure.7.7}{}}
\newlabel{eq:Nested}{{7.19}{190}{Nested logits}{equation.7.2.19}{}}
\citation{Hensher_Greene_2002}
\citation{Heiss_2002}
\newlabel{exm:nestedTravel}{{7.9}{191}{Travel-mode dataset}{example.7.9}{}}
\citation{Tobin_1956}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Tobit models}{193}{section.7.3}\protected@file@percent }
\newlabel{tobit}{{7.3}{193}{Tobit models}{section.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Bias in the case of sample selection. The grey line represents the population regression line. The model is $y_i = x_i + \mitvarepsilon _i$, with $\mitvarepsilon _{i,t} \sim \symcal {N}(0,1)$. The red line is the OLS regression line based on black dots only.}}{194}{figure.7.8}\protected@file@percent }
\newlabel{fig:tobit1}{{7.8}{194}{Bias in the case of sample selection. The grey line represents the population regression line. The model is $y_i = x_i + \varepsilon _i$, with $\varepsilon _{i,t} \sim \mathcal {N}(0,1)$. The red line is the OLS regression line based on black dots only}{figure.7.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Censored dataset with heteroskedasticitiy. The model is $y_i = x_i + \mitvarepsilon _i$, with $\mitvarepsilon _{i,t} \sim \symcal {N}(0,\mitsigma _i^2)$ where $\mitsigma _i = \exp (-1 + x_i)$.}}{196}{figure.7.9}\protected@file@percent }
\newlabel{fig:tobit2}{{7.9}{196}{Censored dataset with heteroskedasticitiy. The model is $y_i = x_i + \varepsilon _i$, with $\varepsilon _{i,t} \sim \mathcal {N}(0,\sigma _i^2)$ where $\sigma _i = \exp (-1 + x_i)$}{figure.7.9}{}}
\newlabel{eq:Econdtruncated}{{7.20}{197}{Tobit models}{equation.7.3.20}{}}
\citation{Cameron_Trivedi_2005}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Conditional means of $y$ in Tobit models. The model is $y_i = x_i + \mitvarepsilon _i$, with $\mitvarepsilon _i \sim \symcal {N}(0,1)$.}}{198}{figure.7.10}\protected@file@percent }
\newlabel{fig:tobit3}{{7.10}{198}{Conditional means of $y$ in Tobit models. The model is $y_i = x_i + \varepsilon _i$, with $\varepsilon _i \sim \mathcal {N}(0,1)$}{figure.7.10}{}}
\citation{Mroz_1987}
\newlabel{exm:WageMroz1}{{7.10}{199}{Wage prediction}{example.7.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Predicted versus observed wages.}}{201}{figure.7.11}\protected@file@percent }
\newlabel{fig:tobit5}{{7.11}{201}{Predicted versus observed wages}{figure.7.11}{}}
\citation{Duan_et_al_1983}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Sample Selection Models}{202}{section.7.4}\protected@file@percent }
\newlabel{SSM}{{7.4}{202}{Sample Selection Models}{section.7.4}{}}
\newlabel{eq:probaPP1}{{7.21}{203}{Sample Selection Models}{equation.7.4.21}{}}
\newlabel{eq:probaPP2}{{7.22}{203}{Sample Selection Models}{equation.7.4.21}{}}
\newlabel{eq:probaPP3}{{7.23}{203}{Sample Selection Models}{equation.7.4.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Probability of observing $y_2$ depending on its value, for different values of conditional correlation between $y_2$ and $y_1^*$.}}{204}{figure.7.12}\protected@file@percent }
\newlabel{fig:SampleSelec}{{7.12}{204}{Probability of observing $y_2$ depending on its value, for different values of conditional correlation between $y_2$ and $y_1^*$}{figure.7.12}{}}
\newlabel{eq:y2y11}{{7.24}{205}{Sample Selection Models}{equation.7.4.24}{}}
\citation{Cameron_Trivedi_2005}
\citation{Mroz_1987}
\newlabel{eq:OLSregHeckit}{{7.25}{206}{Sample Selection Models}{equation.7.4.25}{}}
\newlabel{exm:WageSample}{{7.11}{207}{Wage prediction}{example.7.11}{}}
\citation{Cameron_Trivedi_2005}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Models of Count Data}{209}{section.7.5}\protected@file@percent }
\newlabel{models-of-count-data}{{7.5}{209}{Models of Count Data}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Poisson model}{209}{subsection.7.5.1}\protected@file@percent }
\newlabel{poisson-model}{{7.5.1}{209}{Poisson model}{subsection.7.5.1}{}}
\newlabel{eq:FOCPoisson}{{7.26}{210}{Poisson model}{equation.7.5.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Negative binomial model}{211}{subsection.7.5.2}\protected@file@percent }
\newlabel{negative-binomial-model}{{7.5.2}{211}{Negative binomial model}{subsection.7.5.2}{}}
\citation{Cameron_Trivedi_2005}
\newlabel{exm:Doctorvisits}{{7.12}{212}{Number of doctor visits}{example.7.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Distribution of the number of doctor visits.}}{213}{figure.7.13}\protected@file@percent }
\newlabel{fig:countdata1}{{7.13}{213}{Distribution of the number of doctor visits}{figure.7.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Hurdle model}{215}{subsection.7.5.3}\protected@file@percent }
\newlabel{hurdle-model}{{7.5.3}{215}{Hurdle model}{subsection.7.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Zero-inflated model}{215}{subsection.7.5.4}\protected@file@percent }
\newlabel{zero-inflated-model}{{7.5.4}{215}{Zero-inflated model}{subsection.7.5.4}{}}
\newlabel{exm:Doctorvisits2}{{7.13}{216}{Number of doctor visits}{example.7.13}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Time Series}{221}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{TS}{{8}{221}{Time Series}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction to time series}{221}{section.8.1}\protected@file@percent }
\newlabel{introduction-to-time-series}{{8.1}{221}{Introduction to time series}{section.8.1}{}}
\newlabel{def:whitenoise}{{8.1}{221}{White noise}{definition.8.1}{}}
\newlabel{def:MDS}{{8.2}{221}{Martingale Difference Sequence}{definition.8.2}{}}
\newlabel{exm:ARCH}{{8.1}{222}{ARCH process}{example.8.1}{}}
\newlabel{exm:whiteNotMDS}{{8.2}{222}{}{example.8.2}{}}
\newlabel{eq:lagOp}{{8.1}{222}{Introduction to time series}{equation.8.1.1}{}}
\citation{JST_2017}
\newlabel{def:autocov}{{8.3}{223}{Autocovariance}{definition.8.3}{}}
\newlabel{def:covstat}{{8.4}{223}{Covariance stationarity}{definition.8.4}{}}
\newlabel{def:strictstat}{{8.5}{223}{Strict stationarity}{definition.8.5}{}}
\newlabel{prp:gammaMinus}{{8.1}{223}{}{proposition.8.1}{}}
\newlabel{def:autocor}{{8.6}{223}{Auto-correlation}{definition.8.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Example of a process that is not covariance stationary ($y_t = 0.1t + \mitvarepsilon _t$, where $\mitvarepsilon _t \sim \symcal {N}(0,1)$).}}{224}{figure.8.1}\protected@file@percent }
\newlabel{fig:nonstat1}{{8.1}{224}{Example of a process that is not covariance stationary ($y_t = 0.1t + \varepsilon _t$, where $\varepsilon _t \sim \mathcal {N}(0,1)$)}{figure.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99\% confidence interval of the standard normal distribution ($\pm 2.58$).}}{225}{figure.8.2}\protected@file@percent }
\newlabel{fig:nonstat2}{{8.2}{225}{Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99\% confidence interval of the standard normal distribution ($\pm 2.58$)}{figure.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database.}}{226}{figure.8.3}\protected@file@percent }
\newlabel{fig:autocov}{{8.3}{226}{Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database}{figure.8.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces For order $j$, the slope of the blue line is, approximately, $\hat {\mitgamma }_j/\widehat {\symbb {V}ar}(y_t)$, where hats indicate sample moments.}}{226}{figure.8.4}\protected@file@percent }
\newlabel{fig:autocov2}{{8.4}{226}{For order $j$, the slope of the blue line is, approximately, $\hat {\gamma }_j/\widehat {\mathbb {V}ar}(y_t)$, where hats indicate sample moments}{figure.8.4}{}}
\citation{Anderson_1971}
\newlabel{def:ergodicity}{{8.7}{227}{Mean ergodicity}{definition.8.7}{}}
\newlabel{def:ergod2nd}{{8.8}{227}{Second-moment ergodicity}{definition.8.8}{}}
\newlabel{thm:CLTcovstat}{{8.1}{227}{Central Limit Theorem for covariance-stationary processes}{theorem.8.1}{}}
\newlabel{eq:TCL20}{{8.2}{227}{Central Limit Theorem for covariance-stationary processes}{equation.8.1.2}{}}
\newlabel{eq:TCL2}{{8.3}{227}{Central Limit Theorem for covariance-stationary processes}{equation.8.1.2}{}}
\newlabel{eq:TCL4ts}{{8.4}{227}{Central Limit Theorem for covariance-stationary processes}{equation.8.1.2}{}}
\citation{Newey_West_1987}
\newlabel{def:LRV}{{8.9}{228}{Long-run variance}{definition.8.9}{}}
\newlabel{eq:covSmplMean}{{8.5}{228}{Introduction to time series}{equation.8.1.5}{}}
\newlabel{eq:NWest}{{8.6}{228}{Introduction to time series}{equation.8.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Univariate processes}{228}{section.8.2}\protected@file@percent }
\newlabel{univariate-processes}{{8.2}{228}{Univariate processes}{section.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Moving Average (MA) processes}{228}{subsection.8.2.1}\protected@file@percent }
\newlabel{moving-average-ma-processes}{{8.2.1}{228}{Moving Average (MA) processes}{subsection.8.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces The three samples have been simulated using the following data generating process: $x_t = \mitmu + \mitrho (x_{t-1}-\mitmu ) + \sqrt  {1-\mitrho ^2}\mitvarepsilon _t$, where $\mitvarepsilon _t \sim \symcal {N}(0,1)$. Case A: $\mitrho = 0$; Case B: $\mitrho = 0.7$; Case C: $\mitrho = 0.999$. In the three cases, $\symbb {E}(x_t)=\mitmu =2$ and $\symbb {V}ar(x_t)=1$.}}{229}{figure.8.5}\protected@file@percent }
\newlabel{fig:TVTCL}{{8.5}{229}{The three samples have been simulated using the following data generating process: $x_t = \mu + \rho (x_{t-1}-\mu ) + \sqrt {1-\rho ^2}\varepsilon _t$, where $\varepsilon _t \sim \mathcal {N}(0,1)$. Case A: $\rho = 0$; Case B: $\rho = 0.7$; Case C: $\rho = 0.999$. In the three cases, $\mathbb {E}(x_t)=\mu =2$ and $\mathbb {V}ar(x_t)=1$}{figure.8.5}{}}
\newlabel{def:MAq}{{8.11}{230}{MA(q) process}{definition.8.11}{}}
\newlabel{prp:covMAq}{{8.2}{231}{Covariance-stationarity of an MA(q) process}{proposition.8.2}{}}
\newlabel{eq:autocovMA}{{8.7}{231}{Covariance-stationarity of an MA(q) process}{equation.8.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Simulation of MA processes.}}{232}{figure.8.6}\protected@file@percent }
\newlabel{fig:simMA}{{8.6}{232}{Simulation of MA processes}{figure.8.6}{}}
\newlabel{def:summability}{{8.12}{232}{Absolute and square summability}{definition.8.12}{}}
\newlabel{thm:infMA}{{8.2}{232}{Existence condition for an infinite MA process}{theorem.8.2}{}}
\newlabel{prp:momentsMAinf}{{8.3}{233}{First two moments of an infinite MA process}{proposition.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Auto-Regressive (AR) processes}{233}{subsection.8.2.2}\protected@file@percent }
\newlabel{ARsection}{{8.2.2}{233}{Auto-Regressive (AR) processes}{subsection.8.2.2}{}}
\newlabel{def:AR1}{{8.13}{233}{First-order AR process (AR(1))}{definition.8.13}{}}
\newlabel{prp:statioAR1}{{8.4}{234}{Covariance-stationarity of an AR(1) process}{proposition.8.4}{}}
\newlabel{def:ARp}{{8.14}{234}{AR(p) process}{definition.8.14}{}}
\newlabel{eq:AR}{{8.8}{234}{AR(p) process}{equation.8.2.8}{}}
\newlabel{eq:F}{{8.9}{235}{Auto-Regressive (AR) processes}{equation.8.2.9}{}}
\newlabel{prp:Feigen}{{8.5}{235}{The eigenvalues of matrix F}{proposition.8.5}{}}
\newlabel{eq:Feigen}{{8.10}{235}{The eigenvalues of matrix F}{equation.8.2.10}{}}
\newlabel{prp:stability}{{8.6}{235}{Covariance-stationarity of an AR(p) process}{proposition.8.6}{}}
\newlabel{eq:outside}{{8.11}{235}{Covariance-stationarity of an AR(p) process}{equation.8.2.11}{}}
\newlabel{eq:inside}{{8.12}{235}{Covariance-stationarity of an AR(p) process}{equation.8.2.12}{}}
\newlabel{eq:EAR}{{8.14}{236}{Auto-Regressive (AR) processes}{equation.8.2.14}{}}
\newlabel{eq:gammas}{{8.15}{237}{Auto-Regressive (AR) processes}{equation.8.2.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}AR-MA processes}{238}{subsection.8.2.3}\protected@file@percent }
\newlabel{ar-ma-processes}{{8.2.3}{238}{AR-MA processes}{subsection.8.2.3}{}}
\newlabel{def:ARMApq}{{8.15}{238}{ARMA(p,q) process}{definition.8.15}{}}
\newlabel{eq:ARMApq}{{8.16}{238}{ARMA(p,q) process}{equation.8.2.16}{}}
\newlabel{prp:statioARMApq}{{8.7}{238}{Stationarity of an ARMA(p,q) process}{proposition.8.7}{}}
\newlabel{eq:ARMAwold}{{8.17}{238}{AR-MA processes}{equation.8.2.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}PACF approach to identify AR/MA processes}{239}{subsection.8.2.4}\protected@file@percent }
\newlabel{PACFapproach}{{8.2.4}{239}{PACF approach to identify AR/MA processes}{subsection.8.2.4}{}}
\newlabel{def:partialAC}{{8.16}{239}{Partial auto-correlation}{definition.8.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces ACF/PACF analysis of two processes (MA process on the left, AR on the right).}}{240}{figure.8.7}\protected@file@percent }
\newlabel{fig:pacf}{{8.7}{240}{ACF/PACF analysis of two processes (MA process on the left, AR on the right)}{figure.8.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.5}Wold decomposition}{240}{subsection.8.2.5}\protected@file@percent }
\newlabel{wold-decomposition}{{8.2.5}{240}{Wold decomposition}{subsection.8.2.5}{}}
\newlabel{thm:Wold}{{8.3}{240}{Wold decomposition}{theorem.8.3}{}}
\citation{Anderson_1971}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.6}Impulse Response Functions (IRFs) in ARMA models}{241}{subsection.8.2.6}\protected@file@percent }
\newlabel{impulse-response-functions-irfs-in-arma-models}{{8.2.6}{241}{Impulse Response Functions (IRFs) in ARMA models}{subsection.8.2.6}{}}
\newlabel{prp:computPsi}{{8.8}{242}{IRF of an ARMA(p,q) process}{proposition.8.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces IRFs associated with the three processes. Process 1 (MA(2)): $y_t = \mitvarepsilon _t + \mitvarepsilon _{t-1} + \mitvarepsilon _{t-2}$. Process 2 (ARMA(1,1)): $y_{t}=0.6y_{t-1} + \mitvarepsilon _t + 0.5\mitvarepsilon _{t-1}$. Process 3 (ARMA(4,2)): $y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \mitvarepsilon _t + \mitvarepsilon _{t-1} + \mitvarepsilon _{t-2}$.}}{244}{figure.8.8}\protected@file@percent }
\newlabel{fig:IRFarma}{{8.8}{244}{IRFs associated with the three processes. Process 1 (MA(2)): $y_t = \varepsilon _t + \varepsilon _{t-1} + \varepsilon _{t-2}$. Process 2 (ARMA(1,1)): $y_{t}=0.6y_{t-1} + \varepsilon _t + 0.5\varepsilon _{t-1}$. Process 3 (ARMA(4,2)): $y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \varepsilon _t + \varepsilon _{t-1} + \varepsilon _{t-2}$}{figure.8.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces (P)ACF analysis of Swiss GDP growth.}}{245}{figure.8.9}\protected@file@percent }
\newlabel{fig:IRFgdp1}{{8.9}{245}{(P)ACF analysis of Swiss GDP growth}{figure.8.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Dynamic response of Swiss annual growth to a shock on the innovation $\mitvarepsilon _t$ at date $t=0$. The solid line corresponds to an AR(1) specification; the dashed line corresponds to a MA(2) specification.}}{246}{figure.8.10}\protected@file@percent }
\newlabel{fig:IRFgdp2}{{8.10}{246}{Dynamic response of Swiss annual growth to a shock on the innovation $\varepsilon _t$ at date $t=0$. The solid line corresponds to an AR(1) specification; the dashed line corresponds to a MA(2) specification}{figure.8.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.7}ARMA processes with exogenous variables (ARMA-X)}{247}{subsection.8.2.7}\protected@file@percent }
\newlabel{ARMAIRF}{{8.2.7}{247}{ARMA processes with exogenous variables (ARMA-X)}{subsection.8.2.7}{}}
\newlabel{def:exogeneity}{{8.17}{247}{Exogeneity}{definition.8.17}{}}
\newlabel{def:ARMAX}{{8.18}{247}{ARMAX(p,q,r) model}{definition.8.18}{}}
\newlabel{eq:DLM}{{8.18}{247}{ARMAX(p,q,r) model}{equation.8.2.18}{}}
\citation{Stock_Watson_2003}
\newlabel{eq:dynmultX}{{8.19}{248}{ARMA processes with exogenous variables (ARMA-X)}{equation.8.2.19}{}}
\newlabel{prp:computPsiARMAX}{{8.9}{248}{Dynamic multipliers in ARMAX models}{proposition.8.9}{}}
\newlabel{eq:dynmultX}{{8.20}{248}{Dynamic multipliers in ARMAX models}{equation.8.2.20}{}}
\newlabel{exm:OrangeJuice}{{8.3}{249}{Influence of the number of freezing days on the price of orange juice}{example.8.3}{}}
\citation{Gertler_Karadi_2015}
\citation{Ramey_2016_NBER}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces Response of changes in orange juice price (in percent) to the number of freezing days. The solid (respectively dashed) line corresponds to the ARMAX(0,0,12) (resp. ARMAX(3,0,1)) model. The first model is estimated by OLS (see above), the second by MLE.}}{253}{figure.8.11}\protected@file@percent }
\newlabel{fig:freez4}{{8.11}{253}{Response of changes in orange juice price (in percent) to the number of freezing days. The solid (respectively dashed) line corresponds to the ARMAX(0,0,12) (resp. ARMAX(3,0,1)) model. The first model is estimated by OLS (see above), the second by MLE}{figure.8.11}{}}
\newlabel{exm:Ramey1}{{8.4}{253}{Real effect of a monetary policy shock}{example.8.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces The blue line corresponds to monetary-policy shocks identified by means of the Gertler and Karadi (2015)'s approach (high-frequency change in Euro-dollar futures). The black slid line is the year-on-year growth rate of industrial production.}}{254}{figure.8.12}\protected@file@percent }
\newlabel{fig:Ramey1fig}{{8.12}{254}{The blue line corresponds to monetary-policy shocks identified by means of the Gertler and Karadi (2015)'s approach (high-frequency change in Euro-dollar futures). The black slid line is the year-on-year growth rate of industrial production}{figure.8.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.8}Maximum Likelihood Estimation of ARMA processes}{255}{subsection.8.2.8}\protected@file@percent }
\newlabel{estimARMA}{{8.2.8}{255}{Maximum Likelihood Estimation of ARMA processes}{subsection.8.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces Response of industrial-production growth to monetary-policy shocks. Dashed lines correpsond to the $\pm $ 2-standard-deviation bands.}}{256}{figure.8.13}\protected@file@percent }
\newlabel{fig:Ramey3}{{8.13}{256}{Response of industrial-production growth to monetary-policy shocks. Dashed lines correpsond to the $\pm $ 2-standard-deviation bands}{figure.8.13}{}}
\newlabel{def:Markov}{{8.19}{256}{Markovian process}{definition.8.19}{}}
\newlabel{eq:recursMLE}{{8.21}{257}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.21}{}}
\newlabel{eq:Lstar}{{8.23}{258}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.23}{}}
\newlabel{eq:AROLSmean}{{8.25}{258}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.25}{}}
\newlabel{eq:AROLSsigma}{{8.26}{258}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.25}{}}
\newlabel{eq:OLSregARp}{{8.27}{259}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.27}{}}
\newlabel{eq:olsar1}{{8.28}{259}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.28}{}}
\newlabel{prp:cgceOLSARp}{{8.10}{259}{Large-sample porperties of the OLS estimator of AR(p) models}{proposition.8.10}{}}
\citation{Gourieroux_Monfort_Renne_2017}
\newlabel{eq:Qols}{{8.29}{260}{Large-sample porperties of the OLS estimator of AR(p) models}{equation.8.2.29}{}}
\newlabel{eq:MALstar}{{8.30}{261}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.30}{}}
\newlabel{eq:estimMAq}{{8.31}{262}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.31}{}}
\newlabel{eq:invertible}{{8.32}{262}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.32}{}}
\newlabel{eq:condiVarepsiMABB}{{8.33}{262}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.33}{}}
\newlabel{eq:condiVarepsiMA}{{8.34}{262}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.34}{}}
\newlabel{eq:recvareps}{{8.35}{263}{Maximum Likelihood Estimation of ARMA processes}{equation.8.2.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.9}Specification choice}{263}{subsection.8.2.9}\protected@file@percent }
\newlabel{specification-choice}{{8.2.9}{263}{Specification choice}{subsection.8.2.9}{}}
\newlabel{def:infocriteria}{{8.20}{264}{Information Criteria}{definition.8.20}{}}
\newlabel{prp:infocriteria}{{8.11}{264}{Consistency of the criteria-based lag selection}{proposition.8.11}{}}
\citation{JST_2017}
\newlabel{exm:ICOLS}{{8.5}{265}{Linear regression}{example.8.5}{}}
\citation{Sims_1980}
\citation{Stock_Watson_2016}
\citation{Ramey_2016_NBER}
\newlabel{exm:SwissGrowthAIC}{{8.6}{266}{Swiss GDP growth}{example.8.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Multivariate models}{266}{section.8.3}\protected@file@percent }
\newlabel{VAR}{{8.3}{266}{Multivariate models}{section.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Definition of VARs (and SVARMA) models}{267}{subsection.8.3.1}\protected@file@percent }
\newlabel{definition-of-vars-and-svarma-models}{{8.3.1}{267}{Definition of VARs (and SVARMA) models}{subsection.8.3.1}{}}
\newlabel{def:SVAR}{{8.21}{267}{(S)VAR model}{definition.8.21}{}}
\newlabel{eq:yVAR}{{8.36}{267}{(S)VAR model}{equation.8.3.36}{}}
\newlabel{def:SVARMA}{{8.22}{267}{(S)VARMA model}{definition.8.22}{}}
\newlabel{eq:yVARMA}{{8.37}{267}{(S)VARMA model}{equation.8.3.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}IRFs in SVARMA}{268}{subsection.8.3.2}\protected@file@percent }
\newlabel{IRFSVARMA}{{8.3.2}{268}{IRFs in SVARMA}{subsection.8.3.2}{}}
\newlabel{eq:InfMA}{{8.38}{268}{IRFs in SVARMA}{equation.8.3.38}{}}
\newlabel{exm:IRFVARMA}{{8.7}{269}{IRFs of an SVARMA model}{example.8.7}{}}
\newlabel{eq:VARMA111}{{8.39}{269}{IRFs of an SVARMA model}{equation.8.3.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Covariance-stationary VARMA models}{270}{subsection.8.3.3}\protected@file@percent }
\newlabel{covariance-stationary-varma-models}{{8.3.3}{270}{Covariance-stationary VARMA models}{subsection.8.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces Impulse response functions}}{271}{figure.8.14}\protected@file@percent }
\newlabel{fig:simVAR}{{8.14}{271}{Impulse response functions}{figure.8.14}{}}
\newlabel{eq:condiInfiniteMA}{{8.40}{271}{Covariance-stationary VARMA models}{equation.8.3.40}{}}
\newlabel{eq:VARMA2}{{8.41}{272}{Covariance-stationary VARMA models}{equation.8.3.41}{}}
\newlabel{eq:matrixPHI}{{8.42}{272}{Covariance-stationary VARMA models}{equation.8.3.42}{}}
\citation{Gourieroux_Monfort_Renne_2020}
\citation{Hamilton_1994}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}VAR estimation}{274}{subsection.8.3.4}\protected@file@percent }
\newlabel{estimVAR}{{8.3.4}{274}{VAR estimation}{subsection.8.3.4}{}}
\newlabel{eq:PIVAR}{{8.43}{274}{VAR estimation}{equation.8.3.43}{}}
\newlabel{prp:estimVARGaussian}{{8.12}{274}{MLE of a Gaussian VAR}{proposition.8.12}{}}
\newlabel{eq:Pi}{{8.44}{274}{MLE of a Gaussian VAR}{equation.8.3.44}{}}
\newlabel{eq:betayx}{{8.45}{275}{MLE of a Gaussian VAR}{equation.8.3.45}{}}
\newlabel{eq:olsar1}{{8.47}{275}{VAR estimation}{equation.8.3.47}{}}
\newlabel{prp:OLSVAR}{{8.13}{275}{Asymptotic distribution of the OLS estimate of $\beta _i$}{proposition.8.13}{}}
\citation{Hamilton_1994}
\newlabel{eq:Qols}{{8.48}{276}{Asymptotic distribution of the OLS estimate of $\beta _i$}{equation.8.3.48}{}}
\newlabel{prp:OLSVAR2}{{8.14}{276}{Asymptotic distribution of the OLS estimates}{proposition.8.14}{}}
\newlabel{eq:asymptPi}{{8.49}{276}{Asymptotic distribution of the OLS estimates}{equation.8.3.49}{}}
\newlabel{eq:optimzedLogL}{{8.50}{277}{VAR estimation}{equation.8.3.50}{}}
\citation{Granger_1969}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Block exogeneity and Granger causality}{278}{subsection.8.3.5}\protected@file@percent }
\newlabel{BlockGranger}{{8.3.5}{278}{Block exogeneity and Granger causality}{subsection.8.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.6}Identification problem and standard identification techniques}{279}{subsection.8.3.6}\protected@file@percent }
\newlabel{identification-problem-and-standard-identification-techniques}{{8.3.6}{279}{Identification problem and standard identification techniques}{subsection.8.3.6}{}}
\citation{BERNANKE198649}
\citation{Sims_1986}
\citation{Gali_1992}
\citation{RubioRamirez_et_al_2010}
\citation{Blanchard_Quah_1989}
\citation{Faust_Leeper_1997}
\citation{Gali_1999}
\citation{Erceg_et_al_2005}
\citation{NBERc11177}
\citation{Gerlach_Smets_1995}
\newlabel{eq:systemI}{{8.51}{280}{Identification problem and standard identification techniques}{equation.8.3.51}{}}
\newlabel{eq:covU}{{8.52}{280}{Identification problem and standard identification techniques}{equation.8.3.52}{}}
\citation{DEDOLA20051543}
\newlabel{eq:BBBB}{{8.53}{281}{Identification problem and standard identification techniques}{equation.8.3.53}{}}
\citation{Christiano_Eichenbaum_Evans_1996}
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Response to a monetary-policy shock. Identification approach of Christiano, Eichenbaum and Evans (1996). Confidence intervals are obtained by boostrapping the estimated VAR model (see inference section).}}{283}{figure.8.15}\protected@file@percent }
\newlabel{fig:CEE}{{8.15}{283}{Response to a monetary-policy shock. Identification approach of Christiano, Eichenbaum and Evans (1996). Confidence intervals are obtained by boostrapping the estimated VAR model (see inference section)}{figure.8.15}{}}
\newlabel{eq:ystarVAR}{{8.54}{284}{Identification problem and standard identification techniques}{equation.8.3.54}{}}
\newlabel{eq:cumul}{{8.55}{284}{Identification problem and standard identification techniques}{equation.8.3.55}{}}
\citation{Blanchard_Quah_1989}
\citation{Fischer_1977}
\citation{Blanchard_Quah_1989}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.7}Sign restrictions}{287}{subsection.8.3.7}\protected@file@percent }
\newlabel{Signs}{{8.3.7}{287}{Sign restrictions}{subsection.8.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces IRF of GDP and unemployment to demand and supply shocks.}}{288}{figure.8.16}\protected@file@percent }
\newlabel{fig:BQ4}{{8.16}{288}{IRF of GDP and unemployment to demand and supply shocks}{figure.8.16}{}}
\citation{Arias_et_al_2018}
\newlabel{def:orthogonal}{{8.23}{289}{Orthogonal matrix}{definition.8.23}{}}
\citation{Uhlig_2005}
\citation{Uhlig_2005}
\citation{Uhlig_2005}
\citation{Danne_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces IRF associated with a monetary policy shock; sign-restriction approach.}}{292}{figure.8.17}\protected@file@percent }
\newlabel{fig:signrestr1}{{8.17}{292}{IRF associated with a monetary policy shock; sign-restriction approach}{figure.8.17}{}}
\citation{Uhlig_2004}
\citation{BARSKY2011273}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.8}Forecast error variance maximization}{293}{subsection.8.3.8}\protected@file@percent }
\newlabel{forecast-error-variance-maximization}{{8.3.8}{293}{Forecast error variance maximization}{subsection.8.3.8}{}}
\citation{Rigobon_2003}
\citation{NORMANDIN20041217}
\citation{Lanne_Lutkepohl_2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.9}Identification based on non-normality of the shocks}{296}{subsection.8.3.9}\protected@file@percent }
\newlabel{NonGaussian}{{8.3.9}{296}{Identification based on non-normality of the shocks}{subsection.8.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces This figure compares the distributions of two Gaussian bivariate vectors, $B \miteta _t$ and $BQ\miteta _t$, where $\miteta _{t} \sim \symcal {N}(0,Id)$ (therefore $\miteta _{1,t}$ and $\miteta _{2,t}$ are independent), and $Q$ is an orthogonal matrix.}}{297}{figure.8.18}\protected@file@percent }
\newlabel{fig:preMadeFigureICA}{{8.18}{297}{This figure compares the distributions of two Gaussian bivariate vectors, $B \eta _t$ and $BQ\eta _t$, where $\eta _{t} \sim \mathcal {N}(0,Id)$ (therefore $\eta _{1,t}$ and $\eta _{2,t}$ are independent), and $Q$ is an orthogonal matrix}{figure.8.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.19}{\ignorespaces This figure shows that the impulse response functions associated with an impulse matrix equal to $B$ (black line) or $BQ$ (red line) are different (even if $BB'=BQ(BQ)'$).}}{298}{figure.8.19}\protected@file@percent }
\newlabel{fig:preMadeFigureICA2}{{8.19}{298}{This figure shows that the impulse response functions associated with an impulse matrix equal to $B$ (black line) or $BQ$ (red line) are different (even if $BB'=BQ(BQ)'$)}{figure.8.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.20}{\ignorespaces This figure compares the distributions of two Gaussian bivariate vectors, $B \miteta _t$ and $BQ\miteta _t$, where $\miteta _t{1,t} \sim \symcal {N}(0,1)$, $\miteta _t{2,t} \sim t(5)$, and $Q$ is an orthogonal matrix.}}{298}{figure.8.20}\protected@file@percent }
\newlabel{fig:preMadeFigureICAGaussianStudent}{{8.20}{298}{This figure compares the distributions of two Gaussian bivariate vectors, $B \eta _t$ and $BQ\eta _t$, where $\eta _t{1,t} \sim \mathcal {N}(0,1)$, $\eta _t{2,t} \sim t(5)$, and $Q$ is an orthogonal matrix}{figure.8.20}{}}
\citation{Gourieroux_Monfort_Renne_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {8.21}{\ignorespaces The three plots represent the bivariate distributions of $\miteta _t$ (black) and of $B\miteta _t$ (red), where the two components of $\miteta _t$ are independent, of unit variance, and $B$ is orthogonal. Hence, for each of the three plots, $\symbb {V}ar(B\miteta _t)=Id$.}}{299}{figure.8.21}\protected@file@percent }
\newlabel{fig:ThreePlots}{{8.21}{299}{The three plots represent the bivariate distributions of $\eta _t$ (black) and of $B\eta _t$ (red), where the two components of $\eta _t$ are independent, of unit variance, and $B$ is orthogonal. Hence, for each of the three plots, $\mathbb {V}ar(B\eta _t)=Id$}{figure.8.21}{}}
\newlabel{hyp:NonGauss}{{8.1}{299}{}{hypothesis.8.1}{}}
\newlabel{thm:EK2004}{{8.4}{299}{Eriksson, Koivunen (2004)}{theorem.8.4}{}}
\citation{Gourieroux_Monfort_Renne_2017}
\newlabel{eq:pseudolog}{{8.57}{300}{Identification based on non-normality of the shocks}{equation.8.3.57}{}}
\newlabel{eq:optimprob}{{8.58}{300}{Identification based on non-normality of the shocks}{equation.8.3.58}{}}
\newlabel{eq:optimprob2}{{8.61}{300}{Identification based on non-normality of the shocks}{equation.8.3.61}{}}
\gdef \LT@ii {\LT@entry 
    {1}{36.12479pt}\LT@entry 
    {1}{121.9498pt}\LT@entry 
    {1}{100.88892pt}\LT@entry 
    {1}{131.0311pt}}
\citation{Gourieroux_Monfort_Renne_2017}
\citation{Gourieroux_Monfort_Renne_2017}
\newlabel{tab:distriICA}{{8.1}{301}{Identification based on non-normality of the shocks}{table.8.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{ This table reports usual p.d.f. and their derivatives.}}{301}{table.8.1}\protected@file@percent }
\newlabel{exm:GMR2017}{{8.8}{301}{Non-Gaussian monetary-policy shocks}{example.8.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.22}{\ignorespaces The first row of plots shows the responses of the three endogenous variables to the monetary policy shock in the context of a Cholesky-idendtified SVAR (ordering: inflation, output gap, interest rate). The next three rows of plots show the repsonses of the endogenous variables to the three structural shocks identified by ICA. The last one (Shock 3) is close to the Cholesky-identified monetary policy shock.}}{306}{figure.8.22}\protected@file@percent }
\newlabel{fig:ICAFigIRF}{{8.22}{306}{The first row of plots shows the responses of the three endogenous variables to the monetary policy shock in the context of a Cholesky-idendtified SVAR (ordering: inflation, output gap, interest rate). The next three rows of plots show the repsonses of the endogenous variables to the three structural shocks identified by ICA. The last one (Shock 3) is close to the Cholesky-identified monetary policy shock}{figure.8.22}{}}
\citation{Rigobon_2003}
\citation{LANNE2010121}
\citation{LANNE2010121}
\citation{LUTKEPOHL20172}
\citation{Bernanke_Boivin_Eliasz_2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.10}Factor-Augmented VAR (FAVAR)}{307}{subsection.8.3.10}\protected@file@percent }
\newlabel{factor-augmented-var-favar}{{8.3.10}{307}{Factor-Augmented VAR (FAVAR)}{subsection.8.3.10}{}}
\citation{Bernanke_Boivin_Eliasz_2005}
\citation{Bernanke_Boivin_Eliasz_2005}
\citation{Bernanke_Boivin_Eliasz_2005}
\citation{McCracken_Ng_2016}
\newlabel{eq:FAVAR}{{8.62}{308}{Factor-Augmented VAR (FAVAR)}{equation.8.3.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.23}{\ignorespaces Responses of a monetary-policy shock. FAVAR approach of Bernanke, Boivin, and Eliasz (2005). FRED-MD dataset.}}{310}{figure.8.23}\protected@file@percent }
\newlabel{fig:FAVAR}{{8.23}{310}{Responses of a monetary-policy shock. FAVAR approach of Bernanke, Boivin, and Eliasz (2005). FRED-MD dataset}{figure.8.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.11}Projection Methods}{310}{subsection.8.3.11}\protected@file@percent }
\newlabel{Projections}{{8.3.11}{310}{Projection Methods}{subsection.8.3.11}{}}
\citation{Jorda_2005}
\citation{Jorda_2005}
\newlabel{eq:OLS1}{{8.63}{311}{Projection Methods}{equation.8.3.63}{}}
\newlabel{eq:xetaw}{{8.64}{311}{Projection Methods}{equation.8.3.64}{}}
\citation{Marcellino_et_al_2006}
\citation{Ramey_2016_NBER}
\newlabel{eq:IV1}{{8.66}{312}{Projection Methods}{equation.8.3.66}{}}
\citation{Gurkaynak_et_al_2005}
\citation{KUTTNER2001523}
\citation{Cochrane_Piazzesi_2002}
\citation{Gurkaynak_et_al_2005}
\citation{Piazzesi_Swanson_2008}
\citation{Gertler_Karadi_2015}
\citation{Romer_Romer_2004}
\newlabel{exm:HighFreq}{{8.9}{313}{Identification of Monetary-Policy Shocks Based on High-Frequency Data}{example.8.9}{}}
\newlabel{exm:RomerRomer}{{8.10}{313}{Identification of Monetary-Policy Shocks Based on the Narrative Approach}{example.8.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.24}{\ignorespaces Source: Gurkaynak, Sack and Swanson (2005). Transaction rates of Federal funds futures on June 25, 2003, day on which a regularly scheduled FOMC meeting was scheduled. At 2:15 p.m., the FOMC announced that it was lowering its target for the federal funds rate from 1.25\% to 1\%, while many market participants were expecting a 50 bp cut. This shows that (i) financial markets seem to fully adjust to the policy action within just a few minutes and (ii) the federal funds rate surprise is not necessarily in the same direction as the federal funds rate action itself.}}{314}{figure.8.24}\protected@file@percent }
\newlabel{fig:HighFreq}{{8.24}{314}{Source: Gurkaynak, Sack and Swanson (2005). Transaction rates of Federal funds futures on June 25, 2003, day on which a regularly scheduled FOMC meeting was scheduled. At 2:15 p.m., the FOMC announced that it was lowering its target for the federal funds rate from 1.25\% to 1\%, while many market participants were expecting a 50 bp cut. This shows that (i) financial markets seem to fully adjust to the policy action within just a few minutes and (ii) the federal funds rate surprise is not necessarily in the same direction as the federal funds rate action itself}{figure.8.24}{}}
\citation{Stock_Watson_2018}
\citation{Stock_Watson_2018}
\citation{Stock_Watson_2018}
\@writefile{lof}{\contentsline {figure}{\numberline {8.25}{\ignorespaces Gertler-Karadi monthly shocks, fed funds futures 3 months.}}{317}{figure.8.25}\protected@file@percent }
\newlabel{fig:essaiIV0}{{8.25}{317}{Gertler-Karadi monthly shocks, fed funds futures 3 months}{figure.8.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.26}{\ignorespaces Reponses to a monetary-policy shock, SVAR-IV approach.}}{318}{figure.8.26}\protected@file@percent }
\newlabel{fig:essaiIV1}{{8.26}{318}{Reponses to a monetary-policy shock, SVAR-IV approach}{figure.8.26}{}}
\newlabel{eq:regIV1}{{8.68}{319}{Projection Methods}{equation.8.3.68}{}}
\citation{Gertler_Karadi_2015}
\citation{Stock_Watson_2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.12}Inference}{320}{subsection.8.3.12}\protected@file@percent }
\newlabel{Inference}{{8.3.12}{320}{Inference}{subsection.8.3.12}{}}
\citation{Hamilton_1994}
\citation{Lutkepohl_1990}
\citation{Lutkepohl_1990}
\citation{Kilian_1998}
\citation{Kilian_1998}
\citation{Kilian_1998}
\citation{VARetp}
\citation{Kilian_1998}
\citation{DEGOOIJER2006443}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Forecasting}{325}{section.8.4}\protected@file@percent }
\newlabel{forecasting}{{8.4}{325}{Forecasting}{section.8.4}{}}
\newlabel{prp:smallestMSE}{{8.15}{326}{Smallest MSE}{proposition.8.15}{}}
\newlabel{prp:smallestMSElinear}{{8.16}{326}{}{proposition.8.16}{}}
\newlabel{eq:proj}{{8.69}{326}{}{equation.8.4.69}{}}
\newlabel{eq:linproj}{{8.70}{327}{Forecasting}{equation.8.4.70}{}}
\newlabel{exm:fcstMAq}{{8.11}{327}{Forecasting an MA(q) process}{example.8.11}{}}
\newlabel{exm:fcstARp}{{8.12}{328}{Forecasting an AR(p) process}{example.8.12}{}}
\citation{boxjen76}
\newlabel{exm:fcstARMApq}{{8.13}{329}{Forecasting an ARMA(p,q) process}{example.8.13}{}}
\newlabel{eq:armaForecast}{{8.71}{329}{Forecasting an ARMA(p,q) process}{equation.8.4.71}{}}
\citation{Diebold_Mariano_1995}
\citation{JST_2017}
\newlabel{exm:SwissOutOfSample}{{8.14}{331}{Forecasting Swiss GDP growth}{example.8.14}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Appendix}{335}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{append}{{9}{335}{Appendix}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Principal component analysis (PCA)}{335}{section.9.1}\protected@file@percent }
\newlabel{PCAapp}{{9.1}{335}{Principal component analysis (PCA)}{section.9.1}{}}
\newlabel{eq:PCA11}{{9.1}{335}{Principal component analysis (PCA)}{equation.9.1.1}{}}
\citation{Litterman_Scheinkman_1991}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Linear algebra: definitions and results}{338}{section.9.2}\protected@file@percent }
\newlabel{LinAlgebra}{{9.2}{338}{Linear algebra: definitions and results}{section.9.2}{}}
\newlabel{def:determinant}{{9.1}{338}{Eigenvalues}{definition.9.1}{}}
\newlabel{prp:determinant}{{9.1}{338}{Properties of the determinant}{proposition.9.1}{}}
\newlabel{def:MoorPenrose}{{9.2}{338}{Moore-Penrose inverse}{definition.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities.}}{339}{figure.9.1}\protected@file@percent }
\newlabel{fig:USydsPCA1}{{9.1}{339}{Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities}{figure.9.1}{}}
\newlabel{prp:MoorPenrose}{{9.2}{340}{Properties of the Moore-Penrose inverse}{proposition.9.2}{}}
\newlabel{def:idempotent}{{9.3}{340}{Idempotent matrix}{definition.9.3}{}}
\newlabel{prp:rootsidempotent}{{9.3}{340}{Roots of an idempotent matrix}{proposition.9.3}{}}
\newlabel{prp:chi2idempotent}{{9.4}{340}{Idempotent matrix and chi-square distribution}{proposition.9.4}{}}
\newlabel{prp:constrainedLS}{{9.5}{340}{Constrained least squares}{proposition.9.5}{}}
\newlabel{prp:inversepartitioned}{{9.6}{341}{Inverse of a partitioned matrix}{proposition.9.6}{}}
\newlabel{def:FOD}{{9.4}{341}{Matrix derivatives}{definition.9.4}{}}
\newlabel{prp:partial}{{9.7}{341}{}{proposition.9.7}{}}
\newlabel{prp:absMs}{{9.8}{341}{Square and absolute summability}{proposition.9.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Statistical analysis: definitions and results}{342}{section.9.3}\protected@file@percent }
\newlabel{variousResults}{{9.3}{342}{Statistical analysis: definitions and results}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Moments and statistics}{342}{subsection.9.3.1}\protected@file@percent }
\newlabel{moments-and-statistics}{{9.3.1}{342}{Moments and statistics}{subsection.9.3.1}{}}
\newlabel{def:partialcorrel}{{9.5}{342}{Partial correlation}{definition.9.5}{}}
\newlabel{eq:pc}{{9.2}{342}{Partial correlation}{equation.9.3.2}{}}
\newlabel{def:skewnesskurtosis}{{9.6}{342}{Skewness and kurtosis}{definition.9.6}{}}
\newlabel{thm:CauchySchwarz}{{9.1}{342}{Cauchy-Schwarz inequality}{theorem.9.1}{}}
\newlabel{def:asmyptlevel}{{9.7}{343}{Asymptotic level}{definition.9.7}{}}
\newlabel{def:asmyptconsisttest}{{9.8}{343}{Asymptotically consistent test}{definition.9.8}{}}
\newlabel{def:Kullback}{{9.9}{343}{Kullback discrepancy}{definition.9.9}{}}
\newlabel{prp:Kullback}{{9.9}{343}{Properties of the Kullback discrepancy}{proposition.9.9}{}}
\newlabel{def:characteristic}{{9.10}{343}{Characteristic function}{definition.9.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Standard distributions}{344}{subsection.9.3.2}\protected@file@percent }
\newlabel{standard-distributions}{{9.3.2}{344}{Standard distributions}{subsection.9.3.2}{}}
\newlabel{def:fstatistics}{{9.11}{344}{F distribution}{definition.9.11}{}}
\newlabel{def:tStudent}{{9.12}{344}{Student-t distribution}{definition.9.12}{}}
\newlabel{def:chi2}{{9.13}{344}{Chi-square distribution}{definition.9.13}{}}
\newlabel{def:Cauchy}{{9.14}{344}{Cauchy distribution}{definition.9.14}{}}
\newlabel{prp:waldtypeproduct}{{9.10}{344}{Inner product of a multivariate Gaussian variable}{proposition.9.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Pdf of the Cauchy distribution ($\mitmu =0$, $\mitgamma =1$).}}{345}{figure.9.2}\protected@file@percent }
\newlabel{fig:Cauchy}{{9.2}{345}{Pdf of the Cauchy distribution ($\mu =0$, $\gamma =1$)}{figure.9.2}{}}
\newlabel{def:GEVdistri}{{9.15}{345}{Generalized Extreme Value (GEV) distribution}{definition.9.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Stochastic convergences}{346}{subsection.9.3.3}\protected@file@percent }
\newlabel{StochConvergences}{{9.3.3}{346}{Stochastic convergences}{subsection.9.3.3}{}}
\newlabel{prp:chebychev}{{9.11}{346}{Chebychev's inequality}{proposition.9.11}{}}
\newlabel{def:convergenceproba}{{9.16}{346}{Convergence in probability}{definition.9.16}{}}
\newlabel{def:convergenceLr}{{9.17}{346}{Convergence in the Lr norm}{definition.9.17}{}}
\newlabel{def:convergenceAlmost}{{9.18}{346}{Almost sure convergence}{definition.9.18}{}}
\newlabel{def:cvgceDistri}{{9.19}{346}{Convergence in distribution}{definition.9.19}{}}
\newlabel{prp:Slutsky}{{9.12}{347}{Rules for limiting distributions (Slutsky)}{proposition.9.12}{}}
\newlabel{prp:implicationsconv}{{9.13}{347}{Implications of stochastic convergences}{proposition.9.13}{}}
\newlabel{eq:convgce1}{{9.3}{347}{Stochastic convergences}{}{}}
\newlabel{eq:convgce2}{{9.3}{347}{Stochastic convergences}{equation.9.3.3}{}}
\newlabel{prp:cvgce11}{{9.14}{348}{Convergence in distribution to a constant}{proposition.9.14}{}}
\newlabel{exm:plimButNotLr}{{9.1}{348}{Convergence in probability but not $L^r$}{example.9.1}{}}
\newlabel{thm:cauchycritstatic}{{9.2}{348}{Cauchy criterion (non-stochastic case)}{theorem.9.2}{}}
\newlabel{thm:cauchycritstochastic}{{9.3}{348}{Cauchy criterion (stochastic case)}{theorem.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Central limit theorem}{349}{subsection.9.3.4}\protected@file@percent }
\newlabel{CLTappend}{{9.3.4}{349}{Central limit theorem}{subsection.9.3.4}{}}
\newlabel{thm:LLNappendix}{{9.4}{349}{Law of large numbers}{theorem.9.4}{}}
\newlabel{thm:LindbergLevyCLT}{{9.5}{349}{Lindberg-Levy Central limit theorem, CLT}{theorem.9.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Some properties of Gaussian variables}{350}{section.9.4}\protected@file@percent }
\newlabel{GaussianVar}{{9.4}{350}{Some properties of Gaussian variables}{section.9.4}{}}
\newlabel{prp:bandsindependent}{{9.15}{350}{}{proposition.9.15}{}}
\newlabel{prp:update}{{9.16}{350}{Bayesian update in a vector of Gaussian variables}{proposition.9.16}{}}
\newlabel{prp:truncated}{{9.17}{350}{Truncated distributions}{proposition.9.17}{}}
\newlabel{eq:Etrunc}{{9.4}{350}{Truncated distributions}{equation.9.4.4}{}}
\newlabel{eq:Vtrunc}{{9.5}{351}{Truncated distributions}{equation.9.4.5}{}}
\newlabel{eq:Vtrunc2}{{9.6}{351}{Truncated distributions}{equation.9.4.6}{}}
\newlabel{prp:pdfMultivarGaussian}{{9.18}{351}{p.d.f. of a multivariate Gaussian variable}{proposition.9.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Proofs}{351}{section.9.5}\protected@file@percent }
\newlabel{AppendixProof}{{9.5}{351}{Proofs}{section.9.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces $\symbb {E}(X|X<b)$ as a function of $b$ when $X\sim \symcal {N}(0,1)$ (in black).}}{352}{figure.9.3}\protected@file@percent }
\newlabel{fig:inverseMills}{{9.3}{352}{$\mathbb {E}(X|X<b)$ as a function of $b$ when $X\sim \mathcal {N}(0,1)$ (in black)}{figure.9.3}{}}
\newlabel{eq:XXX}{{9.7}{354}{Proofs}{equation.9.5.7}{}}
\newlabel{eq:lm10}{{9.8}{354}{Proofs}{equation.9.5.8}{}}
\newlabel{eq:lm1}{{9.9}{354}{Proofs}{equation.9.5.9}{}}
\newlabel{eq:lm29}{{9.10}{355}{Proofs}{equation.9.5.10}{}}
\newlabel{eq:lm30}{{9.11}{355}{Proofs}{equation.9.5.11}{}}
\newlabel{eq:lm2}{{9.12}{355}{Proofs}{equation.9.5.12}{}}
\newlabel{eq:lm3}{{9.13}{355}{Proofs}{equation.9.5.13}{}}
\newlabel{eq:multiplier}{{9.14}{355}{Proofs}{equation.9.5.14}{}}
\newlabel{eq:lm20}{{9.15}{355}{Proofs}{equation.9.5.15}{}}
\citation{gourieroux_monfort_1995}
\newlabel{eq:lr10}{{9.16}{357}{Proofs}{equation.9.5.16}{}}
\newlabel{eq:1}{{9.17}{359}{Proofs}{equation.9.5.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Additional codes}{362}{section.9.6}\protected@file@percent }
\newlabel{additional-codes}{{9.6}{362}{Additional codes}{section.9.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.1}Simulating GEV distributions}{362}{subsection.9.6.1}\protected@file@percent }
\newlabel{App:GEV}{{9.6.1}{362}{Simulating GEV distributions}{subsection.9.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.2}Computing the covariance matrix of IRF using the delta method}{363}{subsection.9.6.2}\protected@file@percent }
\newlabel{IRFDELTA}{{9.6.2}{363}{Computing the covariance matrix of IRF using the delta method}{subsection.9.6.2}{}}
\bibdata{book.bib,packages.bib}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Statistical Tables}{364}{section.9.7}\protected@file@percent }
\newlabel{statistical-tables}{{9.7}{364}{Statistical Tables}{section.9.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Quantiles of the $\symcal {N}(0,1)$ distribution. If $a$ and $b$ are respectively the row and column number; then the corresponding cell gives $\symbb {P}(0<X\le a+b)$, where $X \sim \symcal {N}(0,1)$.}}{365}{table.9.1}\protected@file@percent }
\newlabel{tab:Normal}{{9.1}{365}{Quantiles of the $\mathcal {N}(0,1)$ distribution. If $a$ and $b$ are respectively the row and column number; then the corresponding cell gives $\mathbb {P}(0<X\le a+b)$, where $X \sim \mathcal {N}(0,1)$}{table.9.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Quantiles of the Student-$t$ distribution. The rows correspond to different degrees of freedom ($\mitnu $, say); the columns correspond to different probabilities ($z$, say). The cell gives $q$ that is s.t. $\symbb {P}(-q<X<q)=z$, with $X \sim t(\mitnu )$.}}{366}{table.9.2}\protected@file@percent }
\newlabel{tab:Student}{{9.2}{366}{Quantiles of the Student-$t$ distribution. The rows correspond to different degrees of freedom ($\nu $, say); the columns correspond to different probabilities ($z$, say). The cell gives $q$ that is s.t. $\mathbb {P}(-q<X<q)=z$, with $X \sim t(\nu )$}{table.9.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.3}{\ignorespaces Quantiles of the $\mitchi ^2$ distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities.}}{367}{table.9.3}\protected@file@percent }
\newlabel{tab:Chi2}{{9.3}{367}{Quantiles of the $\chi ^2$ distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities}{table.9.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.4}{\ignorespaces Quantiles of the $\symcal {F}$ distribution. The columns and rows correspond to different degrees of freedom (resp. $n_1$ and $n_2$). The different panels correspond to different probabilities ($\mitalpha $) The corresponding cell gives $z$ that is s.t. $\symbb {P}(X \le z)=\mitalpha $, with $X \sim \symcal {F}(n_1,n_2)$.}}{368}{table.9.4}\protected@file@percent }
\newlabel{tab:Fstat}{{9.4}{368}{Quantiles of the $\mathcal {F}$ distribution. The columns and rows correspond to different degrees of freedom (resp. $n_1$ and $n_2$). The different panels correspond to different probabilities ($\alpha $) The corresponding cell gives $z$ that is s.t. $\mathbb {P}(X \le z)=\alpha $, with $X \sim \mathcal {F}(n_1,n_2)$}{table.9.4}{}}
\bibcite{Abadie_Cattaneo_2018}{{1}{2018}{{Abadie and Cattaneo}}{{}}}
\bibcite{Anderson_1971}{{2}{1971}{{Anderson}}{{}}}
\bibcite{Anderson_Hsiao_1982}{{3}{1982}{{Anderson and Hsiao}}{{}}}
\bibcite{Andrews_Stock_Sun_2019}{{4}{2019}{{Andrews et~al.}}{{}}}
\bibcite{angrist_mostly_2008}{{5}{2008}{{Angrist and Pischke}}{{}}}
\bibcite{Arellano_Bond_1991}{{6}{1991}{{Arellano and Bond}}{{}}}
\bibcite{Arias_et_al_2018}{{7}{2018}{{Arias et~al.}}{{}}}
\bibcite{BARSKY2011273}{{8}{2011}{{Barsky and Sims}}{{}}}
\bibcite{BERNANKE198649}{{9}{1986}{{Bernanke}}{{}}}
\bibcite{Bernanke_Boivin_Eliasz_2005}{{10}{2005}{{Bernanke et~al.}}{{}}}
\bibcite{Blanchard_Quah_1989}{{11}{1989}{{Blanchard and Quah}}{{}}}
\bibcite{boxjen76}{{12}{1976}{{Box and Jenkins}}{{}}}
\bibcite{Cameron_Miller_2014}{{13}{2014}{{Cameron and Miller}}{{}}}
\bibcite{Cameron_Trivedi_2005}{{14}{2005}{{Cameron and Trivedi}}{{}}}
\bibcite{Christiano_Eichenbaum_Evans_1996}{{15}{1996}{{Christiano et~al.}}{{}}}
\bibcite{NBERc11177}{{16}{2007}{{Christiano et~al.}}{{}}}
\bibcite{Cochrane_Orcutt_1949}{{17}{1949}{{Cochrane and Orcutt}}{{}}}
\bibcite{Cochrane_Piazzesi_2002}{{18}{2002}{{Cochrane and Piazzesi}}{{}}}
\bibcite{Danne_2015}{{19}{2015}{{Danne}}{{}}}
\bibcite{DEGOOIJER2006443}{{20}{2006}{{{De Gooijer} and Hyndman}}{{}}}
\bibcite{DEDOLA20051543}{{21}{2005}{{Dedola and Lippi}}{{}}}
\bibcite{DEE20041697}{{22}{2004}{{Dee}}{{}}}
\bibcite{Diebold_Mariano_1995}{{23}{1995}{{Diebold and Mariano}}{{}}}
\bibcite{Duan_et_al_1983}{{24}{1983}{{Duan et~al.}}{{}}}
\bibcite{Durbin_1954}{{25}{1954}{{Durbin}}{{}}}
\bibcite{Durbin_Watson_1950}{{26}{1950}{{Durbin and Watson}}{{}}}
\bibcite{Durbin_Watson_1951}{{27}{1951}{{Durbin and Watson}}{{}}}
\bibcite{Erceg_et_al_2005}{{28}{2005}{{Erceg et~al.}}{{}}}
\bibcite{Faust_Leeper_1997}{{29}{1997}{{Faust and Leeper}}{{}}}
\bibcite{Fischer_1977}{{30}{1977}{{Fischer}}{{}}}
\bibcite{Fritsch_et_al_2019}{{31}{2019}{{Fritsch et~al.}}{{}}}
\bibcite{Gali_1999}{{32}{1999}{{Gal\'i}}{{}}}
\bibcite{Gali_1992}{{33}{1992}{{Galí}}{{}}}
\bibcite{Gerlach_Smets_1995}{{34}{1995}{{Gerlach and Smets}}{{}}}
\bibcite{Gertler_Karadi_2015}{{35}{2015}{{Gertler and Karadi}}{{}}}
\bibcite{gourieroux_monfort_1995}{{36}{1995}{{Gouri\'eroux and Monfort}}{{}}}
\bibcite{Gourieroux_Monfort_Renne_2020}{{37}{2020}{{Gouri\'eroux et~al.}}{{}}}
\bibcite{Gourieroux_Monfort_Renne_2017}{{38}{2017}{{Gouriéroux et~al.}}{{}}}
\bibcite{Granger_1969}{{39}{1969}{{Granger}}{{}}}
\bibcite{Greene2003Econometric}{{40}{2003}{{Greene}}{{}}}
\bibcite{Gurkaynak_et_al_2005}{{41}{2005}{{G\"urkaynak et~al.}}{{}}}
\bibcite{Hamilton_1994}{{42}{1994}{{Hamilton}}{{}}}
\bibcite{Hansen_1982}{{43}{1982}{{Hansen}}{{}}}
\bibcite{Hausman_1978}{{44}{1978}{{Hausman}}{{}}}
\bibcite{Heiss_2002}{{45}{2002}{{Heiss}}{{}}}
\bibcite{Hensher_Greene_2002}{{46}{2002}{{Hensher and Greene}}{{}}}
\bibcite{James2013}{{47}{2013}{{James et~al.}}{{}}}
\bibcite{Jorda_2005}{{48}{2005}{{Jord\`a}}{{}}}
\bibcite{JST_2017}{{49}{2017}{{Jord\`a et~al.}}{{}}}
\bibcite{Kilian_1998}{{50}{1998}{{Kilian}}{{}}}
\bibcite{VARetp}{{51}{2022}{{Kim}}{{}}}
\bibcite{KUTTNER2001523}{{52}{2001}{{Kuttner}}{{}}}
\bibcite{Lanne_Lutkepohl_2008}{{53}{2008}{{Lanne and L\"utkepohl}}{{}}}
\bibcite{LANNE2010121}{{54}{2010}{{Lanne et~al.}}{{}}}
\bibcite{Litterman_Scheinkman_1991}{{55}{1991}{{Litterman and Scheinkman}}{{}}}
\bibcite{Lutkepohl_1990}{{56}{1990}{{L\"utkepohl}}{{}}}
\bibcite{LUTKEPOHL20172}{{57}{2017}{{Lütkepohl and Netšunajev}}{{}}}
\bibcite{MacKinnon_White_1985}{{58}{1985}{{MacKinnon and White}}{{}}}
\bibcite{MACKINNON2022}{{59}{2022}{{MacKinnon et~al.}}{{}}}
\bibcite{Marcellino_et_al_2006}{{60}{2006}{{Marcellino et~al.}}{{}}}
\bibcite{McCracken_Ng_2016}{{61}{2016}{{McCracken and Ng}}{{}}}
\bibcite{Meyer_Viscusi_Durbin_1995}{{62}{1995}{{Meyer et~al.}}{{}}}
\bibcite{Mroz_1987}{{63}{1987}{{Mroz}}{{}}}
\bibcite{Nakosteen_Zimmer_1980}{{64}{1980}{{Nakosteen and Zimmer}}{{}}}
\bibcite{Newey_West_1987}{{65}{1987}{{Newey and West}}{{}}}
\bibcite{NORMANDIN20041217}{{66}{2004}{{Normandin and Phaneuf}}{{}}}
\bibcite{Piazzesi_Swanson_2008}{{67}{2008}{{Piazzesi and Swanson}}{{}}}
\bibcite{Ramey_2016_NBER}{{68}{2016}{{Ramey}}{{}}}
\bibcite{Rigobon_2003}{{69}{2003}{{Rigobon}}{{}}}
\bibcite{Roberts_Gelman_Gilks_1997}{{70}{1997}{{Roberts et~al.}}{{}}}
\bibcite{Romer_Romer_2004}{{71}{2004}{{Romer and Romer}}{{}}}
\bibcite{RubioRamirez_et_al_2010}{{72}{2010}{{Ruibio-Ram\'irez et~al.}}{{}}}
\bibcite{Sargan_1958}{{73}{1958}{{Sargan}}{{}}}
\bibcite{Sims_1980}{{74}{1980}{{Sims}}{{}}}
\bibcite{Sims_1986}{{75}{1986}{{Sims}}{{}}}
\bibcite{Stock_Watson_2016}{{76}{2016}{{Stock and Watson}}{{}}}
\bibcite{Stock_Watson_2003}{{77}{2003}{{Stock and Watson}}{{}}}
\bibcite{Stock_Watson_2018}{{78}{2018}{{Stock and Watson}}{{}}}
\bibcite{stock_yogo_2005}{{79}{2005}{{Stock and Yogo}}{{}}}
\bibcite{Tibshirani_2011}{{80}{2011}{{Tibshirani}}{{}}}
\bibcite{Tobin_1956}{{81}{1956}{{Tobin}}{{}}}
\bibcite{Uhlig_2004}{{82}{2004}{{Uhlig}}{{}}}
\bibcite{Uhlig_2005}{{83}{2005}{{Uhlig}}{{}}}
\bibcite{White_1980}{{84}{1980}{{White}}{{}}}
\bibcite{Wu_1973}{{85}{1973}{{Wu}}{{}}}
\gdef \@abspage@last{376}
