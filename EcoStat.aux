\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{apalike}
\HyPL@Entry{0<</S/D>>}
\newlabel{intro}{{}{5}{Econometrics and statistics}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Basic statistical results}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Basics}{{1}{7}{Basic statistical results}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Cumulative and probability density funtions (c.d.f. and p.d.f.)}{7}{section.1.1}\protected@file@percent }
\newlabel{cumulative-and-probability-density-funtions-c.d.f.-and-p.d.f.}{{1.1}{7}{Cumulative and probability density funtions (c.d.f. and p.d.f.)}{section.1.1}{}}
\newlabel{def:cdf}{{1.1}{7}{Cumulative distribution function (c.d.f.)}{definition.1.1}{}}
\newlabel{def:pdf}{{1.2}{7}{Probability distribution function (p.d.f.)}{definition.1.2}{}}
\newlabel{eq:flim}{{1.1}{7}{Cumulative and probability density funtions (c.d.f. and p.d.f.)}{equation.1.1.1}{}}
\newlabel{def:jointcdf}{{1.3}{7}{Joint cumulative distribution function (c.d.f.)}{definition.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The volume between the horizontal plane ($z=0$) and the surface is equal to $F_{XY}(0.5,1)=\symbb {P}(X<0.5,Y<1)$.}}{8}{figure.1.1}\protected@file@percent }
\newlabel{fig:essai3d2}{{1.1}{8}{The volume between the horizontal plane ($z=0$) and the surface is equal to $F_{XY}(0.5,1)=\mathbb {P}(X<0.5,Y<1)$}{figure.1.1}{}}
\newlabel{def:jointpdf}{{1.4}{8}{Joint probability density function (p.d.f.)}{definition.1.4}{}}
\newlabel{def:condcdf}{{1.5}{8}{Conditional probability distribution function}{definition.1.5}{}}
\newlabel{prp:condcdf}{{1.1}{8}{Conditional probability distribution function}{proposition.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Assume that the basis of the black column is defined by those points whose $x$-coordinates are between $x$ and $x+\mitvarepsilon $ and $y$-coordinates are between $y$ and $y+\mitvarepsilon $. Then the volume of the black column is equal to $\symbb {P}(x < X \le x+\mitvarepsilon ,y < Y \le y+\mitvarepsilon )$, which is approximately equal to $f_{XY}(x,y)\mitvarepsilon ^2$ if $\mitvarepsilon $ is small.}}{9}{figure.1.2}\protected@file@percent }
\newlabel{fig:essai3d}{{1.2}{9}{Assume that the basis of the black column is defined by those points whose $x$-coordinates are between $x$ and $x+\varepsilon $ and $y$-coordinates are between $y$ and $y+\varepsilon $. Then the volume of the black column is equal to $\mathbb {P}(x < X \le x+\varepsilon ,y < Y \le y+\varepsilon )$, which is approximately equal to $f_{XY}(x,y)\varepsilon ^2$ if $\varepsilon $ is small}{figure.1.2}{}}
\newlabel{def:independent}{{1.6}{9}{Independent random variables}{definition.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Law of iterated expectations}{10}{section.1.2}\protected@file@percent }
\newlabel{law-of-iterated-expectations}{{1.2}{10}{Law of iterated expectations}{section.1.2}{}}
\newlabel{prp:lawiteratedexpect}{{1.2}{10}{Law of iterated expectations}{proposition.1.2}{}}
\newlabel{exm:mixture}{{1.1}{10}{Mixture of Gaussian distributions}{example.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Example of pdfs of mixtures of Gaussian distribututions.}}{11}{figure.1.3}\protected@file@percent }
\newlabel{fig:mixtureG}{{1.3}{11}{Example of pdfs of mixtures of Gaussian distribututions}{figure.1.3}{}}
\newlabel{exm:Buffon}{{1.2}{11}{Buffon (1733)'s needles}{example.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Schematic representation of the problem.}}{12}{figure.1.4}\protected@file@percent }
\newlabel{fig:Buffon}{{1.4}{12}{Schematic representation of the problem}{figure.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Law of total variance}{12}{section.1.3}\protected@file@percent }
\newlabel{law-of-total-variance}{{1.3}{12}{Law of total variance}{section.1.3}{}}
\newlabel{prp:lawtotalvariance}{{1.3}{12}{Law of total variance}{proposition.1.3}{}}
\newlabel{exm:mixture2}{{1.3}{12}{Mixture of Gaussian distributions (cont'd)}{example.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}About consistent estimators}{12}{section.1.4}\protected@file@percent }
\newlabel{about-consistent-estimators}{{1.4}{12}{About consistent estimators}{section.1.4}{}}
\newlabel{exm:NonConsist}{{1.4}{13}{Example of non-convergent estimator}{example.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Simulation of $\bar {X}_n$ when $X_i \sim i.i.d. \mbox  {Cauchy}$.}}{14}{figure.1.5}\protected@file@percent }
\newlabel{fig:figCauchy}{{1.5}{14}{Simulation of $\bar {X}_n$ when $X_i \sim i.i.d. \mbox {Cauchy}$}{figure.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Central Limit Theorem}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{TCL}{{2}{15}{Central Limit Theorem}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Law of large numbers}{15}{section.2.1}\protected@file@percent }
\newlabel{law-of-large-numbers}{{2.1}{15}{Law of large numbers}{section.2.1}{}}
\newlabel{def:smplpop}{{2.1}{15}{Sample and Population}{definition.2.1}{}}
\newlabel{thm:LLN}{{2.1}{15}{Law of large numbers}{theorem.2.1}{}}
\newlabel{exm:doctorVisits}{{2.1}{16}{}{example.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Distribution of the number of doctor visits. Source: Swiss Household Panel.}}{16}{figure.2.1}\protected@file@percent }
\newlabel{fig:incomeDistri}{{2.1}{16}{Distribution of the number of doctor visits. Source: Swiss Household Panel}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Central Limit Theorem (CLT)}{17}{section.2.2}\protected@file@percent }
\newlabel{central-limit-theorem-clt}{{2.2}{17}{Central Limit Theorem (CLT)}{section.2.2}{}}
\newlabel{thm:LindbergLevyCLT}{{2.2}{17}{Lindberg-Levy Central limit theorem, CLT}{theorem.2.2}{}}
\newlabel{exr:ExoBuffon}{{2.1}{18}{}{exercise.2.1}{}}
\newlabel{exr:ExoPileFace}{{2.2}{18}{}{exercise.2.2}{}}
\newlabel{thm:MCLT}{{2.3}{18}{Multivariate Central limit theorem, CLT}{theorem.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Comparison of sample means}{18}{section.2.3}\protected@file@percent }
\newlabel{comparison-of-sample-means}{{2.3}{18}{Comparison of sample means}{section.2.3}{}}
\newlabel{exm:comparSmplMeans}{{2.2}{19}{Comparison of sample means}{example.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Distribution of yearly gross incomes for Swiss residents under the age of 35. Source: SHP. Vertical dashed lines indicates the sample mean.}}{20}{figure.2.2}\protected@file@percent }
\newlabel{fig:RcompMeans}{{2.2}{20}{Distribution of yearly gross incomes for Swiss residents under the age of 35. Source: SHP. Vertical dashed lines indicates the sample mean}{figure.2.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Statistical tests}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Tests}{{3}{21}{Statistical tests}{chapter.3}{}}
\newlabel{exm:FPFN}{{3.1}{22}{Early Warning Signals}{example.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Size and power of a test}{22}{section.3.1}\protected@file@percent }
\newlabel{size-and-power-of-a-test}{{3.1}{22}{Size and power of a test}{section.3.1}{}}
\newlabel{def:sizepower}{{3.1}{22}{Size and Power of a test}{definition.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The different types of statistical tests}{23}{section.3.2}\protected@file@percent }
\newlabel{the-different-types-of-statistical-tests}{{3.2}{23}{The different types of statistical tests}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Two-sided test. Under $H_0$, $S \sim t(5)$. $\mitalpha $ is the size of the test.}}{23}{figure.3.1}\protected@file@percent }
\newlabel{fig:Illusttest1}{{3.1}{23}{Two-sided test. Under $H_0$, $S \sim t(5)$. $\alpha $ is the size of the test}{figure.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Two-sided test. Under $H_0$, $S \sim t(5)$. $\mitalpha $ is the size of the test.}}{24}{figure.3.2}\protected@file@percent }
\newlabel{fig:Illusttest2}{{3.2}{24}{Two-sided test. Under $H_0$, $S \sim t(5)$. $\alpha $ is the size of the test}{figure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces One-sided test. Under $H_0$, $S \sim \mitchi ^2(5)$. $\mitalpha $ is the size of the test.}}{25}{figure.3.3}\protected@file@percent }
\newlabel{fig:IllusTestOneSided1}{{3.3}{25}{One-sided test. Under $H_0$, $S \sim \chi ^2(5)$. $\alpha $ is the size of the test}{figure.3.3}{}}
\newlabel{exm:Factory}{{3.2}{25}{A practical illustration of size and power}{example.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces One-sided test. Under $H_0$, $S \sim \mitchi ^2(5)$. $\mitalpha $ is the size of the test.}}{26}{figure.3.4}\protected@file@percent }
\newlabel{fig:IllusTestOneSided2}{{3.4}{26}{One-sided test. Under $H_0$, $S \sim \chi ^2(5)$. $\alpha $ is the size of the test}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Factory example.}}{27}{figure.3.5}\protected@file@percent }
\newlabel{fig:FactoryR}{{3.5}{27}{Factory example}{figure.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Asymptotic properties of statistical tests}{27}{section.3.3}\protected@file@percent }
\newlabel{asymptotic-properties-of-statistical-tests}{{3.3}{27}{Asymptotic properties of statistical tests}{section.3.3}{}}
\newlabel{def:asmyptlevel}{{3.2}{28}{Asymptotic level}{definition.3.2}{}}
\newlabel{exm:FactAsymptlevel}{{3.3}{28}{The factory example}{example.3.3}{}}
\newlabel{def:asmyptconsisttest}{{3.3}{28}{Asymptotically consistent test}{definition.3.3}{}}
\newlabel{exm:FactAsymptConsist}{{3.4}{28}{The factory example}{example.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Example: Normality tests}{29}{section.3.4}\protected@file@percent }
\newlabel{example-normality-tests}{{3.4}{29}{Example: Normality tests}{section.3.4}{}}
\newlabel{prp:normSkewKurt}{{3.1}{29}{Skewness and kurtosis of the normal distribution}{proposition.3.1}{}}
\newlabel{prp:conssitCentralMoments}{{3.2}{30}{Consistency of central sample moments}{proposition.3.2}{}}
\newlabel{prp:Asymptg3Normal}{{3.3}{30}{Asymptotic distribution of 3rd-order sample central moment of a normal distribution}{proposition.3.3}{}}
\newlabel{prp:Asymptg4Normal}{{3.4}{30}{Asymptotic distribution of 4th-order sample central moment of a normal distribution}{proposition.3.4}{}}
\newlabel{prp:Asymptg3g4Normal}{{3.5}{30}{Joint asymptotic distribution of 3rd and 4th-order sample central moments of a normal distribution}{proposition.3.5}{}}
\newlabel{prp:JB}{{3.6}{30}{Jarque-Bera asympt. distri}{proposition.3.6}{}}
\newlabel{exm:JB}{{3.5}{30}{Consistency of the Jarque-Bera normality test}{example.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Distribution of the JB test statistic under $H_0$ (normality).}}{31}{figure.3.6}\protected@file@percent }
\newlabel{fig:JBTest2}{{3.6}{31}{Distribution of the JB test statistic under $H_0$ (normality)}{figure.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Distribution of the JB test statistic when the $y_i$'s are drawn from a uniform distribution (hence $H_0$ is not satisfied).}}{32}{figure.3.7}\protected@file@percent }
\newlabel{fig:JBTest3}{{3.7}{32}{Distribution of the JB test statistic when the $y_i$'s are drawn from a uniform distribution (hence $H_0$ is not satisfied)}{figure.3.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Linear Regressions}{33}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ChapterLS}{{4}{33}{Linear Regressions}{chapter.4}{}}
\newlabel{def:essai}{{4.1}{33}{}{definition.4.1}{}}
\newlabel{eq:linearspecif}{{4.1}{33}{}{equation.4.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Hypotheses}{33}{section.4.1}\protected@file@percent }
\newlabel{linearHyp}{{4.1}{33}{Hypotheses}{section.4.1}{}}
\newlabel{hyp:fullrank}{{4.1}{33}{Full rank}{hypothesis.4.1}{}}
\newlabel{hyp:exogeneity}{{4.2}{34}{Conditional mean-zero assumption}{hypothesis.4.2}{}}
\newlabel{prp:implicationExog}{{4.1}{34}{}{proposition.4.1}{}}
\newlabel{hyp:homoskedasticity}{{4.3}{34}{Homoskedasticity}{hypothesis.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Homoskedasticity vs heteroskedasticity. See text for the exact specifications.}}{35}{figure.4.1}\protected@file@percent }
\newlabel{fig:heteroskedasticity}{{4.1}{35}{Homoskedasticity vs heteroskedasticity. See text for the exact specifications}{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Income versus age. Data are from the Swiss Household Panel. The sample is restricted to persons that have completed at least 19 years of study. The figure shows that the dispersion of yearly income increases with age.}}{36}{figure.4.2}\protected@file@percent }
\newlabel{fig:exmpSalarayPhDSHP}{{4.2}{36}{Income versus age. Data are from the Swiss Household Panel. The sample is restricted to persons that have completed at least 19 years of study. The figure shows that the dispersion of yearly income increases with age}{figure.4.2}{}}
\newlabel{hyp:noncorrelResid}{{4.4}{36}{Uncorrelated errors}{hypothesis.4.4}{}}
\newlabel{prp:Sigma}{{4.2}{36}{}{proposition.4.2}{}}
\newlabel{hyp:normality}{{4.5}{36}{Normal distribution}{hypothesis.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Least square estimation}{37}{section.4.2}\protected@file@percent }
\newlabel{LSquares}{{4.2}{37}{Least square estimation}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Derivation of the OLS formula}{37}{subsection.4.2.1}\protected@file@percent }
\newlabel{derivation-of-the-ols-formula}{{4.2.1}{37}{Derivation of the OLS formula}{subsection.4.2.1}{}}
\newlabel{eq:OLSFOC}{{4.3}{37}{Derivation of the OLS formula}{equation.4.2.3}{}}
\newlabel{eq:Mres}{{4.4}{37}{Derivation of the OLS formula}{equation.4.2.4}{}}
\newlabel{eq:Proj}{{4.5}{37}{Derivation of the OLS formula}{equation.4.2.5}{}}
\newlabel{exm:bivar}{{4.1}{38}{Bivariate case}{example.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Properties of the OLS estimate (small sample)}{38}{subsection.4.2.2}\protected@file@percent }
\newlabel{properties-of-the-ols-estimate-small-sample}{{4.2.2}{38}{Properties of the OLS estimate (small sample)}{subsection.4.2.2}{}}
\newlabel{prp:propOLS}{{4.3}{38}{Properties of the OLS estimator}{proposition.4.3}{}}
\newlabel{thm:GaussMarkov}{{4.1}{39}{Gauss-Markov Theorem}{theorem.4.1}{}}
\newlabel{thm:FW}{{4.2}{40}{Frisch-Waugh Theorem}{theorem.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Goodness of fit}{41}{subsection.4.2.3}\protected@file@percent }
\newlabel{goodness-of-fit}{{4.2.3}{41}{Goodness of fit}{subsection.4.2.3}{}}
\citation{Greene2003Econometric}
\newlabel{eq:RR2}{{4.6}{42}{Goodness of fit}{equation.4.2.6}{}}
\newlabel{prp:chgeR2}{{4.4}{42}{Change in SSR when a variable is added}{proposition.4.4}{}}
\newlabel{eq:uu}{{4.7}{42}{Change in SSR when a variable is added}{equation.4.2.7}{}}
\newlabel{eq:uuu}{{4.8}{43}{Goodness of fit}{equation.4.2.8}{}}
\newlabel{prp:chgeInR2}{{4.5}{43}{Change in the coefficient of determination when a variable is added}{proposition.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces This figure illustrates the monotonous increase in the $R^2$ as a function of the number of explanatory variables. In the true model, there is no explanatory variables, i.e., $y_i = \mitvarepsilon _i$. We then take (independent) regressors and regress $y$ on the latter, progressively increasing the set of regressors.}}{44}{figure.4.3}\protected@file@percent }
\newlabel{fig:R2issue}{{4.3}{44}{This figure illustrates the monotonous increase in the $R^2$ as a function of the number of explanatory variables. In the true model, there is no explanatory variables, i.e., $y_i = \varepsilon _i$. We then take (independent) regressors and regress $y$ on the latter, progressively increasing the set of regressors}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Inference and confidence intervals (in small sample)}{44}{subsection.4.2.4}\protected@file@percent }
\newlabel{inference-and-confidence-intervals-in-small-sample}{{4.2.4}{44}{Inference and confidence intervals (in small sample)}{subsection.4.2.4}{}}
\newlabel{eq:distriBcondi}{{4.9}{44}{Inference and confidence intervals (in small sample)}{equation.4.2.9}{}}
\newlabel{prp:expects2}{{4.6}{44}{}{proposition.4.6}{}}
\newlabel{eq:s2}{{4.10}{44}{}{equation.4.2.10}{}}
\newlabel{prp:s2distri}{{4.7}{45}{}{proposition.4.7}{}}
\newlabel{prp:indeps2b}{{4.8}{45}{}{proposition.4.8}{}}
\newlabel{eq:resultstudentt}{{4.11}{46}{Inference and confidence intervals (in small sample)}{equation.4.2.11}{}}
\newlabel{eq:resultstudentt2}{{4.12}{46}{Inference and confidence intervals (in small sample)}{equation.4.2.12}{}}
\newlabel{exm:SHP0001}{{4.2}{46}{Education and income}{example.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The higher the degree of freedom, the closer the distribution of \(t(\mitnu )\) gets to the normal distribution. (Convergence in distribution.)}}{47}{figure.4.4}\protected@file@percent }
\newlabel{fig:chartStudent}{{4.4}{47}{The higher the degree of freedom, the closer the distribution of \(t(\nu )\) gets to the normal distribution. (Convergence in distribution.)}{figure.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Testing a set of linear restrictions}{49}{subsection.4.2.5}\protected@file@percent }
\newlabel{Ftest}{{4.2.5}{49}{Testing a set of linear restrictions}{subsection.4.2.5}{}}
\newlabel{eq:restrictions}{{4.13}{49}{Testing a set of linear restrictions}{equation.4.2.13}{}}
\newlabel{eq:H0Ftest}{{4.15}{49}{Testing a set of linear restrictions}{equation.4.2.15}{}}
\newlabel{eq:W1}{{4.16}{49}{Testing a set of linear restrictions}{equation.4.2.16}{}}
\newlabel{prp:Ftest1}{{4.9}{50}{}{proposition.4.9}{}}
\newlabel{eq:defFstatistics}{{4.17}{50}{}{equation.4.2.17}{}}
\newlabel{prp:Ftest}{{4.10}{50}{}{proposition.4.10}{}}
\newlabel{eq:defFstatistics2}{{4.18}{50}{}{equation.4.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Large Sample Properties}{51}{subsection.4.2.6}\protected@file@percent }
\newlabel{largeSample}{{4.2.6}{51}{Large Sample Properties}{subsection.4.2.6}{}}
\newlabel{prp:asymptOLS}{{4.11}{51}{}{proposition.4.11}{}}
\newlabel{eq:Qasympt}{{4.19}{51}{}{equation.4.2.19}{}}
\newlabel{eq:convgceOLS}{{4.20}{51}{}{equation.4.2.20}{}}
\newlabel{eq:sXX}{{4.21}{51}{Large Sample Properties}{equation.4.2.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Common pitfalls in linear regressions}{52}{section.4.3}\protected@file@percent }
\newlabel{CommonPitfalls}{{4.3}{52}{Common pitfalls in linear regressions}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Multicollinearity}{52}{subsection.4.3.1}\protected@file@percent }
\newlabel{multicollinearity}{{4.3.1}{52}{Multicollinearity}{subsection.4.3.1}{}}
\newlabel{eq:multicollin}{{4.22}{52}{Multicollinearity}{equation.4.3.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Omitted variables}{52}{subsection.4.3.2}\protected@file@percent }
\newlabel{Omitted}{{4.3.2}{52}{Omitted variables}{subsection.4.3.2}{}}
\newlabel{exm:CASchools}{{4.3}{53}{}{example.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Irrelevant variable}{53}{subsection.4.3.3}\protected@file@percent }
\newlabel{irrelevant}{{4.3.3}{53}{Irrelevant variable}{subsection.4.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Instrumental Variables}{54}{section.4.4}\protected@file@percent }
\newlabel{IV}{{4.4}{54}{Instrumental Variables}{section.4.4}{}}
\newlabel{eq:modelIV}{{4.23}{54}{Instrumental Variables}{equation.4.4.23}{}}
\newlabel{eq:exmIV}{{4.24}{54}{Instrumental Variables}{equation.4.4.24}{}}
\newlabel{def:instruments}{{4.2}{54}{Instrumental variables}{definition.4.2}{}}
\citation{stock_yogo_2005}
\citation{Andrews_Stock_Sun_2019}
\newlabel{prp:IV}{{4.12}{55}{Asymptotic distribution of the IV estimator}{proposition.4.12}{}}
\citation{Durbin_1954}
\citation{Wu_1973}
\citation{Hausman_1978}
\citation{Stock_Watson_2003}
\newlabel{eq:IV}{{4.25}{56}{Instrumental Variables}{equation.4.4.25}{}}
\newlabel{exm:priceElasticity}{{4.4}{56}{Estimation of price elasticity}{example.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces This figure illustrates the situation prevailing when estimating a price-elasticity (and the price is endogenous).}}{57}{figure.4.5}\protected@file@percent }
\newlabel{fig:figureIV}{{4.5}{57}{This figure illustrates the situation prevailing when estimating a price-elasticity (and the price is endogenous)}{figure.4.5}{}}
\citation{Stock_Watson_2003}
\citation{DEE20041697}
\newlabel{exm:IVCollegeDistance}{{4.5}{59}{Education and wage}{example.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}General Regression Model (GRM) and robust covariance matrices}{60}{section.4.5}\protected@file@percent }
\newlabel{general-regression-model-grm-and-robust-covariance-matrices}{{4.5}{60}{General Regression Model (GRM) and robust covariance matrices}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Presentation of the General Regression Model (GRM)}{61}{subsection.4.5.1}\protected@file@percent }
\newlabel{presentation-of-the-general-regression-model-grm}{{4.5.1}{61}{Presentation of the General Regression Model (GRM)}{subsection.4.5.1}{}}
\newlabel{eq:assumGLS2}{{4.26}{61}{Presentation of the General Regression Model (GRM)}{equation.4.5.26}{}}
\newlabel{def:GRM}{{4.3}{61}{General Regression Model (GRM)}{definition.4.3}{}}
\newlabel{eq:heteroskedasticity}{{4.27}{61}{Presentation of the General Regression Model (GRM)}{equation.4.5.27}{}}
\newlabel{eq:autocorrelation}{{4.28}{61}{Presentation of the General Regression Model (GRM)}{equation.4.5.28}{}}
\newlabel{exm:autocorrelaaa}{{4.6}{61}{Auto-regressive processes}{example.4.6}{}}
\newlabel{eq:usual}{{4.29}{61}{Auto-regressive processes}{equation.4.5.29}{}}
\newlabel{eq:usual2}{{4.30}{61}{Auto-regressive processes}{equation.4.5.30}{}}
\newlabel{eq:SigmaAutocorrel}{{4.31}{61}{Auto-regressive processes}{equation.4.5.31}{}}
\citation{Cochrane_Orcutt_1949}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Generalized Least Squares}{62}{subsection.4.5.2}\protected@file@percent }
\newlabel{GLS}{{4.5.2}{62}{Generalized Least Squares}{subsection.4.5.2}{}}
\newlabel{eq:betaGLS}{{4.32}{62}{Generalized Least Squares}{equation.4.5.32}{}}
\newlabel{exm:autocorrelaaa2}{{4.7}{62}{GLS in the auto-correlation case}{example.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Asymptotic properties of the OLS estimator in the GRM framework}{62}{subsection.4.5.3}\protected@file@percent }
\newlabel{asymptotic-properties-of-the-ols-estimator-in-the-grm-framework}{{4.5.3}{62}{Asymptotic properties of the OLS estimator in the GRM framework}{subsection.4.5.3}{}}
\newlabel{eq:xsx}{{4.33}{62}{Asymptotic properties of the OLS estimator in the GRM framework}{equation.4.5.33}{}}
\citation{White_1980}
\citation{Newey_West_1987}
\newlabel{prp:XXX}{{4.13}{63}{Consistency of the OLS estimator in the GRM framework}{proposition.4.13}{}}
\newlabel{prp:AsymptGRM}{{4.14}{63}{Asymptotic distribution of the OLS estimator in the GRM framework}{proposition.4.14}{}}
\newlabel{prp:AsymptIVGRM}{{4.15}{63}{Asymptotic distribution of the IV estimator in the GRM framework}{proposition.4.15}{}}
\newlabel{eq:GeneralXSigmaX}{{4.34}{63}{Asymptotic properties of the OLS estimator in the GRM framework}{equation.4.5.34}{}}
\citation{White_1980}
\citation{MacKinnon_White_1985}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}HAC-robust covariance matrices}{64}{subsection.4.5.4}\protected@file@percent }
\newlabel{HAC}{{4.5.4}{64}{HAC-robust covariance matrices}{subsection.4.5.4}{}}
\newlabel{exm:HCheteroskedasticity}{{4.8}{64}{Heteroskedasticity}{example.4.8}{}}
\newlabel{eq:white}{{4.35}{64}{Heteroskedasticity}{equation.4.5.35}{}}
\newlabel{eq:White}{{4.36}{64}{Heteroskedasticity}{equation.4.5.36}{}}
\newlabel{eq:WhiteHC1}{{4.37}{64}{Heteroskedasticity}{equation.4.5.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Situation of heteroskedasticity. The model is $y_i = x_i + \mitvarepsilon _i, \hskip 1em\relax \mitvarepsilon _i \sim \symcal {N}(0,x_i^2)$, where the $x_i$'s are i.i.d. $t(6)$.}}{65}{figure.4.6}\protected@file@percent }
\newlabel{fig:simulHeterosk}{{4.6}{65}{Situation of heteroskedasticity. The model is $y_i = x_i + \varepsilon _i, \quad \varepsilon _i \sim \mathcal {N}(0,x_i^2)$, where the $x_i$'s are i.i.d. $t(6)$}{figure.4.6}{}}
\citation{JST_2017}
\citation{Newey_West_1987}
\newlabel{exm:HCheterAC}{{4.9}{66}{Heteroskedasticity and Autocorrelation (HAC)}{example.4.9}{}}
\newlabel{eq:NW}{{4.39}{67}{Heteroskedasticity and Autocorrelation (HAC)}{equation.4.5.38}{}}
\newlabel{eq:simul11}{{4.39}{67}{Heteroskedasticity and Autocorrelation (HAC)}{equation.4.5.39}{}}
\newlabel{eq:simul22}{{4.40}{67}{Heteroskedasticity and Autocorrelation (HAC)}{equation.4.5.40}{}}
\citation{Durbin_Watson_1950}
\citation{Durbin_Watson_1951}
\newlabel{exm:DurbinWats}{{4.10}{68}{Durbin-Watson test}{example.4.10}{}}
\citation{MACKINNON2022}
\citation{Cameron_Miller_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Cluster-robust covariance matrices}{69}{subsection.4.5.5}\protected@file@percent }
\newlabel{Clusters}{{4.5.5}{69}{Cluster-robust covariance matrices}{subsection.4.5.5}{}}
\newlabel{eq:BBB}{{4.41}{70}{Cluster-robust covariance matrices}{equation.4.5.41}{}}
\newlabel{eq:cluster1}{{4.42}{70}{Cluster-robust covariance matrices}{equation.4.5.42}{}}
\newlabel{eq:definiS}{{4.43}{70}{Cluster-robust covariance matrices}{equation.4.5.43}{}}
\newlabel{hyp:cluster}{{4.6}{70}{Clusters}{hypothesis.4.6}{}}
\newlabel{eq:cluster2}{{4.44}{70}{Cluster-robust covariance matrices}{equation.4.5.44}{}}
\citation{White_1980}
\citation{MACKINNON2022}
\newlabel{eq:AsymptCL}{{4.45}{71}{Cluster-robust covariance matrices}{equation.4.5.45}{}}
\newlabel{hyp:twowaycluster}{{4.7}{71}{Two-way clusters}{hypothesis.4.7}{}}
\newlabel{prp:twoWayCov}{{4.16}{71}{Covariance of scores in the two-way-cluster setup}{proposition.4.16}{}}
\citation{MACKINNON2022}
\citation{MACKINNON2022}
\citation{Tibshirani_2011}
\citation{James2013}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Shrinkage methods}{73}{section.4.6}\protected@file@percent }
\newlabel{shrinkage-methods}{{4.6}{73}{Shrinkage methods}{section.4.6}{}}
\newlabel{eq:minLasso}{{4.47}{74}{Shrinkage methods}{equation.4.6.47}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Panel regressions}{79}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Panel}{{5}{79}{Panel regressions}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Specification and notations}{79}{section.5.1}\protected@file@percent }
\newlabel{specification-and-notations}{{5.1}{79}{Specification and notations}{section.5.1}{}}
\newlabel{eq:panel1}{{5.1}{79}{Specification and notations}{equation.5.1.1}{}}
\citation{Stock_Watson_2003}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The data are the same for both panels. On Panel (b), blue dots are for $t=1$, red dots are for $t=2$. The lines relate the dots associated to the same entity $i$.}}{80}{figure.5.1}\protected@file@percent }
\newlabel{fig:simulPanel}{{5.1}{80}{The data are the same for both panels. On Panel (b), blue dots are for $t=1$, red dots are for $t=2$. The lines relate the dots associated to the same entity $i$}{figure.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Cigarette consumption versus real price in the CigarettesSW panel dataset.}}{81}{figure.5.2}\protected@file@percent }
\newlabel{fig:cigarettes}{{5.2}{81}{Cigarette consumption versus real price in the CigarettesSW panel dataset}{figure.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Three standard cases}{82}{section.5.2}\protected@file@percent }
\newlabel{three-standard-cases}{{5.2}{82}{Three standard cases}{section.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Estimation of Fixed-Effects Models}{82}{section.5.3}\protected@file@percent }
\newlabel{FixedEffect}{{5.3}{82}{Estimation of Fixed-Effects Models}{section.5.3}{}}
\newlabel{hyp:FE}{{5.1}{82}{Fixed-effect model}{hypothesis.5.1}{}}
\newlabel{eq:LSDV}{{5.2}{82}{Estimation of Fixed-Effects Models}{equation.5.3.2}{}}
\newlabel{eq:bfixedeffects11}{{5.3}{83}{Estimation of Fixed-Effects Models}{equation.5.3.3}{}}
\newlabel{eq:bfixedeffects}{{5.5}{83}{Estimation of Fixed-Effects Models}{equation.5.3.5}{}}
\newlabel{eq:a}{{5.6}{83}{Estimation of Fixed-Effects Models}{equation.5.3.6}{}}
\newlabel{eq:LSDV2}{{5.7}{84}{Estimation of Fixed-Effects Models}{equation.5.3.7}{}}
\citation{JST_2017}
\newlabel{exm:JSTPanel}{{5.1}{85}{Housing prices and interest rates}{example.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Estimation of random effects models}{86}{section.5.4}\protected@file@percent }
\newlabel{RandomEffect}{{5.4}{86}{Estimation of random effects models}{section.5.4}{}}
\newlabel{eq:OLSRUM}{{5.8}{87}{Estimation of random effects models}{equation.5.4.8}{}}
\citation{Hausman_1978}
\newlabel{exm:airbnb}{{5.2}{89}{Spatial data}{example.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Airbnb prices for the Zurich area, 22 June 2017. The size of the circles is proportional to the prices. White lines delineate the 12 districts of the city.}}{90}{figure.5.3}\protected@file@percent }
\newlabel{fig:airbnb}{{5.3}{90}{Airbnb prices for the Zurich area, 22 June 2017. The size of the circles is proportional to the prices. White lines delineate the 12 districts of the city}{figure.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Dynamic Panel Regressions}{91}{section.5.5}\protected@file@percent }
\newlabel{DynPanel}{{5.5}{91}{Dynamic Panel Regressions}{section.5.5}{}}
\newlabel{eq:paneldyn}{{5.9}{91}{Dynamic Panel Regressions}{equation.5.5.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Regression residuals. The sizes of the circles are proportional to the absolute values of the residuals, the color indicates the sign (blue for negative).}}{92}{figure.5.4}\protected@file@percent }
\newlabel{fig:airbnb3}{{5.4}{92}{Regression residuals. The sizes of the circles are proportional to the absolute values of the residuals, the color indicates the sign (blue for negative)}{figure.5.4}{}}
\citation{Anderson_Hsiao_1982}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces illustration of the bias pertianing to the LSDV estimation approach in the presence of auto-correlation of the depend variable.}}{93}{figure.5.5}\protected@file@percent }
\newlabel{fig:dynpanel1}{{5.5}{93}{illustration of the bias pertianing to the LSDV estimation approach in the presence of auto-correlation of the depend variable}{figure.5.5}{}}
\citation{Arellano_Bond_1991}
\newlabel{eq:paneldynFisrtDiff}{{5.10}{94}{Dynamic Panel Regressions}{equation.5.5.10}{}}
\citation{Fritsch_et_al_2019}
\citation{Arellano_Bond_1991}
\citation{Arellano_Bond_1991}
\citation{Abadie_Cattaneo_2018}
\citation{angrist_mostly_2008}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Introduction to program evaluation}{98}{section.5.6}\protected@file@percent }
\newlabel{introduction-to-program-evaluation}{{5.6}{98}{Introduction to program evaluation}{section.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Presentation of the problem}{98}{subsection.5.6.1}\protected@file@percent }
\newlabel{presentation-of-the-problem}{{5.6.1}{98}{Presentation of the problem}{subsection.5.6.1}{}}
\newlabel{eq:EY1a}{{5.11}{99}{Presentation of the problem}{equation.5.6.11}{}}
\newlabel{eq:EY0a}{{5.12}{99}{Presentation of the problem}{equation.5.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Randomized controlled trials (RCTs)}{99}{subsection.5.6.2}\protected@file@percent }
\newlabel{randomized-controlled-trials-rcts}{{5.6.2}{99}{Randomized controlled trials (RCTs)}{subsection.5.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Difference-in-Difference (DiD) approach}{99}{subsection.5.6.3}\protected@file@percent }
\newlabel{difference-in-difference-did-approach}{{5.6.3}{99}{Difference-in-Difference (DiD) approach}{subsection.5.6.3}{}}
\newlabel{eq:DiD}{{5.13}{99}{Difference-in-Difference (DiD) approach}{equation.5.6.13}{}}
\citation{Meyer_Viscusi_Durbin_1995}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Source: Abadie et al., (1998).}}{100}{figure.5.6}\protected@file@percent }
\newlabel{fig:figAbadie}{{5.6}{100}{Source: Abadie et al., (1998)}{figure.5.6}{}}
\citation{Meyer_Viscusi_Durbin_1995}
\citation{Meyer_Viscusi_Durbin_1995}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.4}Application of the DiD approach}{101}{subsection.5.6.4}\protected@file@percent }
\newlabel{application-of-the-did-approach}{{5.6.4}{101}{Application of the DiD approach}{subsection.5.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Source: Meyer et al., (1995).}}{101}{figure.5.7}\protected@file@percent }
\newlabel{fig:figMeyer}{{5.7}{101}{Source: Meyer et al., (1995)}{figure.5.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Estimation Methods}{105}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{estimation-methods}{{6}{105}{Estimation Methods}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Generalized Method of Moments (GMM)}{105}{section.6.1}\protected@file@percent }
\newlabel{secGMM}{{6.1}{105}{Generalized Method of Moments (GMM)}{section.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Definition of the GMM estimator}{105}{subsection.6.1.1}\protected@file@percent }
\newlabel{definition-of-the-gmm-estimator}{{6.1.1}{105}{Definition of the GMM estimator}{subsection.6.1.1}{}}
\newlabel{def:GMM}{{6.1}{105}{}{definition.6.1}{}}
\citation{Newey_West_1987}
\newlabel{eq:consistGMM}{{6.1}{106}{Definition of the GMM estimator}{equation.6.1.1}{}}
\newlabel{eq:Shat}{{6.2}{106}{Definition of the GMM estimator}{equation.6.1.2}{}}
\citation{Sargan_1958}
\citation{Hansen_1982}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Asymptotic distribution of the GMM estimator}{107}{subsection.6.1.2}\protected@file@percent }
\newlabel{asymptotic-distribution-of-the-gmm-estimator}{{6.1.2}{107}{Asymptotic distribution of the GMM estimator}{subsection.6.1.2}{}}
\newlabel{eq:asymptGMM}{{6.3}{107}{Asymptotic distribution of the GMM estimator}{equation.6.1.3}{}}
\newlabel{eq:VGMM}{{6.4}{107}{Asymptotic distribution of the GMM estimator}{equation.6.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Testing hypotheses in the GMM framework}{107}{subsection.6.1.3}\protected@file@percent }
\newlabel{overidentif}{{6.1.3}{107}{Testing hypotheses in the GMM framework}{subsection.6.1.3}{}}
\newlabel{eq:HansenSargan}{{6.5}{107}{Testing hypotheses in the GMM framework}{equation.6.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Example: Estimation of the Stochastic Discount Factor (s.d.f.)}{108}{subsection.6.1.4}\protected@file@percent }
\newlabel{example-estimation-of-the-stochastic-discount-factor-s.d.f.}{{6.1.4}{108}{Example: Estimation of the Stochastic Discount Factor (s.d.f.)}{subsection.6.1.4}{}}
\newlabel{eq:sdf}{{6.6}{108}{Example: Estimation of the Stochastic Discount Factor (s.d.f.)}{equation.6.1.6}{}}
\newlabel{eq:momF}{{6.7}{108}{Example: Estimation of the Stochastic Discount Factor (s.d.f.)}{equation.6.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Maximum Likelihood Estimation}{111}{section.6.2}\protected@file@percent }
\newlabel{secMLE}{{6.2}{111}{Maximum Likelihood Estimation}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Intuition}{111}{subsection.6.2.1}\protected@file@percent }
\newlabel{intuition}{{6.2.1}{111}{Intuition}{subsection.6.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations}}{112}{figure.6.1}\protected@file@percent }
\newlabel{fig:MLE1}{{6.1}{112}{The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations}{figure.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Proba. that $y_i-\mitvarepsilon \le Y_i < y_i+\mitvarepsilon $, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function.}}{112}{figure.6.2}\protected@file@percent }
\newlabel{fig:MLE2}{{6.2}{112}{Proba. that $y_i-\varepsilon \le Y_i < y_i+\varepsilon $, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function}{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.}}{113}{figure.6.3}\protected@file@percent }
\newlabel{fig:MLE3}{{6.3}{113}{Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function}{figure.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Definition and properties}{113}{subsection.6.2.2}\protected@file@percent }
\newlabel{definition-and-properties}{{6.2.2}{113}{Definition and properties}{subsection.6.2.2}{}}
\newlabel{def:likelihood}{{6.2}{113}{Likelihood function}{definition.6.2}{}}
\newlabel{exm:normal}{{6.1}{114}{Gaussian distribution}{example.6.1}{}}
\newlabel{def:score}{{6.3}{114}{Score}{definition.6.3}{}}
\newlabel{prp:score}{{6.1}{114}{Score expectation}{proposition.6.1}{}}
\newlabel{def:Fisher}{{6.4}{114}{Fisher information matrix}{definition.6.4}{}}
\newlabel{prp:Fisher}{{6.2}{114}{}{proposition.6.2}{}}
\newlabel{prp:additiv}{{6.3}{115}{Additive property of the Information matrix}{proposition.6.3}{}}
\newlabel{thm:FDCR}{{6.1}{115}{Frechet-Darmois-Cramer-Rao bound}{theorem.6.1}{}}
\newlabel{def:identif}{{6.5}{115}{Identifiability}{definition.6.5}{}}
\newlabel{def:MLEest}{{6.6}{115}{Maximum Likelihood Estimator (MLE)}{definition.6.6}{}}
\newlabel{eq:MLEestimator}{{6.8}{115}{Maximum Likelihood Estimator (MLE)}{equation.6.2.8}{}}
\newlabel{def:likFunction}{{6.7}{115}{Likelihood equation}{definition.6.7}{}}
\newlabel{hyp:MLEregularity}{{6.1}{115}{Regularity assumptions}{hypothesis.6.1}{}}
\newlabel{prp:MLEproperties}{{6.4}{116}{Properties of MLE}{proposition.6.4}{}}
\newlabel{eq:normMLE}{{6.10}{116}{Properties of MLE}{equation.6.2.10}{}}
\newlabel{eq:III1}{{6.11}{116}{Definition and properties}{equation.6.2.11}{}}
\newlabel{eq:I2}{{6.12}{116}{Definition and properties}{equation.6.2.11}{}}
\newlabel{eq:III3}{{6.13}{116}{Definition and properties}{equation.6.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}To sum up -- MLE in practice}{116}{subsection.6.2.3}\protected@file@percent }
\newlabel{to-sum-up-mle-in-practice}{{6.2.3}{116}{To sum up -- MLE in practice}{subsection.6.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Example: MLE estimation of a mixture of Gaussian distribution}{117}{subsection.6.2.4}\protected@file@percent }
\newlabel{example-mle-estimation-of-a-mixture-of-gaussian-distribution}{{6.2.4}{117}{Example: MLE estimation of a mixture of Gaussian distribution}{subsection.6.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Time series of SMI weekly returns (source: Yahoo Finance).}}{118}{figure.6.4}\protected@file@percent }
\newlabel{fig:smiData}{{6.4}{118}{Time series of SMI weekly returns (source: Yahoo Finance)}{figure.6.4}{}}
\newlabel{eq:DeltaMethod}{{6.15}{119}{Example: MLE estimation of a mixture of Gaussian distribution}{equation.6.2.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Comparison of different estimates of the distribution of returns.}}{121}{figure.6.5}\protected@file@percent }
\newlabel{fig:smidistri}{{6.5}{121}{Comparison of different estimates of the distribution of returns}{figure.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}Test procedures}{121}{subsection.6.2.5}\protected@file@percent }
\newlabel{TestMLE}{{6.2.5}{121}{Test procedures}{subsection.6.2.5}{}}
\newlabel{prp:Walddistri}{{6.5}{122}{Asymptotic distribution of the Wald statistic}{proposition.6.5}{}}
\newlabel{eq:varinWald}{{6.17}{122}{Asymptotic distribution of the Wald statistic}{equation.6.2.17}{}}
\newlabel{prp:LMdistri}{{6.6}{122}{Asymptotic distribution of the LM test statistic}{proposition.6.6}{}}
\newlabel{eq:xiLM}{{6.18}{122}{Asymptotic distribution of the LM test statistic}{equation.6.2.18}{}}
\newlabel{def:LR}{{6.8}{123}{Likelihood Ratio test statistics}{definition.6.8}{}}
\newlabel{prp:equivLRLMW}{{6.7}{123}{Asymptotic equivalence of LR, LM, and Wald tests}{proposition.6.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Bayesian approach}{123}{section.6.3}\protected@file@percent }
\newlabel{bayesian-approach}{{6.3}{123}{Bayesian approach}{section.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Introduction}{123}{subsection.6.3.1}\protected@file@percent }
\newlabel{introduction}{{6.3.1}{123}{Introduction}{subsection.6.3.1}{}}
\newlabel{eq:post1}{{6.19}{124}{Introduction}{equation.6.3.19}{}}
\newlabel{eq:post2}{{6.21}{124}{Introduction}{equation.6.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Monte-Carlo Markov Chains}{124}{subsection.6.3.2}\protected@file@percent }
\newlabel{monte-carlo-markov-chains}{{6.3.2}{124}{Monte-Carlo Markov Chains}{subsection.6.3.2}{}}
\newlabel{def:MC}{{6.9}{125}{Markov Chain}{definition.6.9}{}}
\newlabel{eq:alphaXXX}{{6.22}{125}{Monte-Carlo Markov Chains}{equation.6.3.22}{}}
\newlabel{eq:symmQ}{{6.23}{125}{Monte-Carlo Markov Chains}{equation.6.3.23}{}}
\newlabel{eq:hypoQ}{{6.24}{125}{Monte-Carlo Markov Chains}{equation.6.3.24}{}}
\citation{Roberts_Gelman_Gilks_1997}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Example: AR(1) specification}{126}{subsection.6.3.3}\protected@file@percent }
\newlabel{example-ar1-specification}{{6.3.3}{126}{Example: AR(1) specification}{subsection.6.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces The upper line of plot compares prior (black) and posterior (red) distributions. The vertical dashed blue lines indicate the true values of the parameters. The second row of plots show the sequence of $\boldsymbol  \mittheta _i$'s generated by the MCMC algorithm. These sequences are the ones used to produce the posterior distributions (red lines) in the upper plots.}}{130}{figure.6.6}\protected@file@percent }
\newlabel{fig:MCMC8}{{6.6}{130}{The upper line of plot compares prior (black) and posterior (red) distributions. The vertical dashed blue lines indicate the true values of the parameters. The second row of plots show the sequence of $\boldsymbol \theta _i$'s generated by the MCMC algorithm. These sequences are the ones used to produce the posterior distributions (red lines) in the upper plots}{figure.6.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Binary-choice models}{131}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{binary-choice-models}{{7}{131}{Binary-choice models}{chapter.7}{}}
\newlabel{eq:binaryBenroulli}{{7.1}{131}{Binary-choice models}{equation.7.0.1}{}}
\newlabel{eq:genericBinary}{{7.2}{131}{Binary-choice models}{equation.7.0.2}{}}
\gdef \LT@i {\LT@entry 
    {1}{101.52281pt}\LT@entry 
    {1}{171.17975pt}\LT@entry 
    {1}{197.05241pt}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Fitting a binary variable with a linear model (Linear Probability Model, LPM). The model is $\symbb {P}(y_i=1|x_i)=\mitPhi (0.5+2x_i)$, where $\mitPhi $ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\symcal {N}(0,1)$.}}{132}{figure.7.1}\protected@file@percent }
\newlabel{fig:LPM}{{7.1}{132}{Fitting a binary variable with a linear model (Linear Probability Model, LPM). The model is $\mathbb {P}(y_i=1|x_i)=\Phi (0.5+2x_i)$, where $\Phi $ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\mathcal {N}(0,1)$}{figure.7.1}{}}
\newlabel{tab:foo}{{7.1}{132}{Binary-choice models}{table.7.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{ This table provides examples of function \(g\), s.t. \(\symbb {P}(y_i=1|\mathbf {x}_i;\boldsymbol  heta) = g(\boldsymbol  \mittheta '\mathbf {x}_i)\). The LPM case (last row) is given for comparison but, again, it does not satisfy \(g(\boldsymbol  \mittheta '\mathbf {x}_i) \in [0,1]\) for any value of \(\boldsymbol  \mittheta '\mathbf {x}_i\).}}{132}{table.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Probit, Logit, and Log-log functions.}}{133}{figure.7.2}\protected@file@percent }
\newlabel{fig:ProbLogit}{{7.2}{133}{Probit, Logit, and Log-log functions}{figure.7.2}{}}
\newlabel{eq:probit}{{7.3}{133}{Binary-choice models}{equation.7.0.3}{}}
\newlabel{eq:logit}{{7.4}{133}{Binary-choice models}{equation.7.0.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Interpretation in terms of latent variable, and utility-based models}{133}{section.7.1}\protected@file@percent }
\newlabel{latent}{{7.1}{133}{Interpretation in terms of latent variable, and utility-based models}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The model is $\symbb {P}(y_i=1|x_i)=\mitPhi (0.5+2x_i)$, where $\mitPhi $ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\symcal {N}(0,1)$. Crosses give the model-implied probabilities of having $y_i=1$ (conditional on $x_i$).}}{134}{figure.7.3}\protected@file@percent }
\newlabel{fig:LPM2}{{7.3}{134}{The model is $\mathbb {P}(y_i=1|x_i)=\Phi (0.5+2x_i)$, where $\Phi $ is the c.d.f. of the normal distribution and where $x_i \sim \,i.i.d.\,\mathcal {N}(0,1)$. Crosses give the model-implied probabilities of having $y_i=1$ (conditional on $x_i$)}{figure.7.3}{}}
\newlabel{eq:utility}{{7.5}{134}{Interpretation in terms of latent variable, and utility-based models}{equation.7.1.5}{}}
\citation{Nakosteen_Zimmer_1980}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Distribution of $y_i^*$ conditional on $\mathbf {x}_i$.}}{135}{figure.7.4}\protected@file@percent }
\newlabel{fig:Latent}{{7.4}{135}{Distribution of $y_i^*$ conditional on $\mathbf {x}_i$}{figure.7.4}{}}
\newlabel{exm:migration}{{7.1}{135}{Migration and income}{example.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Alternative-Varying Regressors}{135}{section.7.2}\protected@file@percent }
\newlabel{Avregressors}{{7.2}{135}{Alternative-Varying Regressors}{section.7.2}{}}
\citation{Cameron_Trivedi_2005}
\newlabel{eq:utility2}{{7.6}{136}{Alternative-Varying Regressors}{equation.7.2.6}{}}
\newlabel{eq:utility3}{{7.7}{136}{Alternative-Varying Regressors}{equation.7.2.7}{}}
\newlabel{exm:FishingTable}{{7.2}{136}{Fishing-mode dataset}{example.7.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Estimation}{137}{section.7.3}\protected@file@percent }
\newlabel{estimation}{{7.3}{137}{Estimation}{section.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Marginal effects}{138}{section.7.4}\protected@file@percent }
\newlabel{marginalFX}{{7.4}{138}{Marginal effects}{section.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Goodness of fit}{138}{section.7.5}\protected@file@percent }
\newlabel{goodness-of-fit-1}{{7.5}{138}{Goodness of fit}{section.7.5}{}}
\newlabel{exm:creditProbit}{{7.3}{139}{Credit and defaults (Lending-club dataset)}{example.7.3}{}}
\citation{Cameron_Trivedi_2005}
\newlabel{exm:Fisch142}{{7.4}{142}{Replicating Table 14.2 of Cameron and Trivedi (2005)}{example.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Predictions and ROC curves}{143}{section.7.6}\protected@file@percent }
\newlabel{predictions-and-roc-curves}{{7.6}{143}{Predictions and ROC curves}{section.7.6}{}}
\newlabel{exm:FishingROC}{{7.5}{144}{ROC with the fishing-mode dataset}{example.7.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Application of the ROC methodology on the fishing-mode dataset.}}{144}{figure.7.5}\protected@file@percent }
\newlabel{fig:fishing3}{{7.5}{144}{Application of the ROC methodology on the fishing-mode dataset}{figure.7.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Time Series}{145}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{TS}{{8}{145}{Time Series}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction to time series}{145}{section.8.1}\protected@file@percent }
\newlabel{introduction-to-time-series}{{8.1}{145}{Introduction to time series}{section.8.1}{}}
\newlabel{def:whitenoise}{{8.1}{145}{White noise}{definition.8.1}{}}
\newlabel{def:MDS}{{8.2}{145}{Martingale Difference Sequence}{definition.8.2}{}}
\newlabel{exm:ARCH}{{8.1}{145}{ARCH process}{example.8.1}{}}
\newlabel{exm:whiteNotMDS}{{8.2}{146}{}{example.8.2}{}}
\newlabel{eq:lagOp}{{8.1}{146}{Introduction to time series}{equation.8.1.1}{}}
\newlabel{def:autocov}{{8.3}{146}{Autocovariance}{definition.8.3}{}}
\newlabel{def:covstat}{{8.4}{147}{Covariance stationarity}{definition.8.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Example of a process that is not covariance stationary ($y_t = 0.1t + \mitvarepsilon _t$, where $\mitvarepsilon _t \sim \symcal {N}(0,1)$).}}{147}{figure.8.1}\protected@file@percent }
\newlabel{fig:nonstat1}{{8.1}{147}{Example of a process that is not covariance stationary ($y_t = 0.1t + \varepsilon _t$, where $\varepsilon _t \sim \mathcal {N}(0,1)$)}{figure.8.1}{}}
\newlabel{def:strictstat}{{8.5}{147}{Strict stationarity}{definition.8.5}{}}
\newlabel{prp:gammaMinus}{{8.1}{147}{}{proposition.8.1}{}}
\citation{JST_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99\% confidence interval of the standard normal distribution ($\pm 2.58$).}}{148}{figure.8.2}\protected@file@percent }
\newlabel{fig:nonstat2}{{8.2}{148}{Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99\% confidence interval of the standard normal distribution ($\pm 2.58$)}{figure.8.2}{}}
\newlabel{def:autocor}{{8.6}{148}{Auto-correlation}{definition.8.6}{}}
\newlabel{def:ergodicity}{{8.7}{148}{Mean ergodicity}{definition.8.7}{}}
\newlabel{def:ergod2nd}{{8.8}{148}{Second-moment ergodicity}{definition.8.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database.}}{149}{figure.8.3}\protected@file@percent }
\newlabel{fig:autocov}{{8.3}{149}{Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database}{figure.8.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces For order $j$, the slope of the blue line is, approximately, $\hat {\mitgamma }_j/\widehat {\symbb {V}ar}(y_t)$, where hats indicate sample moments.}}{149}{figure.8.4}\protected@file@percent }
\newlabel{fig:autocov2}{{8.4}{149}{For order $j$, the slope of the blue line is, approximately, $\hat {\gamma }_j/\widehat {\mathbb {V}ar}(y_t)$, where hats indicate sample moments}{figure.8.4}{}}
\citation{Anderson_1971}
\citation{Newey_West_1987}
\newlabel{thm:CLTcovstat}{{8.1}{150}{Central Limit Theorem for covariance-stationary processes}{theorem.8.1}{}}
\newlabel{eq:TCL20}{{8.2}{150}{Central Limit Theorem for covariance-stationary processes}{equation.8.1.2}{}}
\newlabel{eq:TCL2}{{8.3}{150}{Central Limit Theorem for covariance-stationary processes}{equation.8.1.2}{}}
\newlabel{eq:TCL4ts}{{8.4}{150}{Central Limit Theorem for covariance-stationary processes}{equation.8.1.2}{}}
\newlabel{def:LRV}{{8.9}{150}{Long-run variance}{definition.8.9}{}}
\newlabel{eq:covSmplMean}{{8.5}{150}{Introduction to time series}{equation.8.1.5}{}}
\newlabel{eq:NWest}{{8.6}{150}{Introduction to time series}{equation.8.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces The three samples have been simulated using the following data generating process: $x_t = \mitmu + \mitrho (x_{t-1}-\mitmu ) + \sqrt  {1-\mitrho ^2}\mitvarepsilon _t$, where $\mitvarepsilon _t \sim \symcal {N}(0,1)$. Case A: $\mitrho = 0$; Case B: $\mitrho = 0.7$; Case C: $\mitrho = 0.999$. In the three cases, $\symbb {E}(x_t)=\mitmu =2$ and $\symbb {V}ar(x_t)=1$.}}{151}{figure.8.5}\protected@file@percent }
\newlabel{fig:TVTCL}{{8.5}{151}{The three samples have been simulated using the following data generating process: $x_t = \mu + \rho (x_{t-1}-\mu ) + \sqrt {1-\rho ^2}\varepsilon _t$, where $\varepsilon _t \sim \mathcal {N}(0,1)$. Case A: $\rho = 0$; Case B: $\rho = 0.7$; Case C: $\rho = 0.999$. In the three cases, $\mathbb {E}(x_t)=\mu =2$ and $\mathbb {V}ar(x_t)=1$}{figure.8.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Univariate processes}{152}{section.8.2}\protected@file@percent }
\newlabel{univariate-processes}{{8.2}{152}{Univariate processes}{section.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Moving Average (MA) processes}{152}{subsection.8.2.1}\protected@file@percent }
\newlabel{moving-average-ma-processes}{{8.2.1}{152}{Moving Average (MA) processes}{subsection.8.2.1}{}}
\newlabel{def:MAq}{{8.11}{152}{MA(q) process}{definition.8.11}{}}
\newlabel{prp:covMAq}{{8.2}{153}{Covariance-stationarity of an MA(q) process}{proposition.8.2}{}}
\newlabel{eq:autocovMA}{{8.7}{153}{Covariance-stationarity of an MA(q) process}{equation.8.2.7}{}}
\newlabel{def:summability}{{8.12}{153}{Absolute and square summability}{definition.8.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Simulation of MA processes.}}{154}{figure.8.6}\protected@file@percent }
\newlabel{fig:simMA}{{8.6}{154}{Simulation of MA processes}{figure.8.6}{}}
\newlabel{thm:infMA}{{8.2}{154}{Existence condition for an infinite MA process}{theorem.8.2}{}}
\newlabel{prp:momentsMAinf}{{8.3}{154}{First two moments of an infinite MA process}{proposition.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Auto-Regressive (AR) processes}{155}{subsection.8.2.2}\protected@file@percent }
\newlabel{ARsection}{{8.2.2}{155}{Auto-Regressive (AR) processes}{subsection.8.2.2}{}}
\newlabel{def:AR1}{{8.13}{155}{First-order AR process (AR(1))}{definition.8.13}{}}
\newlabel{prp:statioAR1}{{8.4}{156}{Covariance-stationarity of an AR(1) process}{proposition.8.4}{}}
\newlabel{def:ARp}{{8.14}{156}{AR(p) process}{definition.8.14}{}}
\newlabel{eq:AR}{{8.8}{156}{AR(p) process}{equation.8.2.8}{}}
\newlabel{eq:F}{{8.9}{156}{Auto-Regressive (AR) processes}{equation.8.2.9}{}}
\newlabel{prp:Feigen}{{8.5}{156}{The eigenvalues of matrix F}{proposition.8.5}{}}
\newlabel{eq:Feigen}{{8.10}{156}{The eigenvalues of matrix F}{equation.8.2.10}{}}
\newlabel{prp:stability}{{8.6}{156}{Covariance-stationarity of an AR(p) process}{proposition.8.6}{}}
\newlabel{eq:outside}{{8.11}{156}{Covariance-stationarity of an AR(p) process}{equation.8.2.11}{}}
\newlabel{eq:inside}{{8.12}{156}{Covariance-stationarity of an AR(p) process}{equation.8.2.12}{}}
\newlabel{eq:EAR}{{8.14}{157}{Auto-Regressive (AR) processes}{equation.8.2.14}{}}
\newlabel{eq:gammas}{{8.15}{158}{Auto-Regressive (AR) processes}{equation.8.2.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}PACF approach to identify AR/MA processes}{158}{subsection.8.2.3}\protected@file@percent }
\newlabel{PACFapproach}{{8.2.3}{158}{PACF approach to identify AR/MA processes}{subsection.8.2.3}{}}
\newlabel{def:partialAC}{{8.15}{158}{Partial auto-correlation}{definition.8.15}{}}
\citation{DEGOOIJER2006443}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces ACF/PACF analysis of two processes (MA process on the left, AR on the right).}}{159}{figure.8.7}\protected@file@percent }
\newlabel{fig:pacf}{{8.7}{159}{ACF/PACF analysis of two processes (MA process on the left, AR on the right)}{figure.8.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Forecasting}{159}{section.8.3}\protected@file@percent }
\newlabel{forecasting}{{8.3}{159}{Forecasting}{section.8.3}{}}
\newlabel{prp:smallestMSE}{{8.7}{160}{Smallest MSE}{proposition.8.7}{}}
\newlabel{prp:smallestMSElinear}{{8.8}{160}{}{proposition.8.8}{}}
\newlabel{eq:proj}{{8.16}{160}{}{equation.8.3.16}{}}
\newlabel{eq:linproj}{{8.17}{160}{Forecasting}{equation.8.3.17}{}}
\newlabel{exm:fcstMAq}{{8.3}{161}{Forecasting an MA(q) process}{example.8.3}{}}
\newlabel{exm:fcstARp}{{8.4}{161}{Forecasting an AR(p) process}{example.8.4}{}}
\citation{Diebold_Mariano_1995}
\citation{Newey_West_1987}
\citation{JST_2017}
\newlabel{exm:SwissOutOfSample}{{8.5}{163}{Forecasting Swiss GDP growth}{example.8.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Appendix}{167}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{append}{{9}{167}{Appendix}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Principal component analysis (PCA)}{167}{section.9.1}\protected@file@percent }
\newlabel{PCAapp}{{9.1}{167}{Principal component analysis (PCA)}{section.9.1}{}}
\newlabel{eq:PCA11}{{9.1}{167}{Principal component analysis (PCA)}{equation.9.1.1}{}}
\citation{Litterman_Scheinkman_1991}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Linear algebra: definitions and results}{169}{section.9.2}\protected@file@percent }
\newlabel{LinAlgebra}{{9.2}{169}{Linear algebra: definitions and results}{section.9.2}{}}
\newlabel{def:determinant}{{9.1}{169}{Eigenvalues}{definition.9.1}{}}
\newlabel{prp:determinant}{{9.1}{169}{Properties of the determinant}{proposition.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities.}}{170}{figure.9.1}\protected@file@percent }
\newlabel{fig:USydsPCA1}{{9.1}{170}{Some PCA results. The dataset contains 8 time series of U.S. interest rates of different maturities}{figure.9.1}{}}
\newlabel{def:MoorPenrose}{{9.2}{171}{Moore-Penrose inverse}{definition.9.2}{}}
\newlabel{prp:MoorPenrose}{{9.2}{171}{Properties of the Moore-Penrose inverse}{proposition.9.2}{}}
\newlabel{def:idempotent}{{9.3}{171}{Idempotent matrix}{definition.9.3}{}}
\newlabel{prp:rootsidempotent}{{9.3}{171}{Roots of an idempotent matrix}{proposition.9.3}{}}
\newlabel{prp:chi2idempotent}{{9.4}{171}{Idempotent matrix and chi-square distribution}{proposition.9.4}{}}
\newlabel{prp:constrainedLS}{{9.5}{171}{Constrained least squares}{proposition.9.5}{}}
\newlabel{prp:inversepartitioned}{{9.6}{172}{Inverse of a partitioned matrix}{proposition.9.6}{}}
\newlabel{def:FOD}{{9.4}{172}{Matrix derivatives}{definition.9.4}{}}
\newlabel{prp:partial}{{9.7}{172}{}{proposition.9.7}{}}
\newlabel{prp:absMs}{{9.8}{172}{Square and absolute summability}{proposition.9.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Statistical analysis: definitions and results}{172}{section.9.3}\protected@file@percent }
\newlabel{variousResults}{{9.3}{172}{Statistical analysis: definitions and results}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Moments and statistics}{172}{subsection.9.3.1}\protected@file@percent }
\newlabel{moments-and-statistics}{{9.3.1}{172}{Moments and statistics}{subsection.9.3.1}{}}
\newlabel{def:partialcorrel}{{9.5}{172}{Partial correlation}{definition.9.5}{}}
\newlabel{eq:pc}{{9.2}{172}{Partial correlation}{equation.9.3.2}{}}
\newlabel{def:skewnesskurtosis}{{9.6}{173}{Skewness and kurtosis}{definition.9.6}{}}
\newlabel{thm:CauchySchwarz}{{9.1}{173}{Cauchy-Schwarz inequality}{theorem.9.1}{}}
\newlabel{def:asmyptlevel}{{9.7}{173}{Asymptotic level}{definition.9.7}{}}
\newlabel{def:asmyptconsisttest}{{9.8}{173}{Asymptotically consistent test}{definition.9.8}{}}
\newlabel{def:Kullback}{{9.9}{173}{Kullback discrepancy}{definition.9.9}{}}
\newlabel{prp:Kullback}{{9.9}{173}{Properties of the Kullback discrepancy}{proposition.9.9}{}}
\newlabel{def:characteristic}{{9.10}{174}{Characteristic function}{definition.9.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Standard distributions}{174}{subsection.9.3.2}\protected@file@percent }
\newlabel{standard-distributions}{{9.3.2}{174}{Standard distributions}{subsection.9.3.2}{}}
\newlabel{def:fstatistics}{{9.11}{174}{F distribution}{definition.9.11}{}}
\newlabel{def:tStudent}{{9.12}{174}{Student-t distribution}{definition.9.12}{}}
\newlabel{def:chi2}{{9.13}{174}{Chi-square distribution}{definition.9.13}{}}
\newlabel{def:Cauchy}{{9.14}{174}{Cauchy distribution}{definition.9.14}{}}
\newlabel{prp:waldtypeproduct}{{9.10}{174}{Inner product of a multivariate Gaussian variable}{proposition.9.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Pdf of the Cauchy distribution ($\mitmu =0$, $\mitgamma =1$).}}{175}{figure.9.2}\protected@file@percent }
\newlabel{fig:Cauchy}{{9.2}{175}{Pdf of the Cauchy distribution ($\mu =0$, $\gamma =1$)}{figure.9.2}{}}
\newlabel{def:GEVdistri}{{9.15}{175}{Generalized Extreme Value (GEV) distribution}{definition.9.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Stochastic convergences}{175}{subsection.9.3.3}\protected@file@percent }
\newlabel{StochConvergences}{{9.3.3}{175}{Stochastic convergences}{subsection.9.3.3}{}}
\newlabel{prp:chebychev}{{9.11}{175}{Chebychev's inequality}{proposition.9.11}{}}
\newlabel{def:convergenceproba}{{9.16}{176}{Convergence in probability}{definition.9.16}{}}
\newlabel{def:convergenceLr}{{9.17}{176}{Convergence in the Lr norm}{definition.9.17}{}}
\newlabel{def:convergenceAlmost}{{9.18}{176}{Almost sure convergence}{definition.9.18}{}}
\newlabel{def:cvgceDistri}{{9.19}{176}{Convergence in distribution}{definition.9.19}{}}
\newlabel{prp:Slutsky}{{9.12}{176}{Rules for limiting distributions (Slutsky)}{proposition.9.12}{}}
\newlabel{prp:implicationsconv}{{9.13}{176}{Implications of stochastic convergences}{proposition.9.13}{}}
\newlabel{eq:convgce1}{{9.3}{177}{Stochastic convergences}{}{}}
\newlabel{eq:convgce2}{{9.3}{177}{Stochastic convergences}{equation.9.3.3}{}}
\newlabel{prp:cvgce11}{{9.14}{177}{Convergence in distribution to a constant}{proposition.9.14}{}}
\newlabel{exm:plimButNotLr}{{9.1}{177}{Convergence in probability but not $L^r$}{example.9.1}{}}
\newlabel{thm:cauchycritstatic}{{9.2}{177}{Cauchy criterion (non-stochastic case)}{theorem.9.2}{}}
\newlabel{thm:cauchycritstochastic}{{9.3}{177}{Cauchy criterion (stochastic case)}{theorem.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Central limit theorem}{178}{subsection.9.3.4}\protected@file@percent }
\newlabel{CLTappend}{{9.3.4}{178}{Central limit theorem}{subsection.9.3.4}{}}
\newlabel{thm:LLNappendix}{{9.4}{178}{Law of large numbers}{theorem.9.4}{}}
\newlabel{thm:LindbergLevyCLT}{{9.5}{178}{Lindberg-Levy Central limit theorem, CLT}{theorem.9.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Some properties of Gaussian variables}{178}{section.9.4}\protected@file@percent }
\newlabel{GaussianVar}{{9.4}{178}{Some properties of Gaussian variables}{section.9.4}{}}
\newlabel{prp:bandsindependent}{{9.15}{178}{}{proposition.9.15}{}}
\newlabel{prp:update}{{9.16}{179}{Bayesian update in a vector of Gaussian variables}{proposition.9.16}{}}
\newlabel{prp:truncated}{{9.17}{179}{Truncated distributions}{proposition.9.17}{}}
\newlabel{eq:Etrunc}{{9.4}{179}{Truncated distributions}{equation.9.4.4}{}}
\newlabel{eq:Vtrunc}{{9.5}{179}{Truncated distributions}{equation.9.4.5}{}}
\newlabel{eq:Vtrunc2}{{9.6}{179}{Truncated distributions}{equation.9.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces $\symbb {E}(X|X<b)$ as a function of $b$ when $X\sim \symcal {N}(0,1)$ (in black).}}{180}{figure.9.3}\protected@file@percent }
\newlabel{fig:inverseMills}{{9.3}{180}{$\mathbb {E}(X|X<b)$ as a function of $b$ when $X\sim \mathcal {N}(0,1)$ (in black)}{figure.9.3}{}}
\newlabel{prp:pdfMultivarGaussian}{{9.18}{180}{p.d.f. of a multivariate Gaussian variable}{proposition.9.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Proofs}{180}{section.9.5}\protected@file@percent }
\newlabel{AppendixProof}{{9.5}{180}{Proofs}{section.9.5}{}}
\newlabel{eq:XXX}{{9.7}{181}{Proofs}{equation.9.5.7}{}}
\newlabel{eq:lm10}{{9.8}{182}{Proofs}{equation.9.5.8}{}}
\newlabel{eq:lm1}{{9.9}{182}{Proofs}{equation.9.5.9}{}}
\newlabel{eq:lm29}{{9.10}{182}{Proofs}{equation.9.5.10}{}}
\newlabel{eq:lm30}{{9.11}{182}{Proofs}{equation.9.5.11}{}}
\newlabel{eq:lm2}{{9.12}{182}{Proofs}{equation.9.5.12}{}}
\newlabel{eq:lm3}{{9.13}{182}{Proofs}{equation.9.5.13}{}}
\citation{gourieroux_monfort_1995}
\newlabel{eq:multiplier}{{9.14}{183}{Proofs}{equation.9.5.14}{}}
\newlabel{eq:lm20}{{9.15}{183}{Proofs}{equation.9.5.15}{}}
\newlabel{eq:lr10}{{9.16}{184}{Proofs}{equation.9.5.16}{}}
\newlabel{eq:1}{{9.17}{186}{Proofs}{equation.9.5.17}{}}
\bibdata{book.bib,packages.bib}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Statistical Tables}{188}{section.9.6}\protected@file@percent }
\newlabel{statistical-tables}{{9.6}{188}{Statistical Tables}{section.9.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Quantiles of the $\symcal {N}(0,1)$ distribution. If $a$ and $b$ are respectively the row and column number; then the corresponding cell gives $\symbb {P}(X\le a+b)$, where $X \sim \symcal {N}(0,1)$.}}{189}{table.9.1}\protected@file@percent }
\newlabel{tab:Normal}{{9.1}{189}{Quantiles of the $\mathcal {N}(0,1)$ distribution. If $a$ and $b$ are respectively the row and column number; then the corresponding cell gives $\mathbb {P}(X\le a+b)$, where $X \sim \mathcal {N}(0,1)$}{table.9.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Quantiles of the Student-$t$ distribution. The rows correspond to different degrees of freedom ($\mitnu $, say); the columns correspond to different probabilities ($z$, say). The cell gives $q$ that is s.t. $\symbb {P}(-q<X<q)=z$, with $X \sim t(\mitnu )$.}}{190}{table.9.2}\protected@file@percent }
\newlabel{tab:Student}{{9.2}{190}{Quantiles of the Student-$t$ distribution. The rows correspond to different degrees of freedom ($\nu $, say); the columns correspond to different probabilities ($z$, say). The cell gives $q$ that is s.t. $\mathbb {P}(-q<X<q)=z$, with $X \sim t(\nu )$}{table.9.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.3}{\ignorespaces Quantiles of the $\mitchi ^2$ distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities.}}{191}{table.9.3}\protected@file@percent }
\newlabel{tab:Chi2}{{9.3}{191}{Quantiles of the $\chi ^2$ distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities}{table.9.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.4}{\ignorespaces Quantiles of the $\symcal {F}$ distribution. The columns and rows correspond to different degrees of freedom (resp. $n_1$ and $n_2$). The different panels correspond to different probabilities ($\mitalpha $) The corresponding cell gives $z$ that is s.t. $\symbb {P}(X \le z)=\mitalpha $, with $X \sim \symcal {F}(n_1,n_2)$.}}{192}{table.9.4}\protected@file@percent }
\newlabel{tab:Fstat}{{9.4}{192}{Quantiles of the $\mathcal {F}$ distribution. The columns and rows correspond to different degrees of freedom (resp. $n_1$ and $n_2$). The different panels correspond to different probabilities ($\alpha $) The corresponding cell gives $z$ that is s.t. $\mathbb {P}(X \le z)=\alpha $, with $X \sim \mathcal {F}(n_1,n_2)$}{table.9.4}{}}
\bibcite{Abadie_Cattaneo_2018}{{1}{2018}{{Abadie and Cattaneo}}{{}}}
\bibcite{Anderson_1971}{{2}{1971}{{Anderson}}{{}}}
\bibcite{Anderson_Hsiao_1982}{{3}{1982}{{Anderson and Hsiao}}{{}}}
\bibcite{Andrews_Stock_Sun_2019}{{4}{2019}{{Andrews et~al.}}{{}}}
\bibcite{angrist_mostly_2008}{{5}{2008}{{Angrist and Pischke}}{{}}}
\bibcite{Arellano_Bond_1991}{{6}{1991}{{Arellano and Bond}}{{}}}
\bibcite{Cameron_Miller_2014}{{7}{2014}{{Cameron and Miller}}{{}}}
\bibcite{Cameron_Trivedi_2005}{{8}{2005}{{Cameron and Trivedi}}{{}}}
\bibcite{Cochrane_Orcutt_1949}{{9}{1949}{{Cochrane and Orcutt}}{{}}}
\bibcite{DEGOOIJER2006443}{{10}{2006}{{{De Gooijer} and Hyndman}}{{}}}
\bibcite{DEE20041697}{{11}{2004}{{Dee}}{{}}}
\bibcite{Diebold_Mariano_1995}{{12}{1995}{{Diebold and Mariano}}{{}}}
\bibcite{Durbin_1954}{{13}{1954}{{Durbin}}{{}}}
\bibcite{Durbin_Watson_1950}{{14}{1950}{{Durbin and Watson}}{{}}}
\bibcite{Durbin_Watson_1951}{{15}{1951}{{Durbin and Watson}}{{}}}
\bibcite{Fritsch_et_al_2019}{{16}{2019}{{Fritsch et~al.}}{{}}}
\bibcite{gourieroux_monfort_1995}{{17}{1995}{{Gouri\'eroux and Monfort}}{{}}}
\bibcite{Greene2003Econometric}{{18}{2003}{{Greene}}{{}}}
\bibcite{Hansen_1982}{{19}{1982}{{Hansen}}{{}}}
\bibcite{Hausman_1978}{{20}{1978}{{Hausman}}{{}}}
\bibcite{James2013}{{21}{2013}{{James et~al.}}{{}}}
\bibcite{JST_2017}{{22}{2017}{{Jord\`a et~al.}}{{}}}
\bibcite{Litterman_Scheinkman_1991}{{23}{1991}{{Litterman and Scheinkman}}{{}}}
\bibcite{MacKinnon_White_1985}{{24}{1985}{{MacKinnon and White}}{{}}}
\bibcite{MACKINNON2022}{{25}{2022}{{MacKinnon et~al.}}{{}}}
\bibcite{Meyer_Viscusi_Durbin_1995}{{26}{1995}{{Meyer et~al.}}{{}}}
\bibcite{Nakosteen_Zimmer_1980}{{27}{1980}{{Nakosteen and Zimmer}}{{}}}
\bibcite{Newey_West_1987}{{28}{1987}{{Newey and West}}{{}}}
\bibcite{Roberts_Gelman_Gilks_1997}{{29}{1997}{{Roberts et~al.}}{{}}}
\bibcite{Sargan_1958}{{30}{1958}{{Sargan}}{{}}}
\bibcite{Stock_Watson_2003}{{31}{2003}{{Stock and Watson}}{{}}}
\bibcite{stock_yogo_2005}{{32}{2005}{{Stock and Yogo}}{{}}}
\bibcite{Tibshirani_2011}{{33}{2011}{{Tibshirani}}{{}}}
\bibcite{White_1980}{{34}{1980}{{White}}{{}}}
\bibcite{Wu_1973}{{35}{1973}{{Wu}}{{}}}
\gdef \@abspage@last{195}
